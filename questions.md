### soru 

> quest": "Merhaba Grafiği yorumlamakta güçlük çektim yardımcı olursanız sevinirim.  3 çizgi neyi temsil ediyor x ve y eksenleri açıklanmış. -7 olarak tahmin edilmiş ortalama skor aslında datada -8 e mi denk geliyormuş ? bunu anlamlandıramıyorum bu benim için ne ifade ediyor eksenlerin logaritmik oldugu söylenmiş fakat herhangi logitmik artışta göremiyorum belkide gözden kaçırıyorum. Bu kötü sınıflandırmaya ait bir grafikse iyi bir sınıflandırmaya grafigi neye benzemeliydi teşekkürler."

![image](image/2.jpg)

> comments":

1. -> Öncelikle şunu söyleyeyim ki; eksenler logaritmik olarak ölçeklendirilmiş. Herhangi bir logaritmik artıştan bahsetmemizi gerektirecek bir grafik değil, yalnızca modelin tahminleri ile gerçek değerler arasındaki örtüşmeye baktığımız bir grafik.X ekseni modelin yaptığı tahmini, Y ekseni ise gerçekteki değeri göstermekte. Ortada yer alan pembemsi lineer çizgi bizim tahminimiz ile gerçekteki (actual) değerinin tam örtüştüğü durumda çizilecek çizgidir. Yani tahmin -6.000'da iken gerçek değeri de -6.000'da ise o point tam olarak pembemsi çizginin üzerine düşer. Üst ve altındaki açık mavi ve yeşilimsi çizgiler ise tahmin ediyorum ki üst ve alt güven aralığı limitleri. Tabiki her noktayı hatasız olarak tahmin edemeyebiliriz bir miktar (%1, 5 belki 10) hata kabul edilebilir. Bu mavi ve yeşil çizgiler de bu güven aralığını temsil ediyor diye düşünüyorum. Tahmin verilerinin daha düşük olduğu (-6.000 ve daha düşük) noktalarda çizgilerin dışında yer alma durumu söz konusu. Bunlar bizim veriyi -en azından bir kısmını- iyi tahmin edemediğimizi gösteriyor. Bunun birkaç sebebi olabilir: İlk sebebi heteroscedasticity, yani değişken varyans. Verinin belli bir kısmı daha fazla gürültü içeriyor olabilir. Lambda değerini olması gerekenden yüksek tutarak fazla regülarize etmiş olabilir ve training setimiz ana verimizin belli alt kümelerini yeteri kadar iyi temsil etmiyor olabilir.
2. ->  ->  eksenlerin logaritmik ölçeklendirildiği crash course içerisinde yer alıyor fakat eksenlerdeki değerlere bakınca logaritmik artış göremediğim için anlamada güçlük çektim. 64 49 36 olarak yazılsaydı mantıklı olurdu gibi ?",
3. ->  ->  Eksenler 10 100 1000 10000 şeklinde olsa idi logaritmik olur du..",
4. -> -> Burada Lambda cok kucukse de, yani model training set'e overfit olduysa, yine prediction bias fazla olmaz miydi? (yani over-regularization'in tam zitti da bu soruna yol acmaz mi?) Neden sadece Lambda'nin cok buyuk olmus olabilecegi dusunuluyor?",
5. -> -> Yaklaşım doğru lambda olması gerekenden fazla küçük olursa overfit olur fakat prediction bias'ın tanımı ile alakalı bir durum söz konusu. Formülü bize şunu söylüyor:Tahminlerin ortalaması - Actual değerlerin ortalaması değerinin olabildiğince az olmasından bahsediyor. Lambdayı olması gerekenden çok büyük seçersek underfitting olacağından bizim modelin tahmin değerleri actual değerlerden uzak kalacaktır ve bu prediction bias'a sebep olacaktır. Fakat lambdayı fazlaca küçük seçersek overfitting olacak ve bizim modelin tahmin değerleri actual değerlere oldukça yakın olacaktır.Bu noktada dolaylı bir problem söz konusu olabilir. O da overfitting'den dolayı test verisinin prediction bias'ının istenenden fazla olması. Dolaylı bir etki olarak lambdayı küçük de seçmek training verisi için değil ancak test verisi için prediction bias yaratabilir diyebiliriz.

### soru

> quest": "Merhaba arkadaşlar. Burada kutucuk içerisine aldığım yeri anlayamadım . Muhtemelen bir yeri kaçırdım , yardımcı olabilirseniz çok sevinirim.",

![image](image/3.jpg)

> comments": 

1. ->  Merhaba,Kutucuk içerisine aldığınız ifade ReLU fonksiyonu, max(0, x)-> 0 ile x'den hangisi büyükse onu geri döndürür.",
2. ->  ->  çok teşekkür ederim şimdi anladım..",
3. ->  ReLU fonksiyonu 0'a kadar (negatif değerlerden 0'a kadar) 0 değeri, 0'dan sonra da kendi değerini (x değerini) verir.örn: x=-2 olsun f(x)=0 olur.x=3 olsun f(x)=3 olur.",

### soru

> quest": "Merhaba, konularla tam bağlantılı olmasa da bir şey takıldı kafama. Bu kurs sürecinde, temel kavramlar gördük. Epoch, learning rate, batch size gibi.. Bunlar, makine öğrenmesinin temeli diyebilir miyiz? Demek istediğim, flapy bird veya snake oynayan/öğrenen bir yapı ile spam mail ayıran yapıda benzer kavramlar bulunuyor mu? Yoksa, başka alanlarda başka kavramlar, başka temeller mi giriyor?", 

> comments":

 1. -> Tabii düşününce, veri seti olmazsa neyi batchlere ayıracağız gibi bir soru da çıkabiliyor ortaya ama genel olarak anlaşıldığını düşünüyorum..",
 2. ->  Flapy bird oynayan yaoiyi bilmiyorum 🙂 Ancak spam mail ayiran yapi olarak tarif ettigin sey binary classification yapiyor,.",
 3. -> Flappy Bird/Snake gibi oyunları öğretmek için kullandığınız metoda göre, temel kavramlar değişiklik gösterecektir. Örneğin, yılan oyununu kendi kendine oynayan bir yazılım için farklı çözüm yaklaşımları olabilir.1 - Algoritmik yaklaşım: Oyunun state'ine göre bir sonraki hamleyi verecek algoritmayı kendiniz kodlarsınız. Burada herhangi bir makine öğrenmesi sözkonusu değil.2 - Supervised Learning: Modelinize yüzbinlerce etiketlenmiş training data verirsiniz. Burada inputlar farklı oyun state'leri, label'lar da ilgili state için oynanması gereken doğru hamle olabilir. Önceden oynanmış oyunlar ile model eğitilebilir.3 - Reinforcement Learning: Yapılan hamle serilerinin sonucunda ödül/ceza sistemi uygulayarak modelin kendi kendine oynayarak öğrenmesi sağlanabilir.4- Genetik Algoritmalar: Bir neural network'ün parametrelerini (weights & biases) yılanın genetiği olarak düşünebiliriz. Mutasyonlar ve crossoverlar ile yeni nesil yılanlar türetilir ve en iyi performansı gösterenler seçilir. Bu bir döngü halinde tekrar edilir ve giderek daha başarılı (fit) yılanlar elde edilmiş olur.Bu başlıklar için benzer kavramlar mutlaka vardır ama hepsinin kendine özgü temel kavramlara sahip olduğu aşikar.",
 4. -> Çok teşekkür ederim, çok açıklayıcı yorum olmuş..",

 ### soru 

> quest": "Merhabalar, ROC curve ve Auc 'un işlevini pek anlayamadım biz bunları nede kullanıyoruz bu bir zorunluluk mudur acaba?",

> comments":
1. ->  Merhaba,[Link](https://community.globalaihub.com/?status/1482-1482-1587629584/#comment.4782.4624.4624) yorumumda bunları açıklamaya çalıştım. Zorunlu değillerdir ama yapmamız modelin performansını ölçmemiz açısından önemlidir.İyi çalışmalar.Community."
2. ->  Precision/recall, F1-score, error rate, accuracy , ROC curve ve AUC classification icin kullanilan performans metriclerdir.Burada classification kisminin altini cizmek istiyorum cunku amacimiz prediction iyapmaksa RMSE, Pearson's correlate coefficient ve coefficient of determination kullanilir.Bu performans metrikleri genel olarak error analizi yapip modelimizi gelistirmeye yarar.",

### soru 

> quest": "Merhaba arkadaşlar,  Roc curve'de neden TPR ve FPR adı verilen iki parametreyi kullanıyoruz? ve bir de FPR 'de neden True negativ'i hesaplamak yerine False positive'i hesaplıyoruz?"

> comments": 

1. ->  Merhaba,TPR dediğimiz parametre Recall metriğidir aslında. Recall şunu sorar True Positivelerin kaç tanesi modelimiz tarafından tahmin edildi? TPR değerimizin kullanım amacı budur.TPR ve FPR değerleri birbirileri ile korelasyonludur ve curve'ümüzü (1,1) noktasında sınırlayabiliyoruz. Farklı threshold değerlerinde bir değer(TRP örnreğin) artarken diğer değer de (FPR örneğin) buna bağlı olarak artış gösterecektir. Bu değerleri kullandığımızda aynı tahmin setinin tpr ve fpr değerlerini görebilmek, eğer modelimizde bir problem var ise  [Understanding AUC - ROC Curvetowardsdatascience.comIn Machine Learning,](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) linkindeki How to speculate the performance of the model? kısmında bunlara değiniliyor) bu kolayca tespit edilebilir.İyi çalışmalar.",
2. ->  ->  merhaba, [Link](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) şu linkte tpr ve fpr değerlerinin birbirlerini pozitif yönde etkilediği ifade edilmiş. Bu değerler birbirlerini hangi yönde etkiliyor. Ayrıca FPR =1-specifity olarak tanimlanmiş. Specifity kavramı crash courseta açıklanmamış ama. Specifityden ne anlamalıyız? Teşekkür ederim.",
"Understanding AUC - ROC Curvetowardsdatascience.comIn Machine Learning, performance measurement is an essential task. So when it comes to a classification problem, we can count on an AUC ….",
3. ->   ->  Merhaba,Evet orada ufak bir anlaşmazlık olmuş.TPR değeri sizin Recall değeriniz. Specifity değeriniz ise Recall değerinizin Positive classlar için değil negative classlar için hesaplanmış değeridir.FPR değeriniz 1-Specifity'dir. Bu bağlamda biz thresholdu arttırdığımızda TPR zaten artacaktır, specifity değerimiz düşeceği için FPR değerimiz artacaktır. Oradaki anlam karmaşasını da düzelttim. Uyarı için teşekkürler.İyi çalışmalar..",
4. -> True Positive Rate = TPR = Recall =TP/(TP+FN) False False Positive Rate = FPR = FP / (FP+TN) = 1-(True Negative Rate) True Negative Rate = TNR = TN/(FP+TN) Roc Curve, True Positive Rate ile False Positive Rate arasindaki iliskiyi ifade ediyor.Ne yaptigimizi iyi anlayalim.Classification yapiyoruz.Bunun icin Precision/recall, F1-score, error rate, accuracy ve ROC curve performance metriklerimiz ve bunlardan yararlanarak hata analizi yapip modelimizi gelistirecegiz.Classification yaparken labellar probabilistic distrubition sonucunda belirleniyor.Roc curve icin true ya da false farketmez kesinlikle pozitif degerlerle ilgileniyoruz ve True, false olasiliklarin degisimine bakiyoruz.Ve tabiki TPR ve FNR 0 ve1 arasinda degerler aliyor.Son olarak sordugun soruya geldigimde FPR yi hesaplarken True Negative Rate i hesapliyorsun, false negativi degil..",
5. ->  [Link](https://youtu.be/4jRBRDbJemMROC) and AUC, Clearly Explained!youtu.beROC (Receiver Operator Characteristic) graphs and AUC (the area under the curve), are useful for consolidating the information from a ton of confusion matric...",
6. ->  ->  Videonun ozetle kismi cok iyi aciklamis.Roc corves threshold degerini belirlemeyi kolaylastiriyor.AUC ise hangi classification methodunun daha iyi oldugu konusunda karar vermemize yardim ediyor..",
7. ->  teşekkürler.",
8. -> Yalniz ROC egrisinden hangi threshold degerinin hangi sonuca vardigini cikarsayamiyoruz (thr degerine gore sirali degiller). O zaman bu egri uzerindeki noktalar bize threshold degerinin ne oldugu bilgisini vermiyor. degil mi? -> ",
9. ->  ->Evet, burada eğri noktalar bize threshold değerini vermez. Burada AUC ile hesapladığımız zaten bütün threshold değerlerinde nasıl bir performans elde edebileceğimiz. ROC curve'ünde ise TPR ve FPR değerleriyle threshold'u elde edemeyiz."

### soru

> quest": "Merhabalar, birkaç sorum var onları sırayla sormak istiyorum. Her soruyu yorum olarak atıyorum.",

> comments": 

1. -> Diğer Metriclerde, “classification_threshold” değerini kullanırken (ki bu değer de 0.52) neden AUC için kendimiz bir değer atadık? Ayrıca bu değeri neye göre belirledik?.",
2. -> Burada, model.add diye başlayarak yeni katmanlar ekliyoruz ve units’de de katmanların ne kadar nöron içereceğini belirliyoruz. Peki, neye göre belirliyoruz? Kaç katman olacağını, kaç nöron olacağını neye göre belirliyoruz?.",
3. -> -> Oncelikle novel bir model yaratirken kac layer ekleyecegini kimse sana soyleyemez.Mesela Oxford tarafindan yayinlanan vgg16 modelini dusun.Neye gore layer sayisini belirlediklerinin belli bir cevabi yok.Ayni sekilde 50 layerdan olusan Resnet 50 ya da Resnet 101.Ancak layer sayisi arttikca ve azaldikca neler oluyor sorusunun cevabi var.Layer sayisi arttikca modelin derinlesir komplex hale gelir.Overfitting probleminin ortaya cikma olasiligi artar..",
4. -> ->  Peki ama şu da var, overfittingden kaçınma yollarımız; learning ratei ayarlamak, öğrenmeyi erken bırakmak gibi yöntemler.. Yani optimum noktaya ulaşana kadar, elimizdeki her şeyi ayarlayarak ilerleyeceğiz, kimse bize neyin ne olduğunu söylemiyor diyorsunuz, öyle mi?.",
5. ->  -> Kimse sana kac layer kullanacagini ya da kac tane kullanman gerektigini soyleyemez cunku kimse bilmiyor.Sadece cok layer eklenince ne oluyor az sayida layer ile ne oluyor kismini tecrube ederek ogreniyorsun.Dedigin gibi overfitting problemleri icin regularization tekniklerini kullaniyorsun.Son olarak, fazla sayida layer daha iyi model anlami tasimaz.Bazen daha basit bir modelle iyi sonuclar alirsin..",
6. -> Belki kaç kez soruldu ama… Sorularımızı cevaplayan mentörlerimiz ve diğer yazılarda da görüyorum, bu değerleri tecrübe edindikçe ve benzer çalışmaları inceledikçe neler vereceğimizi anlıyoruz diyorlar. Fakat, örneğin learning rate’in aşırı veya çok düşük olduğunu loss’taki sapmalardan veya hiç düşmemesinden anlayabiliyorum. Fakat Epoch, batch size’ları gerçekten neye belirleyeceğiz? Bu noktaya geldim hala kafam almıyor...",
7.->  Oguzhan epochs ve batch_size konusunda ne kadar zamanin oldugu , en az kullanacagin datsetinin genisligi kadar onemli.Oncelikle epboch ve batch_ size in anlamini kavradigindan emin olmalisin.Bunun icin sunun gibi bir sorulara rahatlikla cevap vermelisin.Mesela training dataseti=32000 olsun.Eger batch_size = 32 ise training sirasinda kac iterasyon olacak? 1000 olacak.1000 iterasyonun tamamlamasi 1 epoch oldu demek degil mi? Daha hizlica 1 epoch tamamlansin modelimin performansi hakkinda hizlica fikir edinmek istiyorum dersen batch_size ini arttir. Training sirasinda total kaybini ve accuracy yi goreceksin.Traininge devam ederek kaybini azaltmaya accuracy i arttirmaya modelini gelistirmeye devam edebilirsin.epeochs = 10 yap ne kadar zaman aldigina bak.See zaman!",
8. -> ->  Anladım, çok teşekkür ederim 🙂.",
9. ->  ->  training dataset/batch_size = 1 epoch mu ediyor yani",
10. ->  ->  Yukarida verdigim ornege gore yazayim.Training_dataset / batch_size = 1000 iterasyon. Bu 1000 iterasyon tamamlandiginda yani training datasetindeki tum veriler bir kez training safhasindan gecip tamamlandiginda 1 epoch ediyor.",

### soru

> quest": "auc'un diğer metriklerle mi yoksa yalnız başına konması mı daha mantıklı olur? ayrıca auc ve roc kavramlarını anlayamadım. Teşekkürler."

> comments": 

1. -> Merhaba,Positive Rate curve'ü çeşitli threshold değerlerinde precision ve recall arasındaki dengeyi gösterir. AUC'ta ise True Positive Rate(zaten recall oluyor, TP/TP+FN) ve False Positive Rate (FP/FP+TN) arasındaki ilişki grafiğinin alanı alınarak her threshold değeri için performans ölçülmeye çalışılır.(Yani modelin toplam performansı). Bu yüzden AUC'u kullandığınızda precision ve recall kullanmayabilirsiniz.Precision ve recall metrikleri sınıflandırma modelimizin performansını farklı açılardan ölçer. Herhangi biri diğerinden daha iyi diyemeyiz iki metriğin de performansı farklı açılardan ölçtüğünü bildiğimizden threshold değerini ikisini de optimum düzeyde tutacak şekilde ayarlayabiliriz.ROC grafiği her threshold değeri modelimizin için True Positive Rate (namı diğer recall) ve False Positive Rate (negatif örnekler içindeki yanlış pozitiflerin oranını ölçer) değerlerinin plot edildiği bir grafiktir. Bu grafikte TPR değerlerinin FPR edğerlerine göre daha büyük olmasını bekleriz nedenini [Link](https://community.globalaihub.com/?status/1133-1133-1587507559/#comment.4733.4585.4585) linkinde açıklamaya çalıştım. En performanslı durum TRP değerinin 1, FPR değerinin 0 olmasıyken en performanssız durum tam tersidir.( TPR 0, FPR 1 burada negatifleri pozitif, pozitifleri negatif diye tahmin eder.)AUC ise bu grafiğin altında kalan alanı integral ile hesaplayarak aslında her threshold değeri için TPR ve FPR hesaplamasının yapılmasını kolaylaştırmış olur. Her threshold için bu hesaplamaları tek tek yapmaktansa AUC (Area Under the Curve) kullanarak bu hesaplamayaı integral ile kolayca yapar. Bu linkte de bazı yararlı açıklamalar bulabileceğinize inanıyorum. [Link](https://community.globalaihub.com/?status/875-875-1587486608/#comment.4706.4558.4558) İyi çalışmalar.Community",
2. ->  ->  açıklamanız için teşekkür ederim..",

### soru

> quest": "Merhabalar,  Alttaki L2 regularisation konusu ile ilgili olarak, sayet feature'lardan birisi label ile korrole ise, regularisation olmasa bile bu feature'un weight'i yuksek cikmayacak mi? Yani baska bir deyisle non-informative feature'larin learning modelde weight'inin yuksek cikmasinin nedeni L2 regularisation mu yoksa aslinda zaten bu feature'larin label'lar ile korrole olmasi mi? Bu durumda \"non-informative ama label ile korrole feature'larin weight'inin L2 regularisation nedeniyle yukselmesi\" ifadesi dogru olur mu?  Tesekkurler,  L2 regularization may cause the model to learn a moderate weight for some non-informative features. Surprisingly, this can happen when a non-informative feature happens to be correlated with the label. In this case, the model incorrectly gives such non-informative features some of the \"credit\" that should have gone to informative features.",

> comments": 

1. -> Merhabalar, label Ile korrole olan non informative feature in katsayısı L2 regularization olmadan da yüksek çıkacaktır. Ancak L2 ile korrole olmayan featurelarin katsayıları sıfıra yaklaşırken , korrole olan non informative featureimizinda katsaysi korrole olması sebebiyle artacaktır. Yani başlangıçta düşük bir katsayiya sahip olup L2 sebebiyle yükselmiyor. Korrole olması nedeniyle zaten diğer korrole olmayan featurelardan yüksek bir katsayiya sahip oluyor. Eğitim sonunda da label Ile arasında bulunan korrelasyondan dolayı katsayısı artiyor.Edit: but durum spurious correlation (sahte korelasyon) olarak tanımlanmaktadır. Mesela yazin dondurma tüketimi artmaktadır, aynı şekilde denizde boğularak olmelerde artmaktadır. Boğularak ölmeleri arastirdigimiz modelimize dondurma tuketiminide ekledigimizi düşünecek olursak bu durumda dondurma tüketimi yazın gerçekleşen ölümler için etkin bir değişken gibi görünecektir ve yüksek bir weight'e sahip olacaktır.İyi çalışmalar.",
2. -> ->  Merhaba,Bunu 1 hafta kadar önce sormuştum fakat yanıt alamamıştım. Benzer bir konu olduğundan hazır sizi bulmuşken tekrar sorayım. Yukarıda x değişkenleri ile y arasındaki bir korelasyondan söz edilmiş. Eğer modelimizdeki x değişkenleri kendi arasında 0.7 veya 0.8'den daha büyük bir korelasyona sahipse hiçbir şey yokmuş gibi modeli çalıştırmaya devam mı etmeliyiz?.",
3. ->  -> Bu problem MultiCollinearity problemi olarak tanımlanır. Açıklayıcı değişkenlerin(X'lerin) arasında yüksek korelasyon olması sebebiyle karşımıza çıkar. Bu sorunu görmezden gelerek çalışmamıza devam edebiliriz. Ya da soruna sebep olan değişkenlerden birini modelden çıkartabiliriz. Hangisi olacağına karar vermek için her değişkenle deneyerek model performansımıza bakabiliriz. Ya da Temel Bileşen Analizi, Faktör Analizi gibi bir yöntemler ile bu çoklu doğrusal bağlantı sorununu ortadan kaldırmaya çalışabiliriz.Özetle ne modelimizi optimal formuna getirmek için her türlü yaklaşımı deneyip çalıştığımız model için en uygun yöntemi tespit edip, bu şekilde çalışmaya devam etmeliyiz..",
4. -> ->  VIF skorları gözetilerek Ln veya sqrt gibi bir dönüşüm veyahut faktör analizi yapmanın mümkün doğru ama benim yukarıda asıl sormak istediğim (Bunu net sormadım üstü kapalı olarak sormak istemiştim) niçim multicollinearity durumunu derslerdeki hiçbir videoda veya dökümanda görmüyoruz. Bu durum niçin es geçiliyor? Aynı şekilde stationary durumu da birçok örnekle es geçilmiş. Normalde istatistiksel bir analizin temeli olan heteroscedasticity, multicollinearity ve autocorrelation gibi durumlar sırf analizi devam ettirilmek adına görmezden geliniyor gibi geldi. Bu ne derece anlamlı?.",
5. ->  -> ML ile İstatistiksel Analiz birbirlerinden ince bir çizgi ile ayrılır. Bu noktada amacına göre bahsettiğiniz yaklaşımlardan birini ya da bir kaçını tercih ederiz. Amacımız tahmin değil yorumlamak ise, multicollinearity, heteroscedasticity/ homoscedasticity, stationarity gibi faktörler dikkate alınmak durumunda iken, bütün bunlaramacımız tahmin olduğunda model etkinliğimizi artırabilecek bileşenler haline gelebilmekte.Eğitim içeriği makina öğrenimi üzerine olması sebebi ile, verinin işlenmesi model için feature seçimi, bunların anlamlılık testleri ve bahsettiğiniz durumların testleri gibi istatistiksel yaklaşımlara yer verilmemiş. Bütün bunlar, görmezden geliniyor gibi değil de farklı bir eğitim konusu olarak düşünülebilir.",
6. -> ->  Anladım teşekkürler..",

### soru

> quest": "Arkadaşlar merhaba ben L1 Regularization'ı ve feature cross ların ne olduğunu tam anlamadım galiba. Yardımlarınızı bekliyorum. Çok teşekkür ederim 🙂",

> comments": 

1. -> Merhaba, [Link](https://community.globalaihub.com/?status/774-774-1586937745/#comment.4123.4009.4009) linkinde feature cross kısmını açıklamaya çalıştım ama ulaşamazsanız aşağıya alıntılıyorum.\"Merhaba,Öncelikle Feature Cross yapmamızın sebebi modelimizin verileri tek bir lineer çizgiyle ayıramamasıdır. Bu yöntemle yeni bir feature elde edip bu yeni feature modelimizin eğitim sırasında verileri daha etkili ayırıp daha etkili bir hipotez fonksiyonu elde etmesinde yardımcı oluyor. Örneğin elinizde \"dil\" ve \"ülke\" kategorik featureları olsun.Örneğin dil feature değerleri: \"Türkçe, İngilizce, Japonca\"Ülke değerleri de: \"Türkiye, İspanya, Kanada\" olsun.Bu iki kategorik veriyi One Hot Encoding kullanarak binary vector'e çevirdiniz ki modelimiz numerik veri üzerinde çalışabilsin.Eğer siz bu iki feature'ı yani iki binary vector değerini çarparsanız elinizde 9 elemanlı bir binary vector olur. Bu binary vector değerlerinden her biri bir ihtimali temsil eder. Örneğin:[Türkçe ve Türkiye,Türkçe ve İspanya,Türkçe ve Kanada, İngilizce ve Türkçe,.......] gibi.Siz ilgili ihtimalin olduğu indeksteki değere 1 koyduğunuz anda artık o eğitim örneği için o değer geçerlidir. Örneğin [1,0,0,0,0,0,0,0,0] yaptığınızda artık Türkçe ve Türkiye değerini o eğitim örneği için değer belirlemiş olursunuz. Buradaki amaç featureların tek tek tahmine katkısından daha çok katkı sağlamalarını sağlayabilmek. Örneğin dil ve ülke featureları kend başlarına feature olarak katkı sağlarlar ama iki feature'ı çarpıp elde ettiğimiz yeni feature tahminde daha çok katkı sağlayacaktır.\"L1 regülarizasyon yapmamızın sebebi feature cross sonrası oluşacak sparse matrixlerdeki 0 olan feature değerlerinin weightlerini 0layıp onları ortadan kaldırmaktır. L1 regülarizasyon weight değerlerimizden her adımda sabit bir k değerini çıkarır ve sıfırlar. L2 ile karıştırılmamalıdır L2 overfit olmayı engellemek için her adımda weightten kendisinin belli bir yüzdelik dilimini çıkarıp onu 0'a yakınsatır ama asla sıfırlamaz. L1 ise sparse matrixteki 0 değerindeki featureların weight değerlerini sıfırlarlar.Sorunuz olursa sorabilrsiniz.İyi çalışmalar.Community",
2. -> MerhabalarFeature Cross: En basit hali ile elimizde bulunan featurelarımızı çarpmak anlamına geliyor. Mesela:OdaSayısı = [3,5,6,3] ve Konum = [12,25,9,8] olsun: Feature Cross, bu iki feature'mizi eleman düzeyinde çarparak : KonumaGöreOdaSayısı = [3x12, 5x25, 6x9, 3x8] olarak yeni bir sentetik feature elde etmiş oluyoruz. Aslında yaptığımız işlem her iki diziden aynı indise sahip olan elemanları alıp çarparak yeni bir diziye atamak oluyor.L1 Regularization ise, modelimize bir ceza parametresi olarak eklediğimiz katsayısı LAMBDA olan yeni bir parametre. Ve matematiksel formülü: model katsayılarımızın mutlak değerlerinin toplamı olarak ifade edilmekte.(sum(abs(W_i)), i = 1,2,... p, p= feature sayısı.) AMACIMIZ optimal bir ceza ile modelimizi en sade ve en etkin formuna getirebilmek. Optimal cezayı uygulayabilmek için LAMBDA parametresinin optimum değerini tahmin etmemiz gerekmektedir.Lambda'yı 0 almamız halinde kurduğumuz model ile çalışmaya devam etmiş oluruz. Çok büyük bir değer seçersek bu sefer de underfit gibi bir sorun ile karşılaşmış oluruz. Bunlara dikkat ederek optimal değeri bulup modelimizi kuruyor olacağız.Umarım yeterince açık olmuştur 🙂İyi çalışmalar.",
3. ->  ->  Hocam burada yapmaya çalıştığımız şey şu mu? Yani örneğin amacımız dünya üzerindeki yerleşim yerlerindeki median_house_value değerlerini tahmin etmek olsun. Bunun için örneğin latitude değerleri için 200 bucket oluşturalım, longitude değerleri için de 300 bucket oluşturursak. Toplamda 200x300 = 60000 hash_bucket ımız olacak. Dünyanın % 70inin su olduğunu(kara parçası olmadığını) varsayarsak burada bir yerleşim olmayacağı için bunların neredeyse 60000x0.7=42000 gereksiz bu yüzden bunların weight değerini 0 yapmayı amaçlıyoruz. Doğru mudur acaba ? Çok saçma bir soru olmuşsa kusura bakmayın ben de kafamda tam oturtamadığım için sordum..",
4. ->  ->  merhabalar, evet dediğiniz gibi modelimizde bulunan gereksiz değişkenlerin weightlerini sıfırlamak amaç..",
5. ->  ->  Teşekkür ederim..",
6. ->  Çok teşekkür ederim 🙂.",

 ### soru

> quest": "Merhaba Arkadaşlar, Eklediğim kısmı anlayamadım. Yardımcı olur musunuz? Teşekkürler"

> comments": 

1. -> Merhaba,Öncelikle scale invariant şu demek; özelliklerden birini ölçeklendirmek(örneğin 0'dan farklı bir sayı ile çarpmak) tahmin değerini değiştirmez. AUC'un scale invariant olmasının özelliği AUC'un hesapladığı değer bizim tahminlerimizin mertebe sırası, örneğin üst kısımdaki output of log reg kısmına bakarsanız burada hesaplanılan seçilen herhangi bir pozitif değerin seçilen herhangi bir negatif değerin sağında olmasıdır.(modelin rastgele bir pozitif örneği rastgele bir negatif örnekten daha yüksek sıralaması olasılığıdır.) Bunu açıklamam gerekirse;[Link](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5) linkindeki resimler üzerinden anlatımımı yapayım. Bu linkteki Image 6 ve 7'ye bakarsanız bizim istediğimiz en performanslı model yaklaşımı budur. TN ve TP değerlerimizin Threshold değeri itibariyle TN solda TP sağda olacak şekilde ayrılmasıdır. Ama biz genelde Image 8 ve 9'daki bir curve ve plot elde ederiz. Bunun nedeni işin içine FP ve FN girmesidir yani false tahminlerimizin girmesidir. AUC'un en optimum olduğ zaman predictionların TN ve TP olarakl sıralandığı zamandır ve bu yüzden predictionlarımızı prediction değerine göre sıraladığımızda TN değerinin TP değerinin solunda kalması beklenir. AUC zaten bunu hesaplar.1.AUC scale-invariant'tır çünkü burada biz bir prediction değerlerinin mutlak değerleriyle değil dizilim sıralarıyla ilgileniyoruz. Tekrar hatırlatmam gerekirse istediğimz şey TN değerlerinin TP değerlerinin solunda olması (rank olarak TP'den düşük olması) böylece TN ve TP değerlerimiz iyice ayrılıp AUC değerimiz 1 olabilsin.2.AUC classification-threshold-invarianttır çünkü burada hangi threshold'un seçildiğine bakılmaksızın modelin tahminlerinin kalitesini ölçer.Ancak bu iki durum da bazı zamanlarda AUC'un kullanılabilirliğini kısıtlamaktadır.1.Scale invariant olması her zaman istenen bir durum değildir çünkü probability outputlarımızın iyi bir şekilde kalibre edilmesini isteyebilirz. Bu da ranklerden ziyade absolute valuelarına ihtiyaç duyduğumuz anlamına gelit ama AUC absolute valuelar ile değil TN ve TP değerlerinin dizilim rankıyla ilgileniyordu.2.Classification-threshold-invariant da her zaman istenen bir durum değildir. Bunun nedeni ise FN ve FPler arasında fazla bir cost açığı olan durumlarda bir tip classification error değerinini(Örneğin sadece FP) mimize etmenin kritik olabileceğindendir. azaltmak kritik olabilir. Bu minimize işlemini thresholdu yeniden ayarlayarak yapabilrisiniz ama burada hatırlarsak classification-threshold-invariant sayesinde modelin performansı seçilen threshold değerine bakılmaksızın ölçülüyordu. O yüzden classification-threshold-invariant özelliği bu tür optimizasyonlar için pek kullanışlı olmayacaktır.İyi çalışmalar.",
[Understanding AUC - ROC Curve]([Link](https://towardsdatascience.com/understanding-auc-roc-curve-68b2303cc9c5)
2. ->  ->  açıklayıcı yazı için çok teşekkürler.",
3. ->  ->  Biz threshold değerini değiştirdiğimizde TPR ve FPR'i dolayısıyla ROC'i grafiğinin şeklini değiştirmiş oluyoruz. Mesela threshold değerini arttırdığımızda yanlış hesaplamıyorsam TPR'nin de FPR'nin düşmesini, en iyi ihtimal sabit kalmasını bekliyoruz. İkisindeki değişim oranının aynı olması çok zor olduğu için (yani değişim oranları farklıysa birbirlerini kompanse etmeleri zor olduğu için) grafiğinin integrali yani AUC de aynı kalamazmış gibi geliyor. Bu düşünüşteki hata nereden kaynaklanıyor, çelişkinin sebebi nedir bir türlü bulamadım. Yardımcı olabilirseniz sevinirim.",
4. ->  ->  Merhaba,ROC curve'unu farklı TPR ve FPR değerleri için çiziyoruz. Bu değerler fakrlı thresholdlarda değişen değerlerdir. Threshold arttığı zaman TPR değerimiz azalır. Bunu nedeni TP/(TP+FN) formülündeki FN değerinin threshold değeri ile artması olacaktır. FPR de 1-TPR olduğu için TPR düşerken FPR'nin artmasını bekleriz ama bu artış iki tarafta da aynı şekilde olmaz (simple lineer bir artış sergilemez.) ROC, farklı threshold değerleri için belirlenen bu TPR ve FPR değerlerinin çizildiği bir grafik bu grafikte de TPR ile FPR'nin değişim oranları aynı olmadığı ve hata oranı (FP,FN) içeren bir modelimiz olduğunu varsayarsa ROC curve'ümüz parabolik bir görüntü alacaktır. Her threshold değeri için farklı bir ROC curve'ü çizilmez, ROC curve'ü her threshold değeri için TPR ve FPR değerlerini içerir. Bu mantıkla ROC curve'ü sabit kalacağından AUC değeri de sabit kalacaktır.İyi çalışmalar..",
5.  ->  ->  ROC curve zaten farklı thresholdlar için bütün senaryoları üzerinde barındırıyor. Kavramları karıştırmışım bir an. Anladım, çok teşekkür ederim..",
6. ->  Çok net açıklayıcı bilgilerin için çok teşekkürler ->  iyi çalışmalar,

### soru

> quest": "Merhaba Arkadaşlar, Accuracy kısmında eklediğim yeri tam olarak anlayamadım. Yardımcı olabilir misiniz? Teşekkürler",

> ![image](image/1.jpg)

> comments": 
    
1. -> Merhaba,Burada demek istediği %91 accuracy değerinizin olması ilk başta güzel görünebilir ama her seferinde benign tahmin eden bir modelin de %91 accuracy değeri olur. Yukarısındaki örneğe bakacak olursak elimizdeki tablo şu şekilde olacak;- M - BM 1 1B 8 90Bu kısmın accuracy değeri 90+1/90+1+1+8=91/100Hepsi benign tahmin edilseydi oluşacak tablo:- M - BM 0 0B 9 91Çünkü 100 tane tahminimiz vardı 100'ünü benign olarak tahmin ettik ve bunların 9 tanesi malignanttı yani false negative oldu. 91 tanesi ise true negative oldu. Bu durumda 91+0/91+9=91/100 olur yani burada accuracy'nin veri dağılımının dengesiz olduğu(negatif ve pozitif değerler arasında ayrıklık olduğunda) veri setlerinde tek başına yeterli olmayacağını söylemekte.Hatalı gördüğünüz yer olursa düzeltmekten çekinmeyin.İyi çalışmalar.",
2. ->  Accuracy hesaplandığında %91 çıkıyor. Buna bakarak sınıflandırmanın başarılı olduğunu düşünebiliriz ama confusion matrixi incelediğimizde daha farklı bir tabloyla karşı karşıya kalıyoruz. FP ve TN değerlerine bakarsak, iyi huylu tümöre(benign) sahip 91 hastamız var ve 90 kişide benign olduğu tahmin edilmiş. 1 kişi yanlış tahmin edilmiş. Bu başarılı bir tahmin. Ancak, TP ve FN değerlerine bakarsak toplam 9 kişide kötü huylu tümör(malignant) var fakat modelimiz sadece 1 kişide olduğunu tahmin etmiş. Kalan 8 kişide kötü huylu tümör tespit edilememesi oldukça kötü bir tahmin olduğunu gösterir. Burada da diyor ki dengesiz sınıflandırılmış veri setlerinde(class-imbalanced dataset) yalnızca accuracy e bakmak ve buna göre modelin iyi tahmin yapıp yapmadığına karar vermek yeterli değildir. Bu veri setimiz de oldukça dengesiz, pozitif ve negatif etiket sayısı arasında önemli derecede bir eşitsizlik var.",
3. ->  Çok teşekkürler ->  ->  arkadaşlar şimdi gayet iyi anladım 🙂.",

### soru 

> quest": "Burada prediction ve identifie arasındaki fark nedir? Teşekkür ederim.",

> comments":

1. ->  Before/After aralarındaki fark.Yani sonuçlar çıkmadan önce \"predict\" kelimesi kullanılırken, sonuçları alıp yorumladıktan sonra \"identified, addressed, classified\" vb. daha net anlamı olan kelimeler kullanırız. Ki bu şekilde sonuçların elimizde olduğunu onlara göre yorum yaptığımızı belirtmiş oluruz.Edit: Pardon dikkatsizliğime geldi, Recall sonucu için yorum yapılıyormuş orada.Modelin etkinliğinden bahsederken x% doğru tahmin ediyor şeklinde ifade edilir. Ancak Recall bir ölçüm birimidir. amacı da What proportion of actual positives was identified correctly? sorusuna cevap vermektir. Yani tahmin ile ilgili değil de sonuç ile ilgilidir. Hali ile elimizde bulunan bilinen birşeyi yorumlarken x% doğru tahmin etmiş gibi bir şey kullanmayız. Yukarıda yazmış olduğum ilk paragraf kısmi olarak doğruydu.İyi çalışmalar.",
2. ->  ->  Teşekkür ederim..",
3. -> Kesinlik (Precision) ise Positive olarak tahminlediğimiz değerlerin gerçekten kaç adedinin Positive olduğunu göstermektedir.Duyarlılık (Recall) ise Positive olarak tahmin etmemiz gereken işlemlerin ne kadarını Positive olarak tahmin ettiğimizi gösteren bir metriktir.İlk cümlede eğer bizim modelimiz 0.5 precision değerine sahipse bir tümörün kötü huylu olduğunu %50 oranında doğru \"tahmin\" eder demiş.İkinci cümlede ise eğer modelimiz 0.11 recall değerine sahipse tüm kötü huylu tümörlerin %11'ini doğru \"teşhis etmiş\" demektir demiş.Burada aslında anlam olarak kelimeler arasında bir fark yok. Buradak fark precision ve recall arasındaki farktır.Precision şunu sorar, olumlu tanımların ne kadarı gerçekten doğruydu?Ne demek istiyorum?Örneğin positive değerimiz kurdun tehdit oluşturması olsun. Biz kurdun tehdit oluşturuşunu kaç kere tahmin etmişiz ve bu tahminlerin kaçı doğruydu bunun oranını yakaladığımızda precision'I bulmuş oluyoruz. Örneğin biz 7 kez kurdun tehdit oluşunu doğru tahmin edip, 5 kez de kurt tehdit oluşturmuyorken tehdit oluşunu tahmin edersek precision oranımız 7/7+5'ten 7/12 olur. Yani benim kurt benim için tehdit oluşturuyor dediklerimin kaçında kurt gerçekten bizim için tehdit oluşturuyordu?Recall ise şunu sorar, gerçek pozitiflerin ne kadarı doğru bir şekilde tahmin edildi? Örneğin kurt gerçekten tehdit oluşturuyorken ben bu tehditlerin kaç tanesini doğru tahmin edebildim. Kurdumuz bize 20 kez tehdit oluşturuyor olsun. Biz bunların 13 tanesini doğru bir şekilde kurt tehdit oluşturuyor olarak tahmin edip, 7 kez yanlış kurt tehdit oluşturmuyor dersek bizim recall oranımız (13/13+7=13/20) olur. Yani kurdun benim için her tehdit oluşturuşunda ben bunların kaç tanesini doğru bir şekilde tahmin edebildim?Daha faydalı bir link için: [Link](https://medium.com/@gulcanogundur/do%C4%9Fruluk-accuracy-kesinlik-precision-duyarl%C4%B1l%C4%B1k-recall-ya-da-f1-score-300c925feb38) İyi çalışmalar."
4. Doğruluk (Accuracy) , Kesinlik(Precision) , Duyarlılık(Recall) ya da F1 Score ?medium.comVeri bilimi projelerinde en doğru modelin hangisi olması gerektiğine karar vermek için iş birimlerinden gelen talepleri iyi…1 month ago 13 people like this.Like ReportReply",
5. ->  ->  Teşekkür ederim..",

### soru 

> quest": "Merhaba, Genel olarak classification modelleri hakkında birkaç sorum var. 1) AUC ve ROC Curve modelimizde tam olarak nasıl kullanılıyor? Farklı thresholdlara göre tp rate ve fp ratelerimizi hesaplayıp ROC curve'ü oluşturuyoruz. ROC Curve'ün altında kalan alan da AUC oluyor. Bu alan bize modelin pozitif ve negatif örnekleri ne kadar iyi ayırt ettiğini veriyor. F-score'u en yüksek threshold da en iyi threshold oluyor. Ancak bu threshold modele nasıl dönüyor? Yani gradient descentin weightleri değiştirdiği gibi bir mekanizma var mı yoksa thresholdu kendimizin mi belirlemesi gerekiyor? (ki programlama örneğinde kendimiz belirliyorduk) 2) Lojistik regresyonun hata fonksiyonu neye göre belirlendi? Çalışma mantığını tam anlayamadım. 3) Regularization kısmını eklemeden önce weightleri  kendisinden learning rate * kayıp fonksiyonunun türevini çıkartarak değiştiriyorduk. Regularization kısmını ekleyince weightlerden bir de ilaveten lambda * regularizationun türevini çıkartarak mı güncelliyoruz? Umarım açık olmuştur. Cevaplarınız için teşekkür ederim.

> "comments": 

1. -> Merhaba,Anladığım kadarıyla:1. Threshold modele dönüyor derken biz numerik bir verimizi alıp, bu numerik verimizi threshold değerinden büyük küçük olma durumlarına göre yeni sentetik bir feature üzerinden ayırıyoruz. (Yeni featureımız değer>threshold ise 1, değilse 0). Biz her gradient descent yaptığımızda aynı threshold üzerinden modelimiz bu tahminleri öğreniyor ve test set üzerinde yaptığı tahminlerini güçlendiriyor. Programlama egzersizinde de accuracy değeri 0.8 çıkıyordu. Burada ROC, AUC ve F skorlarının amacı sınıflandırma modelimizin performansını ölçüp optimum threshold değerini bulmamıza yardım etmek. Burada belli threshold değerlerine göre accuracy, recall, precision, ROC, AUC ve F skoru değerlerimiz değişiklik gösterecektir. Burada yapmamız gereken uygun threshold değerini bu metriklere bakarak bulabilmek.2.Hata fonksiyonundan kastınızı L1 ve L2 regülariazsyonu olarak anladığım için soruya bu şekilde cevap vereceğim.L1 regülarizasyonu sparsity matrix gibi (içinde milyonlarca trilyonlarca 0 olup az sayıda 1 olan matrixler) RAM'de fazlasıyla yer kaplayan matrixlerin weight değerlerini sıfırlamaya yarar böylece gereksiz 0 değerlerini RAM'de tutmak zorunda kalmayız. Yani önemsiz feature değerlerini de böylece yok etmiş olur. Çünkü L1 regülarizasyon her adımda theta'nın mutlak değerinden sabit bir değer çıkarır. Bu metodu feature selection için kullanabiliriz.L2 regülarizasyonu ise theta değerlerini 0'a yaklaştırır fakat tamamen sıfırlamaz. Her adımda theta değerinin belli bir yüzdesini eksiltir bu nedenle theta değeri asla 0 olamaz (0'a çok yaklaşsa bile) L2'yi ise overfitting durumunu önlemek amacıyla theta değerlerini cezalandırmak için kullanabiliriz. L2'de hiçbir weight sıfırlanmayacağı için feature değerleri eksilmez.3. Link yönlendirmeleri çalışıyor mu emin olmadığım için ekteki resmi ekledim. Regularizasyon yaptığınızda (L2) normalde cost function'ınızın sonuna theta0 dışındaki (çünkü onun feature değeri yok) tüm theta değerlerinin karelerinin ortalaması*lambda ekliyordunuz. Gradient descent'te de her adımda cost function'ın türevini her theta değeri için aldığınızdan dolayı formülde Repeat kısmının altında kalan şekle döner ver aslındaa lambdamız thetamızı cezalandırmış olur.Hatalı kısımlarım varsa düzeltmelere açığım.İyi çalışmalar.",
2. ->  ->  Görseldeki alpha learning rate mi?.",
3. ->  ->  Evet, alpha değeri learning rate..",
4. ->  Hata fonksiyonundan kastım Log Loss idi..",
5. -> ->  Hata fonksiyonunun log loss olmasınn sebebi şu(binary classification mantığında yani iki farklı label çıktımızın olduğnu düşünerek anlatacağım. Örnek iyi huylu veya kötü huylu tümör.):Lojistik regresyon 0 veya 1 arasında olur çünkü tahmin değeri döndürür. Sigmoid fonksiyondur. Buna istinaden de log loss fonksiyonumuz hatayı tespit ederken label değeriyle tahmin değerini karşılaştırır, bir değerin doğru tahmin edilip edilmediğini kontrol eder. Aşağıdaki resimde log loss ve sigmoid fonksiyonlarının 1 ve 0 arasında olduğunu görebilirsiniz. label değeriniz 1 ise ve siz 1 tahmin etmişseniz cost 0 olur, aksi halde -sonsuza kadar gider. Burad istediğimiz label değerimizin doğru tahmin edildiğindeki ve edilmediğindeki costu hesaplamak. Eğer siz continuous ve sonsuz bir değer kümesi içerisindeki costu hesaplamak isteseydiniz değerlerinizi 1-0 ile kıyaslamanız mantıksız olacaktı.Umarım açıklayıcı olabilmişimdir.İyi çalışmalar.."

![image](image/4.jpg)

6. ->  Merhaba, resimdeki yi'ler gerçek değerlerimiz. p(yi) ise tahminimiz (0 ile 1 arasında). Dikkat ederseniz bizim gerçek y değerimiz yani label = 1 ise eşitliğin sağ tarafı tamamen 0 oluyor(1-y). label = 0 ise sol taraf tamamen 0 oluyor. Yani aynı anda sadece bir taraf aktif oluyor. Tahminimiz de sol taraf için 1 olma olasılığı sağ taraf için 0 olma olasılığı. Tahminimiz label'dan ne kadar uzaksa hatayı ona göre buluyoruz..

![image](image/5.jpg)

### 2.hafta quizi

> quest": "2. hafta quizi

### soru 

> quest": "Merhaba. Classification: True vs. False and Positive vs. Negative kısmında, pozitif ve negatif durumlara göre hikaye göz önüne alındığında False positive(FP) ve False negative(FN) durumlarının yer değiştirmesi gerekmez mi ?  Teşekkürler.",

> comments": 

1. ->  Burada negatiflik kafa karıştırıyor olabilir. Kanser testi yaparken kanserli hücre bulunursa sonuç Pozitif çıkar. Bulunmazsa Negatif çıkar. Bu örnekten daha iyi anlaşılabilir anlamlar 🙂 Doğru kanserli teşhis : True Positive. Kanser olmayana Kanser Teşhisi : False Positive. Kanser olup da Kanser değil teşhisi : False Negative. Burada testin pozitif çıkması gerekirken negatif çıkıyor. Kanser olmayana da Kanser değil sonucu : True Negative. Sonuç olarak : False Positive, normalde olmaması gereken şeyi olmuş gibi tahmin etmek. False Negative ise, olması gereken şeyi olmamış gibi tahmin etmek.Yerine göre FP yerine göre de FN'ler tehlikeli olabiliyor..",
2. ->  ->  Teşekkür ederim..",
3. ->  Confusion matrix oluşturulurken modelin sonuçları ile gerçek durumun uyumuna göre çıktılar isimlendirilir.Positive --> Wolf (Pozitif sınıf)Negative --> No Wolf (Negatif sınıf)True ve False etiketi de gerçek durumla modelin uyumuna göre atanır. Eğer uyum varsa True yoksa False etiketi atanır. Shepherd(model) No Wolf dediğinde Negative ve yanlış karar olduğu için False yani False NegativeShepher Wolf dediğinde Positive ve yanlış karar olduğu için False yani False PositiveBu şekilde düşünmen Confusion matrisini doğru bir şekilde oluşturmana yardımcı olabilir..",
4. ->  ->  Pozitiflik ve negatiflik durumu modelin söylediğine mi bağlı yoksa reality'ye mi ?.",
5. ->  ->  ->  un cevabi yeterli oldu sanirim.",
6. ->  ->  Evet teşekkür ederim..",
7. -> Confusion matrix'te kendimize y=1 değeri seçeriz bu da true olur. Örneğin y=1 değeri kurdun tehdidi (positive) olsun. y=0 kurdun tehdit etmiyor oluşu (negative) olsun.Eğer kurt tehdit ediyorsa ve modelimiz bunu doğru tahmin etmişse true positive olur. (gerçek değer y=1, tahmin edilen y=1)Eğer kurt tehdit etmişse ve modelimiz kurdun tehdit etmediğini tahmin etmişse false negative. (gerçek değer y=1, tahmin edilen y=0. Negatif tahmin edlmş ama yanlış)Eğer kurt tehdit etmiyorsa ve modelimiz bunu yanlış tahmin edip kurdun tehdidini tahmin etmişse false positive olur. (gerçek y=0, tahmin edilen y=1. Positive tahmin edilmiş ama yanlış )Eğer kurt tehdit etmiyorsa ve modelimiz bunu doğru tahmin etmişse true negative olur. (gerçek değer y=0, tahmin edilen y=0)Yani y=1 olma durumu positive, y=0 olma durumu negative.Tahmin ile gerçek değerin eşleşme durumu true, tahmin ile gerçek durumun eşleşmeme durumu false olur.İyi çalışmalar.",
8. ->  ->  Mantığında yanlışlık yok ancak en son paragrafında belirttiğin \"Tahmin ile gerçek değerin eşleşme durumu positive, tahmin ile gerçek durumun eşleşmeme durumu negatif olur.\" önermesi tam olarak doğru değil çünkü positive ve negative burada label ya da classlarımız oluyor. Tahmin ile eşleşip eşleşmeme durumuna göre True veya False olarak adlandırılıyor.İyi çalışmalar, kolay gelsin..",
9. ->  ->  Düzeltme için teşekkürler, ilgili düzeltmeleri yapıyorum.İyi çalışmalar..",
10. ->  ->  Eğer kurt tehdit etmişse ve modelimiz kurdun tehdit etmediğini tahmin etmişse false positive. demişsiniz ancak tabloda bu durum false negative olarak gösterilmiş. Ben de sizin dediğiniz gibi düşünmüştüm. Galiba yanlış düşünmüşüz..",
11. ->  ->  Hayır tabloda yanlışlık yok. Şöyle açıklayayım:- shepherd kurt var derse positive, kurt yok derse negative.- shepherd kurt gerçekten varken kurt var derse true positive, kurt yok derse false negative.- shepherd kurt gerçekten yokken kurt var derse false positive, kurt yok derse true negative.Buradaki yanlış anlaşılmaya tabloya reality açısından bakmanız yol açıyor. Reality değil shepherd'ın söylediğine göre positive ve negative'i yerleştirmelisiniz..",
12. ->  ->  Teşekkür ederim. Şimdi anladım..",
13. ->  Buradaki yazıyı okuyabilirsiniz, yararlı olacağını düşünüyorum. [Link](https://medium.com/@sengul_krdrl/hata-matrisini-anlamak-7035b7921c0f)
"Hata Matrisini Anlamakmedium.comVeri aldığımızda ön işleme , veri temizleme ve kurulduktan sonra ilk adım onu bitmemiş bir modele beslemek ve tabii ki olasılıklarda çıktı…",

### soru

> quest: "Merhaba, tf.keras Linear Regression egzersizlerinin kodlamalarına çalışırken bu hatayı alıyorum. Versiyonumu da kontrol ettim 2.1.  tf.keras.metrics'ler arasında RootMeanSquaredError çıkmıyor. Bu durumu nasıl çözebilirim ? Bunun yanında diğer hatayı da açıklayabilirseniz sevinirim. Sayfadaki kodda da aynen bu şekilde yazıyor. Şimdiden teşekkürler.",

> comments:
  
1. ->  Import ettiğiniz paketlerinizi gözden geçirin muhtemelen importlarınızda bir hata olabilir.Edit: kodunuzu paylaşırsanız daha sağlıklı çözüm yolu sunulabilir..",
2. -> import pandas as pdimport tensorflow as tffrom matplotlib import pyplot as pltdef build_model(my_learning_rate):model= tf.keras.models.Sequential()model.add(tf.keras.layers.Dense(units=1, input_shape=(1,)))model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate), loss=\"mean_squared_error\", metrics=[tf.keras.metrics.RootMeanSquaredError()])return modeldef train_model(model,feature,label,epochs,batch_size):history=model.fit(x=feature, y=label, batch_size=None, epochs=epochs)trained_weight=model.get_weights()[0]trained_bias=model.get_weights()[1]epochs=history.epochhist=pd.DataFrame(history.history)rmse=hist[\"root_mean_squared_error\"]return trained_weight, trained_bias, epochs, rmseprint(\"Defined create_model and train_model\")def plot_the_model(trained_weight, trained_bias, feature, label):plt.xlabel(\"feature\")plt.ylabel(\"label\")plt.scatter(feature, label)x0=0y0= trained_biasx1=my_feature[-1]y1=trained_bias + (trained_weight*x1)plt.plot([x0,x1], [y0,y1], c='r')plt.show()def plot_the_loss_curve(epochs, rmse):plt.figure()plt.xlabel(\"Epoch\")plt.ylabel(\"Root Mean Squared Error\")plt.plot(epochs, rmse, label=\"Loss\")plt.legend()plt.ylim([rmse.min()*0.97, rmse.max()])plt.show()print(\"Defined the plot_the_model and plot_the_loss_curve functions.\")my_feature = ([1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, 9.0, 10.0, 11.0, 12.0])my_label = ([5.0, 8.8, 9.6, 14.2, 18.8, 19.5, 21.4, 26.8, 28.9, 32.0, 33.8, 38.2])learning_rate=0.01epochs=10my_batch_size=12my_model = build_model(learning_rate)trained_weight, trained_bias, epochs, rmse = train_model(my_model, my_feature,my_label, epochs,my_batch_size)plot_the_model(trained_weight, trained_bias, my_feature, my_label)plot_the_loss_curve(epochs, rmse).",
3. ->  ->  Kodunuz sorunsuz çalışmakta, herhangi bir sıkıntı yok, paketlerinizi manuel kurmuş iseniz versiyon sorunu yaşıyor olabilirsiniz, paketlerinizi kaldırıp yeniden kurmayı deneyebilirsiniz..",
4. ->  ->  İlginiz için teşekkür ederim. Tensorflow 1.12'ydi onu 2.1 olarak da güncelledim ama sorunum çözülmedi. Kaldırmayı tekrar denerim..",
5. ->  Pycharm kullaniyor iseniz , keras kütüphanesini ayrı yüklemeyi deneyin. Nedense tensorflow dan import edince saçma hataları bende alıyordum. Ayrı ayrı kütüphaneleri yüklenince ortadan kalktı..",
6. ->  ->  merhaba , muhtemelen ben de bu problemi yasiyorum fakat pycharm da keras kutuphanesini nasil ayrica yukleyecegmi bilmiyorum. nasil yukleyebilirim? teşekkurler.",
7. ->  Bugüne kadar Google Colab,Spyder(Anaconda Distributed) ve Jupyter Notebook (Anaconda Distributed) platformlarında çalışıp bir hata ile karşılaşmadım.Bu platformlardan birinde çalışıyorsanız ve tensorflow versiyonunuzda herhangi bi sıkıntı yok ise, [Link](https://keras.io/) sitesinden optimizer modüllerini inceleyip sorununuzu çözebilirsiniz.
    
### soru

> quest: "Merhaba Arkadaşlar, Genel bir tekrar yaparken ekteki örneği tam anlayamamışım. Yardımlarınızı rica ediyorum. Teşekkürler.",

> comments:
  
1. -> Regülarizasyonda değişkenlere atanan ağırlıkların karelerinin toplamının olabildiğince düşük tutulması hedeflenir. Ancak bunu yaparken ağırlıkları olması gerektiğinden fazlasıyla düşürmek underfitting'e; olması gerekenden az düşürmek (yani regülarisyonda kullanılan lambda değerini 0 veya 0'a yakın seçmek) overfitting'e sebep olur. Bu örnekte ilk şıkta bir tane feature'ın ağırlığını büyük seçmekten bahsediyor. Yukarıdaki açıklamama göre bu istenmeyen bir durum. Ayrıca diğer feature'ların ağırlıklarını 0 değerine eşitlemek onları regresyon denkleminden çıkartmak anlamına gelir. Yani denklemimiz y = B0 + w1* b1 olmakla kalır ki bu da istemediğimiz bir durum. Diğer değişkenleri boşuna denklemde kullanmamıştık. Böyle yaparak muhtemel bir underfitting'e sebep olduk. İkinci şık da ilk şıkka benzer. 1 feature yüksek ağırlığa sahipken diğerlerinin ağırlığı 0'a yakınsak durumda. Tek farkı feature'ları direkt denklemden atmaktansa düşük ağırlıklar vererek model öğrenme kapasitesini sınırlandırmış oluyoruz. Bu da yine bir underfitting ile karşılaşmamızı muhtemel kılıyor.Genel manada düşünürsek; anlamlı bir regülarizasyon için tek ve/veya birden fazla (ancak tüm featurelar değil) feature'a odaklanmak yeterli bir regülarisyon sağlamıyor. Kısa bir örnek ile bitireyim. İlk şık için 4 feature ile açıkladığımız bir denklemimiz olsun ve ilkinin ağırlığı 5 iken diğerleri 0 olsun. Kareler toplamı 25 edecektir. 3. şık için yine 4 feature'umuz olsun fakat ağırlıkları sırasıyla 0.2, 0.5, 0.1 ve 0.2 olsun. Bu durumda kareler toplamımız yalnızca 0.34 edecektir. 3. şıkkın doğru olmasının mantığı da budur.,
2. -> Merhaba,Soruda bir modelimiz ve bu modelimizde birbirine karakteristik olarak benzeyen, korelasyonlu ama birinin küçük miktarda rastgele noise içerdiği feature'ımız olduğunu söylemiş. Eğer bu modeli L2 regulazosyonu ile eğitrsek bu iki feature'In weight değerleri ne olur demiş.1.şık için: Bir feature büyük weight değerine sahip olurken diğer feature'In weight'İ 0 olur demiş. Bu şu yüzden yanlıştır:L2 regularizasyonumuz ağırlıkları neredeyse 0'a yakınsamaya zorlar. Bu yüzden diğer weight 0 olmaycakağından bu cevap elendi.2.şık için: l2 regularizasyonumuz büyük weightleri küçük weightlere göre daha çok cezalandırır ve 0'a yakınsatır. [Link](http://community.globalaihub.com/community/status/1191-1191-1587125026/#comment.4315.4196.4196) linkindeki yorumumdaki resimde gradient descent formülüne bakarsanız thetaj ne kadar büyük olursa düşüşü o kadar fazla olur. (1-alpha rate*(lambda/m)) theta j ile çarpılır thetaj ne kadar büyükse küçülme o kadar fazla olur. Bu yüzden küçük weight değerleriyle büyük weight değerlerinin sıfıra yakınsadıkları değerler arasında uçurum olmaz.3.şık için: Bu sorunun cevabını 2.şıkta verdim. Büyük weight değerleri küçük weight değerlerine göre daha çok ezalandırılacağı için 0'a yakınsadıkları değer aynı olacak birbirine fazlasıyla yakındır.İyi çalışmalar.",
3. ->  Şimdi kavramlar net oldu bende arkadaşlar çok teşekkürler ->  -> .",
    
### soru

> quest: "Bu denklemdeki log(x) in amacı nedir yani içindeki değişkene ne yaptığı için çok fazla kullanıyorlar. Eğer logaritmayla alakalı bi şeyse logaritmanın machine learningdeki işlevinide kısaca açıklayabilir misiniz?. Teşekkürler.",

![image](image/6.jpg)

> comments:
  
1. ->  Merhaba,Burada log kullanmamızın sebebini aşağıdaki resimdeki log loss fonksiyonlarımızın grafikte gösteriminden görebiliriz. Resmimizde kırmızı ile altını çizdiğim kısmı alırsak y=1 ve y=0 iken neden cost function farklılığı olduğunu görebilriz. Lojistik regresyonda amacımız 1 ve 0 arası bir olasılık elde etmek olduğu için cost function'ımızda da 0-1 arasındaki logaritmik bir doğru elde etmeliyiz.(0-1 çünkü sigmoid fonksiyonumuz y ekseninde 0-1 arasında ). Lojistik regresyonda y=1 için sonucu için h(x) sonucumuzun da 1 olmasını bekleriz yani tahmini doğru gerçekleştirmesini bekleriz, eğer h(x) 0 olursa costumuz -sonsuza yaklaşır yani costumuz oldukça artar. Lojisik regresyon problemlerimizi lineer regresyonla çözemeyeceğimiz için farklı bir yaklaşım sergilememiz gerekti. Aklınıza takılan bir şey olursa sormaktan çekinmeyin.İyi çalışmalar.",

![image](image/7.jpg)

2. ->  ->  Merhaba, Niye squared loss oldugunu anlamadim.",
3. ->  ->  Merhaba,Squared loss derken nereyi kastettiniz? Sorunuzu tam anlayamadım. Loss function'da 0-1 arası bir çıktı üreteceğimiz için log kullanıyoruz..",
4. -> Modelin tahmin etmesini istediğimiz değerler 0 ve 1 olduğu için log loss kullanıyoruz.Örneğin, bir input için beklenen output 0 ise ve model 1 tahmininde bulunduysa, log loss'un büyük bir değer üretmesini bekliyoruz. (model kötü bir tahminde bulunmuş.)Bir input için beklenen output 0 ise ve model 0 tahmininde bulunduysa, log loss'un küçük bir değer üretmesini bekliyoruz. (model iyi bir tahminde bulunmuş.)Benzer şekilde, beklenen output 1 ise ve model 0.8 tahminini yaptıysa, log loss nispeten küçük olacak ancak 1 tahmininde bulunsaydı log loss daha da küçük olacaktı.",
5. -> Bunun ana temelini merak ediyorsan matematiksel olarak aynı sonuca çıkmaları. Olay kısaca şuModelinin tahmin ettiği değerlerin doğruluğunu ölçmen için şöyle bir örnek vereyimElinde kırmızı ve mavi noktalar var. Bu noktaları bulunan bir dikdörtgende mavi ve kırmızı noktalara yerleştirmesini istiyorsun. Yani kız ve erkek var diyelim kızları kızlarla gruplandırmak erkekleri erkeklerle gruplandırmak istiyorsun veya.Model 1 ve Model 2 aşağıda ki gibi tahminler üretiyor.( Tahmini Çizgi olarak düşün kırmızı ve mavi alanı ayıran)Buna göre kırmızı noktaların gerçekten kırmızıda olma olasılığı ve mavi noktaların doğru yerde olma olasıkları verilmiş. Bu modelin doğruluğunu anlamak için tüm olasılıkları çarpman ve çıkan sayıya bakman lazım. Örneğin sağda ki modelde kırmızı iki nokta için kırmızı alanda olma olasılkları , ve mavi noktaların mavi alanda olma olasılıkları verilmiş ( Bu model çıktıları olasılıkları)Burda gördüğün üzere daha düzgün modelde bu çarpım daha yüksek çıkıyor , daha kötü tahmin eden modelde daha düşük. Eee hala niye log yapıyoruz dersen olay şu ki burda 4 tane örneğimiz var ve bu örnek normal bir model kat kat fazla ve bu çarpım işlemi bilgisayarı / işlem gücünü gereksiz yorar. Bunun yerine matematiksel olarak aynı işlemi veren log yapıyoruz. Çarpım işleriminin logunu alınca toplama işlemi olur ( Lise matematiği 🙂 ) ve bu işlemi çarpmadan loga çevirdiğimizde logloss ve crossentropy loss gibi kavramlar ortaya çıkıyor (cross entropide log loss'un binary classification dışında kullanmak için) Tabi olasılıklar 1 ile 0 arasında olduğu için bunların logları - sayılar olucak malum log 1 = 0 log 0 = tanımsız , log 1 ile log 0 arasında ki değerler negatif değerler. Şimdi olasılıkların logunu alıunca misal log(0.8) ( bunlar 10 tabanında veya 2 tabanında farketmiyor çok ben 10 diye düşünüyorum şuan)log (0.8) = -0.096log ( 0.2) = -0.69gördüğün üzere sayı değeri düşük olan log , gene daha küçük çıkıyor ( negatif sayılarda - (sayı büyüdükçe - sonsuza yaklaşır )bu sebeple bunların çıktılarını direk karşılaştırırmak zor oluyor negatiflerini alıyoruz ve log'un negatifliğinden kurtuluyoruz. Burdan da toplama yapılınca daha demin gösterdiğim çarpma işleminin benzeri olayla Log sayılarının negatif alınmış toplamı büyük olanlar yüksek çıkıyor buda kötü demek yani . Misal log0.2 negatifi alınca 0.69 , bu tarz kötü tahminler sayıyı yükseltip yüksek bir loss yani fazla hata olmuş oluyorBiraz uzun oldu ama 🙂",

### soru 

> quest: "Quiz gayet güzel ve seviyeli olmuş kod kısmından ziyade mantık odaklı olmuş ki işin en önemli kısmı olduğunu düşünüyorum. Kodların arkadında neler olduğunu anlamak için makine öğrenmesini matematiğini anlamak adına YouTube da İlker Birbil Hocanın \"Makine Öğrenmesi\" dersleri var. Çok fayalı olduğunu düşünüyorum. Linkini burdan paylaşıyorum \"Google Machine Learning Crash Course\" beraber takip ederseniz çok faydalı olacağını düşünüyorum. Herkese iyi çalışmalar diliyorum. [Link](https://www.youtube.com/watch?v=eKrnMr--bDY&amp;list=PLZcbvMjrj9DVU6g2A5e6voeigUtSMsAJH) 

> comments": 
    1. ->  çok teşekkürler, çok güzel bir içerik..",
    2. ->  ->  iyi çalışmalar.",
    3. ->  Kaynak için teşekkür ederim, çok yararlı bir paylaşım olmuş..",
    4. ->  ->  iyi çalışmalar.",
### soru 

> quest": "Merhabalar. Lojistik regresyonda  regularization un extrem önemli olduğu söylenmiş fakat önemini kavrayamadım. \"Without regularization, the asymptotic nature of logistic regression would keep driving loss towards 0 in high dimensions. \" Şu cümleyi matematiksel olarak açıklayabilir misiniz?",

> comments":

1. ->  Merhaba,Lojistik regresyonda hipotez fonksiyonumuz lineer değil sigmoid bir fonksiyon olur. (Resimde görebilirsiniz.) Asimptode fonksiyon olan sigmoid fonksiyonumuz 1 ve 0'a yakınsayarak x ekseni boyunca sonsuza gider. Burada doğası gereği yüksek boyutlarda 0'a yaklaşmaya devam eder dediği budur. Yüksek boyut dediği fazla feature olmasıdır ve fazla feature demek weight değerlerinin fazla ve komplikeye yakın olması demek. Regülarizasyonun weight değerlerini azalttığını biliyoruz bu yüzden kritik bir öneme sahiptir. [Link](http://community.globalaihub.com/community/status/1191-1191-1587125026/#comment.4315.4196.4196) linkindeki gradient descent fonksiyonu lojistik regresyon için de geçerlidir Yalnız tek fark hipotez fonksiyonumuz artık 1/1+e^(-thetatranspose*X vector) yani sigmoid fonksiyonumuzdur.İyi çalışmalar.",
2. ->  ->  Teşekkür ederim. Ben yüksek boyuttan 2 ve daha fazla featurelı datalar yani uzayda çok boyutlu vektörler oluşturan datalar olduğunu anlamıştım.",
3. ->  ->  Merhaba,Aslında orada demek istediğim adet olarak yüksek weight değerleriydi ama yanlış anlaşılmaya mahal vermemek için düzelttim. Düzeltme için teşekkür ederim. Yüksek boyu tdediğimiz fazla feature değerimiz buna bağlı olarak fazla ve yüksek sayıda weight değerlerimizin olmasıdır..",
4. -> ->  high dimensions durumunda loss'un 0'a gitmesini engellemek icin regularization yapilmasinin onemli oldugu belirtiliyor. Training'in amaci loss'u minimize etmek iken (normalde loss'un 0'a yaklasmasini isterken), burada loss'un kontrolsuzce 0'a gitmesini istemiyoruz, cunku bu senaryoda loss 0'a giderken ogrenme yapilamiyor, diyebilir miyiz?.",
5. ->  -> Regularizasyon bizim modelimizin basitlik ile karmaşıklığı arasındaki dengeyi sağlamalıdır. Loss 0'a giderken öğrenme yapılırken overfit olur ve modelimiz eğitim setini ezberler böylece artık test loss'umuz artmaya başlar.İyi çalışmalar..",

### soru 

> quest": "Regularization for Simplicity: Playground Exercise (L2 Regularization)'da bu sonuca nasıl vardık?",

> comments": 

1. ->  Merhaba,Modeli eğittikten sonra featurelar ile output arasındaki bağlantıların üzerine mouse ile gelirsen her bir feature için weight değerini gösterecektir. Verilen task 1'i çalıştırdığında regularization kullanmıyorsun, 500 epoch sonra her bir feature için weight değerlerini bir yere not al. Task2'de regularization rate değeri 0.3 olarak verip 500 epoch sonrası weight değerlerini kontrol et. Weight değerlerinin task 1'e göre 0'a yakın olarak konumlandığını göreceksin.Bunun sebebi L2 regularization işleminin modeldeki kompleksliği azaltması yani weight değerlerini 0'a yaklaştırmasıdır.İyi çalışmalar.",

### soru 

> quest": "Merhaba Arkadaşlar,  Regularization for Simplicity: Playground Exercise bölümünün Task 3'ünün cevabında :  \"Given the randomness in the data set, it is impossible to predict which regularization rate produced the best results for you. For us, a regularization rate of either 0.3 or 1 generally produced the lowest Test loss.\" demiş.  Ama ben regularization rate'i 0.1 yaptığımda 0.3 ve 1'e göre daha düşük test loss'u elde ettim. Ekte görebilirsiniz. Acaba ben mi bir yerlerde hata yaptım. Yardımcı olursanız sevinirim. Çok teşekkürler.",

> comments": 

1. ->  Merhaba,Bende de 0.1 lambda değeri en optimum test loss değerine sahip. Bir hata yaptığınızı düşünmüyorum, Bunun kontrolünü ben de yaptım ve sanırım o kısma yanlış yazılmış. Eğer bunun bir hata olduğunu düşünen varsa bizi düzeltebilir.İyi çalışmalar..",
2. ->  Bende de öyle, Kullandıkları dataset ile ilgili olduğunu düşünüyorum mesela sol üstteki dataset için dedikleri doğru.",
3. ->  tırnak içerisinde belirttiğin ifadeden anlayabildiğim kadarıyla playground exerciselerda generate edilen herbir dataset için belirli oranda bir rastgelelik sözkonusu. o nedenle optimum parametre değerleri değişebiliyor. bende de 0.3 lambda değeri optimum sonucu veriyordu mesela..",
4. ->  Merhabalar, lamda değerini biraz daha küçültmen lazım, görselde farklı lambda değerleri için elde edilen sonuçları ekledim,yeşil olanlardan herhangi biri iyi sonuç veriyor,aradaki fark 0.01 civarında.",


### soru  

> quest": "Merhaba. Regularization kısmında, lambda değerinin büyük olması durumunda modelin underfit olmasının ve tersi durumda overfit olmasının sebebini kafamda oturtamadım. Açıklayabilir misiniz ? Teşekkürler.",

> comments": 

1. ->  Merhaba Furkan,Lambda değeri L2 regularization işleminin sonuca ne kadar fazla etki edeceğini belirleyen hiperparametredir. Lambda değerini genellikle 0 ile 1 arasında konumlandırıyoruz. Eğer lambda değeri 0 olursa regularization uygulanmayacak demektir. Bizim regularization ile amacımız overfitting engellemek, dolayısıyla çok düşük lambda değeri overfittingi engellemeyecektir. Çok yüksek bir değer seçersek de bu sefer modeldeki çoğu weight hızlıca baskılanacaktır ve öğrenme tam anlamıyla gerçekleşemeyecektir, bundan dolayı underfitting oluşabilir.İyi çalışmalar.",
2. ->  ->  Teşekkür ederim..",
3. ->  anladığım kadarı ile regularization için loss ve complexity değerleri arasında bir denge sözkonusu. bu dengeyi lambda değeri ile sağlamaya çalışıyoruz. lambda değerini yüksek vermemiz durumunda model complexitysini azaltmaya odaklanıyoruz ancak daha basit bir model tahmin değerlerinin istenilenden düşük olmasına (underfit) neden olabiliyor. aksi durumda, yani lambda değerini düşük verdiğimiz durumda bu kez basit bir modelden ziyade daha doğru tahmini veriler verebilecek bir modeli önceliklendirmiş oluyoruz. bu da daha kompleks, overfitting nedeni ile test verileri ile daha uyumlu ancak yeni veriler ile tahminde muhtemelen daha başarısız olacak bir modele neden oluyor. bu nedenle lambda değerini optimize etmek gerekiyor diye anladım. yanlış anladıysam düzeltme gelirse sevinirim..",
4. ->  ->  Teşekkür ederim..",


### soru 

> quest": "Merhaba,  Regularization 'da playground exercise kısmında task2'de açıklama kısmında cok fazla feature cross kullanmak eğitim verisindeki gürültüye uyma fırsatı olur buda testte kötü başarıya neden olur mu demek istiyor?Yani training yaparken training verisindeki gürültüye uymaması mı lazım modelin?  Teşekkürler",

> comments": 

1. ->  Merhaba,Modelimiz ne kadar fazla feature içerirse o kadar weight içerir ve daha da komplike olur. Komplike oldukça hipotez fonksiyonumuz eğitim setindeki noise dataları bile ezberler böylece eğitim lossumuz çok düşük olur. Yalnız bu eğitim setimizi çok iyi ezberleyen ve karmaşık olan modelimiz yeni gelen veri tahminini yapamayacaktır çünkü sadece eğitim verisini ezberleyip hipotez fonksiyonunu sadece eğitim setini bilecek gibi oluşturmuştur. (Grafikte eğitim setini çok güzel ve hatasız şekilde ayırmıştır ama yeni gelen veriyi grafikte konumlandırmakta ve analiz etmekte başarılı olamayacaktır.)İyi çalışmalar.",
2. ->  ->  Teşekkürler:).",

> quest: "Regularization kısmında playgroundda regularization ratei arttırdığımda nedense test loss artmaya başladı. Cevapta 0 dan 0.3 e çektiğinde test lossda gözle görülür bir azalma göreceksin diyor.",

> comments:

1. ->  Merhaba,Modeli her eğittinizde aynı training ve test loss değerlerini almazsınız. Muhtemelen tekrar eğittiğinizde bu sonucun değiştiğini göreceksiniz. hger çalıştırdığındaki test ve training loss değişiminin sebebi modelimizdeki verilerin test,train ve validation olarak ayrılmadan önce shuffle edilmesidir.Regularization her gradient descent adımında weight değerlerini daha da 0'a yakındattığı için loss değerleri daha da düşüklük gösterecektir.İyi çalışmalar.",
2. ->  Merhabalar,Hatalı değilsem training data size ve batch size gibi değerlerde değişiklik yaparak test etmişsiniz.Sonucunuzun beklenen şekilde olmayışı bu parametrelerin hatalı seçiminden olabilir, varsayılan olarak gelen değerler ile testinizi yaparsanız azalma olduğunu görebilirsiniz.İyi çalışmalar.,
3. ->  Merhaba,aşağıdaki görsel ile lambda parametresini açıklamaya çalıştım.",
    

### soru 

> quest: "Merhaba Arkadaşlar, Lower Learning rate (with early stopping), güçlü L2 regularization ile aynı etki yarattığını anladım. Ama eklediğim 2 paragrafta tam ne anlatılmak isteniyor anlayamadım. Yardım olabilirseniz çok sevinirim. Çok teşekkürler.",

> comments:

1. ->  Merhaba,Datasetine sürekli olarak bir data akışı varsa yani sürekli yeni example'lar ekleniyorsa datanın trendi zamanla değişebileceği için elinizdeki datayla hatayı iyice düşürecek (converge edecek) kadar train etmiyoruz, training i erken bitiriyoruz çünkü yeni trendi anlayacak kadar elimizde data yok diyor. Bir de yeni gelen data için daha genel bir modelimiz olmuş oluyor ve test loss azalıyor.İkinci söylediği de early stopping ve regularization rate benzer etkilere sahip. Siz optimum regularization rate i bulmak istiyorsanız early stopping etkisi olmamalı, sadece regularization rate etkisi olmalı. Bu yüzden iterasyon sayısını büyük seçin ki early stopping olmasın sadece regularization rate in etkisini görün diyor anladığım kadarıyla 🙂",
2. -> Merhaba,Burada early stopping yani erken durdurmadan bahsetmiş. Erken durdurmak modelimiz eğitilirken modelimiz tam yakınsamadan eğitimi durdurmak demektir. Yazdığınız gibi lower learning rate kullanıp modelinizin eğitmini tam yakınsamadan durdurursanız lambdanın oluşturacağı etkiyi oluşturur. Nerede durduracağımızı da şöyle bilebiliriz: validasyon setimizin loss oranı artıyorsa eğitimi orada bitirirsiniz ki tahminleri optimum bir şekilde yapabilecek overfit olmayan bir model elde edebilelim.Lambda ile learning rate parametrelerinin etkilerini karşılaştırabilirsiniz. Bunu gözlemleyebilmek için ben şöyle bir şey yaptım(Her iki adımda da 1000 epoch ile eğittim):1.Playground'da Regularization:None dedim ve learning rate değerini 0.1'e ayarlayarak eğitim işlemini başlattım. Test ve Training loss değerleri önce düşmeye başladı ama epoch sayısı arttıkça training loss değeri minimum değerde sabit kalıyorken test loss değeri sürekli artmaya başladı. Eğer training loss en minimum değere gelmeden biraz önce early stopping yapsaydık ve eğitimi yarıda kesseydik hem test hem de training loss değerleri optimum seviyede olacaktı.2.Learning rate değerimi 0.1'den değiştirmeden regularization:L2 seçtim ve regularization rate değerimizi 0.1 olarak ayarladım. Eğitimi başlatıp beklediğimde hem test hem training datanın değerlerinin 0'a olabildiğince yakınsadıktan sonra minimum değerde kaldıklarını ve 1.adımımın aksine test loss değerinin artmadığını gözlemledim. Buradan çıkaracağımız sonuç learning rate eğer eğitimi erken durdurursak lambda ile aynı etkiyi oluşturur.İyi çalışmalar.",
3. ->  Çok teşekkürler arkadaşlar ->  ->  . İyi çalışmalar..",

### soru 

> quest: "Merhaba, şurada anlatılan açıklamaları tam anlayamadım. Yardımcı olabilir misiniz?",

> comments:
  
1. ->  Merhaba,Burada bu model karmaşıklığını iki şekilde ele aldığından bahsediyor.-Modelimizdeki weightler model karmaşıklığımızdır. Modelimizdeki weightlerin mutlak değeri ne kadar büyükse o kadar karmaşıktır. Örneğin 25x1+7x2 olduğunu varsayarsak x1 feature'ımızın weight değeri x2 feature'ımızın weight değerinden daha karmaşıktır diyebiliriz.Başka bir örnek olarak -150x1+77x2 dersek yine x1'in weight'i x2'nin weight değerine göre daha karmaşık olacaktır. Zaten regülarizasyon işlemimiz de karmaşıklığı azaltmak yani weight değerlerini olabildiğince 0'a yaklaştırmak. Weight ne kadar 0'a yaklaştırsa o kadar basit olur. (Fazla basit olursa underfitting, fazla komplike olursa overfitting olur.)-Diğer karmaşıklık yaklaşımı ise sıfırdan farklı weight değerlerine sahip feature sayısı modelimizin karmaşıklığı olur.İyi çalışmalar.1 month ago 13 people like this.Like ReportReply",
2. ->  ->  Teşekkür ederim şimdi anladım..",
    
### soru 

> quest: "Bucket enlem ve boylarımızı böldüğümüz zaman binnning yaptığımızda karşımıza çıkan [0,0,0,1,0,0,0] grubunun adı değil mi ya da bu işlemin adı? Şu kısmı ben tam anlayamadım. Bucket ve Crosses Feature arasındaki fark...  Bucket'te mi ayrı bir yöntem yani? Umarım derdimi anlatabilmişimdir. Şimdiden teşekkürler.",

> comments:
  
1. ->  Merhaba,Evet, bucket sizin yazdığınız her bir binary vectordür. Aralarındaki farkı [Link](http://community.globalaihub.com/community/status/519-519-1587052575/#comment.4259.4239.4239) linkinde bunu anlatmaya çalıştım. Sorunuz olursa tekrar sorabilirsiniz.İyi çalışmalar..",
    
### soru 

> quest: "Merhaba Arkadaşlar, Feature Crosses - Playground Exercises kısmında neden X1 ve X2 yerine X12 ve X22'yi seçtiğimizi anlayamadım. Ve X12 ve X22 neyi ifade ediyor tam anlayamadım.  Diğer bir sorum :  \"If you enter a negative value for the feature cross, the model will separate the blue dots from the orange dots but the predictions will be completely wrong. \" ifadesinde neden negatif değer girersek tahminler yanlış olur kısmı tam net oturmadı.  Çok teşekkürler",

> comments:
  
1. -> Merhaba,Burada verimizin dağılımına bakarsak bu problemimiz lineer regresyonla çözülemez yani y=w0+w1x1+w2x2+...+wmxm tarzında bir yaklaşım kullanamayız. Peki bunun için ne yapabiliriz? Elimizdeki feature değerlerindenm sentetik değerler üretmeyi deneyebiliriz veya elimizdeki feature değerlerini evaluate edebiliriz. Ama ben burada biraz matematiksel bir hileye kaçacağım.(Kaçmayacaksam de elimdeki featurelar üzerinden yeni featurelar ve feature değerleri elde etmeye çalışacaktım tabi bunu bir anda yapmayacak hangi feature verilerinin bu işlemlere gireceğini analiz ile belirleyecektim.) Veri dağılımlarını incelediğimizde mavilerin ortada dairesel şekilde toplandığnı ve turuncuların da çevresinde toplandıklarını görebiliyorum. Aşağıdaki resimde dairenin formülünün x^2+y^2=r^2 olduğunu biliyorum. Bu yüzden elde edeceğim yeni feature değeri kareli bir değer olmalı. Burada x12 x1'in karesi, x22 ise x2'nin karesi demek oluyor. Bu nedenle bizim de bu outputta dairesel bir çıktı elde edebilmemiz için featurelarımızın karesini alıp bu problemi linear regresyondan polynomial regression'a çevirmemiz gerekiyor. Bu yüzden x1^2 ve x2^2 kullandık.\"If you enter a negative value for the feature cross, the model will separate the blue dots from the orange dots but the predictions will be completely wrong. \" kısmında ise şunu demek istemiş: Siz eğer x1x2 feature'ına negatif bir değer verirseniz verileriniz yine düzgün ayrılacaktır. Ama negatif değer verdiğinz için grafikte görünen turuncu değerleri mavi değerler olarak, grafikte gördüğünüz mavi değerleri de turuncu değer olarak tahmin edecektir.İyi çalışmalar.1 month ago 25 people like this.Like ReportReply",
2. ->  Çok teşekkürler ->  çok net anladım. İyi çalışmalar.",
    

### soru 

> quest: "Merhabalar, Feature Crosses : Playground Exercises : More Complex Feature Crosses : Task2 de cevabın neden \"Using both x1^2 and x2^2 as feature crosses. (Adding x1x2 as a feature cross doesn't appear to help.)\" olduğunu anlayamadım. Yardımcı olabilirseniz sevinirim.",

> comments:
  
1. ->  Merhaba,[Link](http://community.globalaihub.com/community/status/1133-1133-1587160937/#comment.4376.4241.4241) linkinde bunu açıklamaya çalıştım. Sorunuz olursa sormaktan çekinmeyin.İyi çalışmalar..",
2. ->  ->  Teşekkürler, anlamama yardımcı oldunuz..",
    

### soru 

> quest: "Merhaba.  Bu histogramı tam olarak anlayamadım. Weight Frequency kavramı ile ne temsil ediliyor ?",

> comments:
  
1. ->  Merhaba,Öncelikle lambda'nın weight değerileri üzerine etkisini anlamak için [Link](http://community.globalaihub.com/community/status/1191-1191-1587125026/#comment.4315.4196.4196) yorumunu okuyabilirsiniz.Lambda değeri ne kadar büyük olursa weight değerleri o kadar küçülür yani 0'a o kadar yaklaşır. Aşağıda küçük bir kısmı çıkmış resimde de lambda değerini küçültürseniz histogramınız dikleşmeye başlar çünkü weight değerleriniz çok az bir küçülme gösterir yani 0'a çok az yaklaşım gösterir.İyi çalışmalar.,
2. -> Her çubuk belli bir değer aralığını (min_değer, max_değer) temsil ediyor. Çubuğun uzunluğu ise, weight'lerin kaç tanesinin o değer aralığında olduğunu ifade ediyor.,
3. ->  Teşekkür ederim..",

### soru 

> quest: "Merhaba ben feature crosses kısmını tamamen anladığımı düşünüyorum ama aklıma takılan bir konu var. Şu linkte [Link](https://developers.google.com/machine-learning/crash-course/feature-crosses/crossing-one-hot-vectors\) binned_latitude ve binned_longtitudede her ikisininde 5 elemanı var. ve bunların cross productının sonucu 25 elemanlı bir vektör olmuş oluyor cross product sonucunda normalde aynı boyutta bir vektör bulmamız gerekmezmiydi burda nasıl bir istisna oluştu.",

> comments:
  
1. ->  a = [0,1,2,3,4]b = [0,1,2,3,4]a x b = [(0,0),(0,1),(0,2),(0,3),(0,4),(1,0), ... ,(1,4),..(4,0), ... ,(4,4)]çarpım bu şekilde olduğundan 25 elemanlı vektör oluşuyor..",
2. ->  ->  Verdiğim linkte şöyle bir şey geçiyordu. The term cross comes from cross product. Ben aslında bu yaptığımız işlem 3 boyutlu vektörlerin cross productıyla bir ilgisi olup olmadığını merak etmiştim. Çünkü bildiğim kadarıyla 3 boyutlu vektörün ötesinde cross product alamıyoruz ve iki vektörün cross productı bize yine 3 boyutlu bu iki vektöre dik bir vektör daha veriyordu..",
3. ->  The term cross comes from cross product ifadesinin geçtiği yerin ss'ini gönderebilirmisin ?.",
r. ->  Yanlış link yollamışım pardon. ["Feature Crosses: Encoding Nonlinearity  |  Machine Learning Crash Course](https://developers.google.com/machine-learning/crash-course/feature-crosses/encoding-nonlinearity)
5. >  ->  Burada üçüncü değişkenin ilk iki değişkenin cross product'ı ile elde edildiğini söylüyor..",
6. ->  \"Çünkü bildiğim kadarıyla 3 boyutlu vektörün ötesinde cross product alamıyoruz \" kanısına nereden vardın..",
7. -> Aslında bin'ler arasındaki işlem kartezyen çarpım (cartesian product). Bence \"The term cross comes from cross product.\" ifadesindeki \"cross product\" referansı yanıltıcı olmuş. Zira burada matematikte bildiğimiz anlamıyla \"cross product\" işlemi gerçekleşmiyor..",

### soru {

> quest: "Merhaba, şu kısmı anlayamadım, neden binlere ayırıyoruz ve neden o şekilde encode ediyoruz?",

> comments:
  
1. ->  Merhaba,[Link](http://community.globalaihub.com/community/status/875-875-1586778150/#comment.3974.3871.3871) bu yorumda binning'İ anlatmaya çalıştım. Aklınıza hala soru takılırsa sorabilirsiniz.İyi çalışmalar.,
2. ->  ->  teşekkür ederim.",

### soru 

> quest: "Merhaba arkadaşlar. Ben L2 Regularization ve Lambda kısmını tam anladığımı düşünmüyorum. Yardımcı olabilir misiniz? Teşekkürler ????",

> comments:
  
1. -> Merhaba,Öncelikle regülarizasyon yapmamızın sebebi overfit durumunu önlemek. Gradient descent algoritmamız her seferinde optimum değere yaklaşmak için eğitim modelini validation modele göre ayarlar (yani ilgili weight değerlerinin ayarlamasını yapar.) Burada regularizasyonun görevi komplex modeli cezalandırmak.Kompleks model ne demek?Biz kompleks modelin iki tanımıyla ilgileneceğiz:1. Modeldeki tüm featureların ağırlıkları(weight değerleri)2. Değeri 0'dan farklı olan feature sayısımız.Kompleks modelimizi nasıl cezalandıracağız?Yukarıdaki kompleks model tanımından da anlayacağımız üzere burada weight değerlerini azaltarak modelin karmaşıklığını azaltmaya çalışacağız. Weight değeri yüksek olan featurelar modeli daha da karmaşıklaştırır. Bu yüzden weight değerleri 0'a ne kadar yakınsa model o kadar az kompleks yani basittir. Ama burada şöyle bir husus var. Weight değerlerinin 0'a acayip yakın olması y=w0+w1x1+w2x2+....+wmxm denkleminde tüm w değerlerini 0'a yakınsatacağından bu denklemi sadece y=w0 ele alırsak burada high bias durumu yani underfitting oluşur. Tam tersi durumu düşünürsek de weight değerleri training seti ezberleyecek kadar karmaşık olacağından overfitting (high variance) olur yani tahmin için yeni gelen datanın tahmininde başarısız olur.Cezalandırma işlemi için cost function denklemimize regularization term dediğimiz denklemi ekliyoruz. Yani yeni cost function'ımız resimdeki gibi oluyor.Burada her bir weight değerinin karelerinin toplamının ortlamasını alıp regularization parametremizle çarpıyoruz yalnız w0'ı eklemiyoruz çünkü w0 değerimiz herhangi bir feature'a ait bir weight değil, bias değeri. Bu parametrenin önemi gradient descent adımımıza baktığımızda ortaya çıkıyor. Burada hweight0 dışındaki tüm weightlere regularization uygulanıyor. Eğer yeşil kutucuğa aldığım yerde learning rate'i içeri dağıtıp thetaj parantezine alırsanız thetaj'nin kaysayısının (1-learningrate*(lambda/m)) olduğunu görebilirsiniz. Burada regularization parametresi çok önemli çünkü thetaj'yi 0'a fazla yakın veya fazla uzak bir değer yaparsanız yukarıda bahsettiğim overfitting ve underfitting problemlerine kapı açmış olursunuz. O yüzden burada lambdayı optimum seçebilmek çok önemli.Umarım resim ve anlatımım açıklayıcıdır :)İyi çalışmalar.,
    

### soru

> quest: "Bu koddaki -tf.feature_column.numeric_colum()- gibi fonksiyonların kullanımını ve mantığını bilmemiz şuanlık yeterli midir yoksa fonksiyonları tamamen ezberleyip bu şekilde yazmamız mı gereklidir. Teşekkürler.",

> comments:
  
1. ->  Mantığını ve kullanımını bilmeniz şimdilik yeterli. Zamanla kullandıkça mutlaka ezbere yapabileceksiniz..",
    
### soru 

> quest: "Merhaba, burada posta kodu nasıl daha iyi bir feature oluyor? Arada linear ilişki olmamasına rağmen posta kodlarını kullanırsak linear bir ilişki oluşturmaz mıyız?",

> comments:
  
1. -> Merhaba,Anladığım kadarıyla biz latitude ve longitude değerlerinden spesifik bir koordinat üretmeye çalışıyoruz ama bu koordinat değerleri aslında binlenmiş değerler ve örnek anlamları şu şekilde: 0<lat<60 ve 25<long<77 Yani iki feature'ı (latitude ve longitude) birbiri ile çarptık ve yeni bir feature elde ettik. Yeni bir feature elde etmemiz demek yeni bir weight değeri elde etmemiz demek ve bu da modelimizin daha karmaşık olması demek. (Model kompleksitemiz weight sayısı ve değerleriyle doğru orantılıdır.) Eğer siz aynı işlevi görecek ve binlenmeye (yani kategorize edilmeye) ihtiyaç duymayan numerik bir değer bulabilirseniz hem bu verileri binlemenize gerek kalmaz (ki bunun hem zaman hem space kompleksitesinden de kurtulmuş olursunuz) hem de model kompleksiteniz daha az olur. Neticede posta kodu her mahalle için unique'tir. Burada posta kodu kullanacaksanız ve sizin eviniz iki sokağın köşesindeyse multi-hot encoding yapıp bu değerleri binary vector'e alabilme imkanınız vardır veya tek yerdeyse one hot encoding ile de binary vector feature değerleri elde edebilirsiniz. Yalnız önceki örneğe nazaran iki feature değeri çarpıp 3.bir feature elde edip bu işlemlerimi ona göre yapmaktansa verisetimdeki posta kodu feature'ını yaparak tüm bu işlemleri gerçekleştirebildim.İyi çalışmalar.",

### soru    

> quest: "Ben crossing one-hot vectors kısmında verilen örneği anlayamadım. Feature Crosses kısmında biraz zorlandım galiba...   Cevaplarınız için şimdiden teşekkür ederim.",

> comments:
  
1. ->  binned_latitude ve binned_longitude değerleri belirli aralıktaki enlem ve boylamları temsil ediyor. Bunlara feature cross uyguladığımızda ise, AND operatorü uygulamış oluyoruz. Yani fiyatını tahmin etmek istediğimiz evimizin enlem değeri 15 derece ve boylam değeri 13 derece olsun. Feature cross kullanarak tüm olası enlem ve boylam aralıklarını yazıyoruz ve bizim örneğimizin içinde bulunduğu aralığı 1, diğerlerini 0 yapıyoruz. Örneğimiz; 10 < lat <=20 VE 0 < lon <=15 aralığında bulunuyor. Bunu aynı şekilde one-hot temsili ile gösterip tahmin oranı(predictive power) daha iyi bir feature oluşturmuş oluyoruz.",
2. ->  Anladığım kadarıyla Feature Crosses kısmında yapmaya çalıştığımız şey şuna benziyor; örneğin birinin boy ve kilo verisi var elimizde bu kişinin obezite olup olmadığını tahmin etmeye çalısıyoruz bunun için direk boy ve kiloyu kullanarak oluşturulcak model yerine bu 2 feature'ı kullanarak vücut kitle indexi olan sentetikbir feature oluşturup daha güzel bir temsil yapmayı istiyoruz. Aynı şekilde Feature Crosses kısmında da örnek üzerinde latitude ve longitude tek tek kullanmak yerine crosslayıp işlem yaparak daha iyi bir temsil yapmayı amaçlıyoruz. Hatam varsa düzeltirseniz sevinirm.",
3. -> Şimdi anladım cevaplar için çok teşekkür ederim 🙂.",

### soru     

> quest: "Merhaba, Neden train ve validation loss egrilerinin birbirine cok yakin olmasini saglamamiz gerektigi konusunda bir yerde takiliyorum. Bana cok yakin olmamalari generalization icin daha iyi bir sey gibi geliyor. Yani loss farki fazlayda iki kumenin birbirine daha az benzedigi gibi bir dusuncem oluyor. Bu da generalization icin istenen birsey olamaz mi? Validation set dersindeki programlama egzersizinde train, val, test hatalarinin birbirine cok yakin oldugu bulunuyor. Bu saglanmasi gereken ve ideal bir durum mu?",

> comments:
  
1. ->  Merhaba. Modelimizi train veri kümesi ile eğitirken, validation ve test setlerinde test ediyoruz. Eğer train loss’u düşük, validation loss’u yüksekse ve aralarında çok fark varsa, modelimizin hiç görmediği bir veri kümesinde kötü performans sergilediği için overfit olduğu ve kötü bir model olduğunu görürüz, test setinde test etmemize gerek kalmaz. Asıl amaç, modeli train ettikten sonra val ve train loss curvelerinin yakın olması, ardından test loss curve’nin diğer iki curve’e yakın olmasıdır, generalizationda asıl istediğimiz de bu zaten, overfitting olmaması. İyi çalışmalar.",
2. ->  ->  tesekkurler.",

### soru 

> quest: "Herkese merhaba  Ben Representation with a Feature Crossda kod parçacığına yazılan resolution_in_degrees değişkenini neden kullandığımızı anlayamadım",

> comments:
  
1. ->  Merhaba,Binning yapma işlemi sırasında verdiğiniz boundaries listesine göre numerik değerleri gruplandırma işlemi yapar. Örneğin elinizde yaşlar verileri var ve bunları binning ile gruplamak istiyorsunuz. Sizin boundaries listeniz: [10,20,30,40,50,60,70,80,90] olursa bu sınırlar için gruplandırmalar yapar. Buna göre 76 ile 77 aynı grupta olacaktır. Bunun için ise numpy'In arange metodu kullanılmıştır. np.arange(minimum değer, maksimum değer, minimumdan maksimuma giderken kaçar kaçar artacağı) buradaki resolution_in_degrees değişkeni ise bu artışın kaçar olacağını belirlemek için kullanılmıştır. Verdiğim yaş örneğinde bu değer 10 iken, Representation with a Feature Cross örneğimizde 1'dir.İyi çalışmalar.1 month ago 12 people like this.Like ReportReply",
2. ->  ->  çok teşekkür ederim 😀.",
3. -> ->  Merhaba, anlamadığım şey şu,Bu gruplara bucket denmiyor mu? Şu TASK 4'te takıldım. Teşekkürler 🙂.",
4. ->  Merhaba,Sanırım burada bucket olarak bahsettiği şey iki feature'ın ayrı ayrı binning yapılıp oluşturduğu bucketlar yani binningden sonra oluşan binary vectorler. (Longitude ve latitude featureları ayrı ayrı binning yapıldılar.) Feature cross dediği şey ise longitude ve latitude için bu ayrı yapılan binning featurelarının çarpılması ve ortaya çıkan yeni binary vector değeri. Burada sormak istediği şey ise siz bu featureları çarpmadan ikisini de binning yaparsanız performans ne olur (featurelar ayrı weight değerlerine sahip olursa ne olur?), iki feature'ın binninglerini çarparsanız ne olur? (İki feature'ı çarpıp sentetik bir feature elde ederseniz ve tek bir weight değeriniz olursa performans ne olur?).Umarım açıklayıcı olabilmişimdir :)İyi çalışmalar.",
5. -> ->  Çokk teşekkür ederim Fethi bey....",
6. ->  -> Ben teşekkür ederim iyi çalışmalar..",


### soru 

> quest: "Merhabalar , Representation kısmında one-hot encoding ile multi-hot encoding arasında farkı anlayamadı.Bunun dışında bir de mesela one hot encoding yaparken bütün streetleri sıfır yapıpı sadece bizim kendi feature olarak seçtiğimiz street'i mi 1 yapıyoruz mesela iki tane street verilirse iki tane mi 1 koyuyoruz?",

> comments:
  
1. -> Merhaba,Bunu bir örnek üzerinden anlatmam gerekirse: Elimizde bir veriseti var ve verisetinde sokak isminde bir feature'ımız var. Sokak isimlerimiz string yani kategorik bir veridir. Kategorik verileri modelimizin anlayabilmesi için numerik veriye sokmamız gerekir. Yalnız burada şuna dikkat etmeliyiz. Eğer iki tane sokak ismi içeren bir veri örneğimiz varsa? O zaman işin içine Multi-Hot Encoding giriyor.Sokaklarımızın hepsini One Hot encoding çevirme yöntemimizi kullanarak tekrar numerik veriye çevirdiğimizde bu sefer o sokak isimlerine karşılık gelen feature alanları 1 olmalıdır. Multi Hot Encoding'de One-Hot Encoding'in aksine birden fazla feature değeri 1 olabilir. Yani bir kişinin evi aynı anda iki sokakta da olabilir.Multi-Hot Encoding Örneği(Tek bir kategori seti için örneğin yaş aralıkları):[1, 0, 0 , 1]One Hot Encoding'i ise şu örnekle açıklayabilirim: Elimizde bir veriseti var ve verisetinde yaş isimli bir feature'ımız bu. Bu feature'ı binning kullanarak binary veri gruplarına binary vector ile grupladığımızı düşünün. Bu gruplamalarda yeni feature'ımızın binary vector değerleri yalnızca 1 adet 1 değeri içerebilir çünkü bir kişi aynı anda iki yaş grubuna giremez.One Hot Encoding Örneği (Tek bir kategori seti için örneğin yaş aralıkları): [0, 0, 0, 1]İyi çalışmalar.1 month ago 12 people like this.Like ReportReply",
2. ->  teşekkür ederim.",
    

### soru 

> quest: "Merhaba, bu soruda [binned longitude x binned latitude] ve [binned roomsperPerson] şeklinde iki feature kullanmak doğru cevaptan daha mı kullanışlı olurdu yoksa daha kullanışsız mı?",

> comments:
  
1. -> Merhaba,[binned longitude x binned latitude] ve [binned roomsPerPerson] kullanmak daha kullanışsız olurdu. Çünkü sizin istediğiniz şey longitude, latitude ve roomsPerPerson değerlerinin her biri için ihtimali featurelaştırıp modelnize sokmak çünkü hesaplanacak weight değerleri her ihtimal için farklı olacak. Örneğin sizin verdiğiniz örnek için ([binned longitude x binned latitude] ve [binned roomsPerPerson]):Latitude x Longitude roomsPerPerson37.79 ve 76.51 3Bu durumda latitude x longitude içn ayrı bir weight değeri, roomsPerPerson değeri için ayrı bir weight değeri hesaplanacaktır. Ama [binned longitude x binned latitude x binned roomsperPerson] kullanırsanız durum aşağıdaki gibi olur:Latitude x Longitude x roomsPerPerson37.79 v3 76.51 ve 3Yani bu ihtimal için tek bir weight değeri olur modelinizin hesaplayacağı bu weight değeri diğer ihtimale göre daha tutarlı olacaktır çünkü bu ihtimal için tek bir weight değeri varken aynı ihtimale karşılık gelen [binned longitude x binned latitude] ve [binned roomsperPerson] ihtimalinde 2 weight hesaplancaktır. (Bir tanesi [binned longitude x binned latitude] için, diğeri [binned roomsperPerson] için.)İyi çalışmalar.",
2. ->  bin etmesek ne olur, neden bin ettik.",
3. ->  Fethi nin soyledigi gibi iki weights degeri hesaplanacaktir. Daha kullanisli olur mu sorusunun cevabi hayir daha kullanisli olmaz.Cunku daha fazla weights demek daha fazla hesaplama demektir,.Bu da daha uzun training sureci ve yuksek computational cost demektir..",
4. -> ->  yan.",
5. -> ->  ->  yani daha çaba gerektiren şeyler olsada yinede doğru bir yöntem midir? Kullanışlı olmasa bile?.",
6. ->  -> Merhaba, hayır değildir çünkü bu durumda daha fazla weight sayımız olacak ve modelimizin karmaşıklığı daha da artacak, bu da modelimizi overfitting olmaya daha da yaklaştıracaktır. Burada bu argümanı sunmamın sebebi bu 3 feature'ın ayrı ayrı korelasyonu ile oluşacak yeni sentetik verinin korelasyonu arasında performans farkı vardır. Bu yüzden siz bu 3 feature'I çarpıp tek bir sentetik feature elde edip buna bağlı olarak tek bir weight değeri elde edebilliyorken 3 tame farklı feature kullanmanız hem performans açısından modelin training süresi ve costunu uzatacaktır hem de tahminleriniz label ile bu featurelar arasındaki korelasyonun daha az olması sebebiyle daha da az performanslı oluşacaktır.İyi çalışmalar..",


### soru 

> quest: "One-Hot Encoding kısmını anladım fakat burada anlatılan Sparse Representation kısmında 0 olan verilerin alınmamasını nasıl sağlayacağız ve verisetinde bu işlem nasıl bir etkiyle gözükecek ? Veri setine tablo şeklinde baktığımızda da 0 yerine boşluklar ya da \"Nan\" vb. şeyler mi göreceğiz. Daha önce sorulduysa görmedim kusura bakmayın  iyi günler",

> comments:

1. -> Merhaba arkadaşlar,Sparse representation kısaca şu demek oluyor. One hot encoding bildiğiniz üzere elimizdeki veriyi bir sözlüğe göre indeksleme işlemi şeklinde düşünebilirsiniz.Burada nasıl oluyordu mesela \"Ben bugün çalışıyorum\" cümlesi için \"Ben\" kelimesini kodlamam için (Sözlüğümüzün boyutu -kelime hacmi- 42000 kelime olduğunu varsayarsak). Sadece \"Ben\" kelimesini gösterim içinSözlük boyutunda bir tane dizi tanımlamam lazım ve bu dizininde sadece \"Ben\" kelimesine karşılık gelen indeks değerinin 1 diğerlerinin 0 olduğu bir dizi oluşturmuş olacağız.Diğer 2 kelime içinde aynı işlemi yaptığımızı düşürsek. Dikkat ettiğiniz üzere her bir kelime için sözlük boyutu kadar diziler oluşturuyoruz ve bu dizilerin büyük bir kısmı 0, boş, null veya ne derseniz artık.Böyle matrixlere sparse matrix denir. Bu hem hafız hemde işletim açısından zaman alacağından bu matriksleri daha az hafıza alanı kaplayan matriksler olara gösterebiliriz.Yine aynı cümle üzerinden gidecek olursak Ben sözlüğün 140. bugün 885. ve çalışıyorum 1987. kelimeleri olduğunu düşünelim.O zaman bu cümleyi kısaca [140, 885, 1987] şeklinde tanımlayabiliriz. Bu şekilde 42000 x 3 byte yerine sadece 3 byte hafıza alanı ile aynı gösterimi sağlamış olduk.Ek olarak burada bir açıklamayı faydalı görüyorum. Bu yeni indeksler her ne kadar sayı olsalarda aralarında büyüklük küçüklük vb. matematiksel ifadelerin kullanılmaması gerekir.Çünkü bunlar pointer değerleridir. Kategorik verileridir. Her ne kadar sayı olsalarda sayısal veriler değildirler..",
2. ->  ->  Teşekkür ederim.",
3. ->  ->  Buradan yola çıkarsak her durumda one hot encoding kullanmak yerine sparse representation kullanmak çok daha mantıklıdır diyebilir miyiz? Doğru mu anladım teyit etmek istedim..",
4. ->  ->  Benim anladığım kadarıyla veri kümesi aşırı büyük değilse One-Hot Encoding yeterli olarak görülüyor ama veri kümesi büyük ise Sparse Representation yapıyoruz ki hafıza alanından tasarruf edebilelim..",
5. -> ->  ->  One hot encoding yerine sparse kullanabilirsiniz.Bunun bir sakıncası yok genel olarak kullanım şöyle oluyor diyebiliriz.Encoding işlemi yapılacak olan özellik bir çok değer alıyorsa bunun bu özelliği sparse representation olarak kullanmak daha mantıklı oluyor.Bir örnek ile kısaca açıklayacak olursak 2 özelliğimiz olsun bunlar il ve sokak. İllerde de sadece istanbul Ankara ve İzmir geçtiğini düşünelim.Sokak isimleri ile bu 3 şehirin sokak isimleri olacağından bunu şehir özelliği için one-hot encoding yaparken şehir isimleri için sparse daha uygun olabilmektedir.Sparse'ın bir dezavantajı şu oluyor ilgili indeksin neye referans ettiğini bir yerde saklamanız gerekiyor. Ama one-hot encoding için yeni özellikler (feature'lar) oluşturduğunuz için verinize başka bir look-up table benzeri bir şeye ihtiyaçınız yok..",
6. ->  ->  anladım teşekkürler.",
7. ->  ->  Merhabalar. Tekrar yaparken bu konu hakkında size bir şey danışmak istedim. Önceki yorumlarda örneğin şehir kümesini one-hot yaptığımızda her şehir ismi bir feature oluyor ve bunu,is_istanbul is_bursa is_antalya1 0 0şeklinde gösteriyoruz. Tablodan düşünürsek sparse vektöre çevirince sizindediğinize göre yalnızca İstanbulun indeksi tutuluyor. Peki bu üstte verdiğim örnekteki şekilde tabloya görsel olarak etki ediyor mu? Yoksa sparse olayı sadece arka planda verinin tutulma şekliyle mi ilgili?.",
8. ->  ->  Merhaba açıklama için teşekkürler fakat son kısımdaki ifadenizi anlayamadım --\" Bu yeni indeksler her ne kadar sayı olsalarda aralarında büyüklük küçüklük vb. matematiksel ifadelerin kullanılmaması gerekir. Her ne kadar sayı olsalarda sayısal veriler değildirler.\"-- biraz daha açıklayabilirmisiniz bu sayıların indexleri arasında büyüklük küçüklük ilişkisi kurmayacaksak bilgisayara nasıl o indexte bulunduklarını anlatıcak yani bu indexte bulunan veri aynı zamanda başka bir featurun 1532. indexi ile korelasyon halinde oldugunu biliyoruz ve bu ilişkiyi kaybetmemeleri için index numaralarını kullanmamız gerekirse ne yapmamız gerekir ?.",
9. ->  Merhabalar -> ,Bir örnek ile açıklayacak olursam şöyle söylüyebiliriz. Örneğin Meslek adında bir özelliğimiz olsun. Ve bu bilgi karakter olarak tutuluyor. (Mühendis, Doktor, Öğretmen, Avukat vb.)Bu bilgileri sparse encoding şeklinde kodladığımızda Mühendis için 1, Doktor için 2, Öğretmen için 3 ve Avukat için de 4 numaralı indeksler ile ifade etmiş olduğumuzu düşünelim.Burada ( 4 > 1) Avukat Mühendis'ten büyük değildir. Elimizdeki değerler nümerik ama bunlar arasında matematiksel işlemler yapılmamalı..",
    

### soru 
"question_isim": 1. -> uploaded 1 photo",
> quest: "Sparse gösteriminde, one hot encoding yaptıktan sonra oluşan ve çoğunluğu 0'larla dolu vektörü kullanırken, sadece dolu kısımlarını ele alan bir representation'dan bahsedilmiş. Bu gösterimde (sağdaki tabloda yani) dolu olan kutunun indexi bulunan sütunu feature sütununa mı ekliyoruz? Eğer öyle yapıyorsak modelimizi kötü etkilemez mi? Bu kısmı tam anlamadım. Şimdiden teşekkür ederim",

> comments:
  
1. -> Burası benim de kafamı karıştırdı. Şu şekilde yorumladım; Sparse representation 0 harici değerleri indisleriyle birlikte tutuyor, bunu da vektör çarpımı vb. işlemlerde hız kazanmak için yapıyor (boşuna 0 olan elemanlar ile çarpma yapmamak için) Ancak bunu feature olarak kullanmak istediğimizde tekrardan \"dense\" haline çevrilmesi gerekiyor. Bkz: [TensorFlow Core v2.1.0](https://www.tensorflow.org/api_docs/python/tf/sparse/SparseTensortf.sparse.SparseTensor)
2. -> -> Feature matrisindeki sütun sayımız değişmiyor ve elimizdeki sparse tablosu featurelardan ayrıca bulunan bir bilgi, onu sadece işlem yaparken kolaylık olsun diye kullanıyoruz . Doğru anlamış mıyım?.",
3. -> -> Evet, en azından ben bu şekilde anladım. Mentor arkadaşlarımız da yorumlarsa daha net bilgi edinmiş oluruz..",
4. -> -> Tamamdır teşekkür ederim.",
5. ->  Merhabalar aynı soru olduğundan şurada cevaplamaya çalıştım.[Link](http://community.globalaihub.com/community/status/918-918-1587034561/#comment.4209.4144.4144)
6. -> ->  Açıklayıcı olmuş teşekkürler.",
    
### soru 
j
> quest: "Merhaba,  Feature Crosses Programming Exercise kısmında aşağıdaki parametrenin işlevini anlayamadım;  --------------------------------------- # Create a feature cross of latitude and longitude. latitude_x_longitude = tf.feature_column.crossed_column([latitude, longitude], hash_bucket_size=100) ---------------------------------------  Burada \"hash_bucket_size\" parametresi tam olarak neyi ifade ediyor ve değeri neye göre belirleniyor?  Teşekkürler şimdiden.",

> comments:
  
1. ->  Merhaba,Feature Crossing yaptığnız anda ortaya çıkacak yeni binary vector'daki eleman sayısını temsil etmektedir. Örneğin elinizde resimdeki gibi bir feature crossing output'u olsun. Burada binary vector listesindeki(bu listedeki binary vectorlerin her biri yeni bir feature değeridir çünkü biz iki feature'ı çarptığımızda yeni featurelar elde ederiz.) her bir binary vector hash bucket, her bir hash bucket içindeki eleman sayısı hash_bucket_size olur. Peki bu değeri az verirsek ne olur?Eğer hash_bucket_size'ı 100 değil de 10 verseydik kategorilenmiş verilerimizin çarpım sonuç verktörünün daha küçük bir binary vector'de toplanmasını zorlayacaktık. Bu da birbiri ile alakasız kategorilerin aynı alanda maplenmesine yol açabilirdi. Bu yüzden de modelimiz bu iki alakasız veriyi aynı kategoriden sayacaktı.İyi çalışmalar.",

![image](image/8.jpg)

2. ->  ->  Cevabınız için çok teşekkürler. Peki bu değerin parametre olarak kullanıcı tarafından girilmesinin spesifik bir sebebi var mı? Yani tf.feature_column.crossed_column() fonksiyonuna latitude ve longtitude feature'larını verdiğimizde bu \"hash_bucket_size\" değeri kullanıcıya bırakılmadan da hesaplanabilirdi diye düşünüyorum. Tekrardan teşekkürler..",
3. ->  ->  Merhaba,Anladığım kadarıyla feature crossing verdiğimiz orjinal featureları alıp onları çarpar ve berlirlediğimiz hash_bucket_size kadar bucketlar içine bu çarpılan değerleri yerleştirir.hash_bucket_size değerini vermemiz, bütün ihtimal dahilinde kalan indis sayısını (featurelar çarpıldıktan sonra) bilmeden önce, modelimizi kayıp verme riski olsa dahi oluşturmamıza yaramaktadır. Kaynak: [what TensorFlow hash_bucket_size matters](https://stackoverflow.com/a/45219489/6139104) İyi çalışmalar.",
4. ->  ->  Teşekkürler..",
5. ->  10 buckets for latitude.10 buckets for longitude. Olduğu için hash_bucket_size=100 oluyor. Yanlışım varsa düzeltirseniz sevinirim.,
6. ->  ->  Doğru, 10x10 feature değeri size 100 feature değeri verir yani feature crossing yaptığınızda 100 elemanlı yeni bir binary vector elde edersiniz. Benim resimde verdiğim örnek longitude ve latitufe değerleri için değil, sadece bucket'ı gösterebilmek adına başka bir örnekti. Açıklamanız için teşekkürler, iyi çalışmalar..",
7. ->  ->  Ben teşekkür ederim. İyi çalışmalar dilerim..",
8. ->  ->  Cevabınız için teşekkürler..",
    
### soru 

> quest: "Merhabalar, log ve min fonksiyonları tam olarak ne işlev yaptığı için grafikler değişti ve neden 1.kodda( total_room/population+1) yani neden +1 dedik, 2.kodda  (total_room/population, 4) dedik ,4 ün işlevi nedir. Teşekkür ederim.",

![image](image/9.jpg)

> comments:
  
1. ->  Merhabalar,Log fonksiyonu verisetindeki her roomsPerPerson verisinin logaritmasını alıyor. Logaritması alınan değerler küçülüyor ve küçülen bu yeni değerlerle yeni bir grafik oluşturulduğunda bile hala outlier yani diğer verilerden çok uzakta verilerin olduğu görünüyor. Bunun nedeni ise roomsPerPerson feature değerleri normalde 3-4-5 gibi değerler iken bizim outlier değerimiz 50 ve 5 ile 50'nin logaritması alınsa bile aralarında yine bir açıklık olacaktır.Bizim istediğimiz verisetimizdeki bu outlier sorununu gidermek. Bunun için ise yapabileceğimiz şey roomsPerPerson için kendimiz bir maksimum değer belirlemek (örneğin 4 belirlemiş olalım) ve o maksimum değerden büyük her değeri (örneğin outlier değerimiz 50) belirlediğimiz maksimum değere eşitleyelim.min fonksiyonu ise totalRooms/population işlemini yapıyor ama bu değer 4'ten büyük ise 4 değerini alıyor.İyi çalışmalar..",
2. -> ->  ama sonuçta 4 değeri normal değerlerden yüksek değil mi yinede.. YANİ, dağılımdan uzakta... Burada onun seçilmesinin nedeni nedir?.",
3. ->  -> ->  'nın paylaştığı resme bakarsanız burada değer dağılımlarının 4'ü de kapsadığını ve 4'ün outlier değer olmadığını görebilirsiniz. Siz sanırım logaritma alındıktan sonraki grafik için konuşuyorsunuz. Burada logaritma alıp bu işin içinde çıkacağınızı düşünebilirsiniz ama bu şekilde de outlier sorununu çözemediniz, o yüzden logaritma alma yaklaşımına girmeden cap veya clip yöntemini kullanabilirsiniz denmiş. 4 seçilmesinin belli bir nedeni yok, bu değerin ne olacağı tamamen size kalmış 5 de alabilirdik 3 de 6 da..",
4. ->  aykırılı değerleri onlemek adına 4 den büyük değerler alırsa bu değerler 4 olarak sabitleniyor diye anladım ben..",
5. -> +1 denmesinin sebebi \"log\" fonksiyonunun 0 değeri için tanımsız olmasından kaynaklanıyor. (totalRooms/population) = 0 olması durumunda roomsPerPerson hesaplanabilmesi için +1 değeri eklenmiş, +1 yerine +0.00001 de eklenebilirdi..",
6. ->  -> Merhaba, bu durum için +0.00001 değeri eklenmesi muhtemelen doğru olmazdı çünkü log fonksiyonunda 0-1 arası değerler negatif değerler vereceğinden tablomuz gerçekten uzaklaşmış olurdu diye düşünüyorum..",
7. -> ->  Evet +1 bu örnek için en doğrusu. İlgili feature'ın negatif değer alması bir anlam ifade etmiyor. (0,+] aralığında normalize olması dediğiniz gibi daha uygun..",
    
### soru 

> quest: "Öncelikle merhabalar, representation-qualities of good features kısmında bu ifadeyi tam olarak anlayamadım. Bu kısım ile ilgili bir açıklama yapabilir misiniz? Teşekkür ederim.

![image](image/10.jpg)

> comments:
  
1. ->  Merhaba,Az önce bu kısımları [Link](http://community.globalaihub.com/community/status/190-190-1586982868/#comment.4163.4048.4048) linkindeki yorumumda açıklamaya çalıştım 🙂 Aklınıza bir şey takılırsa tekrardan cevaplayabilirim.İyi çalışmalar..",
2. ->  ->  gayet anlaşılabilir bir yorum olmuş, açıklamanız için teşekkür ediyorum 🙂.",
    
### soru 

> quest: "Merhaba,  Resimdeki kısmı genel olarak anlayamadım.Burada tam olarak ne demek istiyor acaba?  Teşekkürler.

![image](image/11.jpg)

> comments:
  
1. -> Merhaba,Bu kısımda demek istediğini resimedki örnek üzerinden anlatayım.Elinizde bir verisetin var ve verisetinde quality_rating isimli bir feature var. Bu feature'ın değerinini girilmediği veri örnekleriniz olabilir. Siz burada şöyle bir yaklaşımda bulunabilirsiniz:Eğer quality_rating isimli feature'ınızın verisi girilmemiş ise siz el ile -1 atayabilirsiniz ve -1 olan quality_rating feature'ının aslında verisi girilmemiştir diyebilirsiniz. Ama bu yanlıştır çünkü siz -1 koyarak quality_rating feature'nın varsayılan aralığı olan 0-1 arasındaki değerlerden farklı bri değere girdiniz. Bu aynı zamanda modelinizin öğrenme sırasında da performansı düşürecek ve hatalı tahminlere yol açacaktır. Bunun önüne şöyle geçebilirsiniz:Bool tipinde (0 veya 1 alan) bir feature oluşturursunuz ve bu feature değeriniz sizin quality_rating feature'ınızın değerinin girilip girilmediği bilgisini tutar. quality_rating girilmişse yeni bool feature'ınıza 1, girilmemişse 0 verdiğinizi varsayalım. Peki işimiz bitti mi? Hayır ama çok az kaldı :)Eğer sizin quality_rating değeriniz discrete bir değer ise yani alabileceği değerler sınırlıysa: feature değerinin kayıp olduğunu belli eden bir değer ekleyin (örneğin elinizde sadece 0,1 değerleri var ise 2 değerini ekleyip bu 2'nin sadece girilmemiş değerler için kullanıldığını belirtebilrisiniz. resimdeki örnekte quality_rating continuos bir değer olduğu için bu yöntem orada işe yaramaz.).Eğer sizin quality_rating değeriniz continuous bir değer ise: bu kayıp değerlerin modeli etkilememesi için quality_rating feature datalarının ortalama değerlerini girilmemiş kısımlara yazın.İyi çalışmalar.1 month ago 13 people like this.Like ReportReply",
2. ->  ->  feature değerlerimizin continuous veya discrete olması, problemimizin classification veya regression olduğunu belirlemiyor diye biliyorum, yazdığınızdan öyle anlaşılıyor ya da ben mi yanlış anladım ? Ya da siz quality_rating'i bizim tahmin etmek istediğimiz değer olarak aldınız.",
3. ->  ->  Merhaba, anlık kafa karışıklığıyla quality_rating'i label olarak almışım. Düzeltme için teşekkürler 🙂",
4. ->  ->  is_quality_rating_defined bool feature yarattığımız yeterli değil mi? Continuous değerlerde ortalama ile değiştirmeye gerek yok diye düşünüyorum. Bu feature'i işlerken oluşturduğumuz bool feature bakarak bu boş değerlere goz ardi edebiliriz. Neyi etkiler bu eğer değiştirmezsek?.",
5. ->  ->  Bildiğim kadarıyla belli featureları göz ardı etme diye bir şey yok, sadece o feature'ın mensup olduğu veri örneğini göz ardı edebilirsiniz ama ya iki feature'ınız var ise ve biri boş biri doluysa o zaman bu veri örneğini göz ardı etmekten söz edemeyiz. Bu yüzden de bu veri örneğini alabilmek ama alırken de boş olan feature değerinin feature ortalamasına etki etmemesi için ortalamayı feature değeri olarak verebiliriz..",
6. ->  ->  Genel tekrar yaparken aklıma bir şey takıldı sormak istedim. Continuous ise ortalama alıyoruz. Discrete ise o değerin yazılmadığını belli eden bir sınıf oluşturuyoruz. is_quality_rating diye ekstra özellik açmanın gerekliliği nedir? Buradan modelin quality belirlenenlere daha yüksek bir ağırlık verdiği mi anlaşılmalı?.",
7. ->  ->  Merhaba,[Makine Öğrenmesi’nde Ham Veri’den Öznitelik](https://medium.com/@ftfethi/makine-%C3%B6%C4%9Frenmesinde-ham-veri-den-%C3%B6znitelik-%C3%A7%C4%B1karmak-8a6e234ada46) linkindeki yazımın 3.maddesinde bu konuya detaylı değinmeye çalıştım. Eğer aklınıza takılan bir şey olursa sormaktan çekinmeyin.İyi çalışmalar.",
8. ->  ->  Teşekkür ederim okuyacağım..",
    
### soru 

> quest: "Merhaba,  Representation kısmında kategorik değerler için one-hot-encoding kullanıldığını anlatıyor.Bu kullanımda feature için tek bir ağırlık var değil mi ?Bu ağırlık bu binary vector ile çarpılınca mesela [0,0,5.0] yada [5,0,0,0] oluyor ikisi içinde 5 değerimi toplanacak?  Birde seyrek temsilden bahsediliyor.Bunu nasıl kullanacağımızı anlamadım.Her bir kategorik değer için sayısal değer atıcaz fakat her biri için farklı ağırlıklar elde edicez olarak anladım ,ama peki nasıl her biri için farklı ağırlık elde edilcek?  Teşekkürler",

> comments:
  
1. -> Merhaba Buse,One-Hot-Encoding yaptığında bahsettiğin vektördeki her bir değer ayrı bir feature oluyor ve farklı weight değerleri alıyorlar. Örneğin elindeki datasette şehir adlarının olduğu bir feature var ve içerisinde [\"İstanbul\", \"Ankara\", \"İzmir\", \"Bursa\"] kategorik değerleri var. One-Hot-Encoding yaptığın zaman bu değerler datasete kolon olarak eklenirler. [\"İstanbul\", \"Ankara\", \"İzmir\", \"Bursa\"] kolon adları olmak üzere eğer satırda bu değerler geçiyorsa 1, geçmiyorsa 0 olur. Mesela ilgili satırda \"Ankara\" var ise [0,1,0,0] olur. Bu değerleri feature olarak eklediğimiz için her birinin weight değeri farklı olacaktır, çünkü normal şartlarda her bir şehir adının sınıflandırma sonucuna etkisi farklı çıkmalıdır.Seyrek temsil olarak bahsettiğin yani sparse represantation için ise yukarıda verdiğim örneğe ek olarak tüm dünya şehirlerini kullandığımızı düşünelim. Dünya üzerinde yaklaşık 2.5 milyon şehir var ve her biri için bir kolon oluşturup sadece ismi geçen şehri 1 ile işaretleyip diğerlerini 0 yapmak pek mantıklı değil.Bu gibi durumlarda kullanılıyor ve sadece 1 olan yani ilgili satırda geçen kategorik değerler gösteriliyor..",
2. ->  ->  Sparse representationın ne olduğunu tam olarak anlayamadım sadece kelime sayısı milyonları bulduğunda one hot encodinging yetersiz kaldığı durumlarda kullanılıyor diye anladım sparse repin kullanımı aklımda pek canlanmadı. İnternettede baya karmaşık formüller var yanlış anlamadıysam sparse rep. konusu sözlü olarak anlatmaya pek müsait değil?.",
3. ->  ->  Seyrek temsil için verdiğiniz örnekten gidecek olursak sadece Ankara şehir kolonunda 1 olan bir örnek için sadece \"Ankara\" feature'ı mı tutuluyor? Geri kalan kolonlara 0 değerleri feature engineering adımlarında mı ekleniyor?.",
4. ->  ->  Seyrek gösterimi (Sparse representation) şu şekilde açıklamaya çalışayım. Seyrek gösterim veri setinde sadece sıfır olmayan özelliklerin (feature) gösterildiği özellikle büyük (big data) veri setlerinde kullanılan bir gösterim şeklidir. Görsel ile açıklamaya çalıştım.",
5. ->  ->  Teşekkürler.Peki sparse representation 'da 1000 kategorik değer varsa 1000 feature eklemiyo muyum yani?Bu kısmı gene anlayamadım..",
6. ->  ->  Ama her örnek için O olmayan değer değişecek.Mesela şehir bir örnekte Ankara,diğerinde İzmir?Gene hepsi için feature olması lazım yoksa her example için aynı feature mı olacak yani?.",
7. -> ->  evet yine 1000 feature ekleniyor ancak sparse matrix olarak ekleniyorlar. Tensorflow üzerinde çalışırken bunun ile ilgili metodlar var.Bildiğin gibi bir yazılım dilinde(Python için konuşalım) variable içine matris tanımlarsan (Numpy ile) onun uzunluğu kadar hafızada yer kaplayacaktır. Eğer elimizde bahsettiğimiz ölçüde büyük bir matris var ise memory üzerine yazılamaz çünkü matris içerisindeki 0'lar da yer kaplar. Bu noktada sparse matrixler kullanılır, bu matrisler farklı mapping teknikleri ile 0 olan verileri boş olarak alır, sadece değer içeren veriler matris içerisinde depolanır. Dolayısıyla memory problemi oluşmaz. Sparse matrixleri kullanmak için Python'da Scipy kütüphanesini kullanabilirsin.Aşağıda bu konu ile ilgili yazıları bulabilirsin ancak Machine Learning için şimdilik bu konuların detaylarını bilmek zorunda değilsin, merak ettiğin için paylaşıyorum :)Farklı Sparse Matrix türleri:[Link](https://matteding.github.io/2019/04/25/sparse-matrices/)
8. -> One hot encoding yaptığımızda bir nitelikten, nitelikte bulunan düzey sayısı kadar yeni nitelik elde ederiz. Örneğin şehir niteliğimizde İstanbul,Ankara ve İzmir düzeyleri olsun. Şehir niteliğine one-hot encoding yaptığımızda is_İstanbul, is_Ankara , is_İzmir şeklinde yeni nitelikler elde ederiz.Örnek olarak şöyle bir verimiz olsunbuyukluk (m2), şehir, oda_sayisi, fiyat135 , İzmir, 3 , 270000buna one-hot encoding uyguladığımızda verimizdeki nitelikler ve değerleribuyukluk (m2), is_İstanbul, is_Ankara, is_İzmir, oda_sayisi, fiyat135 , 0, 0 , 1, 3, 270000e dönüşür. Artık modele girecek veri gösterimi son elde ettiğimiz gösterim. Yani her bir nitelik için ayrı bir ağırlığa sahip olacağız. Dolayısıyla is_İstanbul, is_Ankara ve is_İzmir niteliklerinin ağırlıkları birbirinden farklı olacak.",
9. ->  Fatih Bey'in örneğinden devam ederek anlatayım, farkettiyseniz burada sayı ile ölçülemeyen özelliğimiz \"şehir bilgisi\". One hot encoding ile şehir çeşidi kadar yeni özelliği (yani sütunu) tablomuza eklediğimizi düşünün, yani her bir şehir ismi için tabloya bir sütun daha ekliyoruz, bunlar İstanbul, Ankara ve İzmir için birer yeni sütun oluyor örnekte. Bizim gözlemimizdeki (yani satırımızdaki) ev hangi şehirde ise o sütunumuz 1, diğerleri 0 oluyor. Bu şekilde gözlemimizin o özelliğini kodlamış oluyoruz. Ben en basit haliyle bu şekilde kullanıyorum. Umarım anlatabilmişimdir.",
    
### soru 

> quest: "Arkadaşlar merhaba. Ben \"Feature Crosses\" bölümünü etkinliklere rağmen genel olarak anlayamadım. İnternet üzerinde faydalı bir site veya video şeklinde bir kaynak bulamadım. Bu konuyla ilgili bir site ya da herhangi bir kaynak bilgisi olan var mı? İyi çalışmalar 🙂",

> comments:
  
1. ->  Merhaba,[Link](http://community.globalaihub.com/community/status/774-774-1586937745/#comment.4123.4009.4009) linkinde Feature Cross'u genel hatlarıyla kabaca anlatmaya çalıştım. Eğer bu kısımda sorularınız eksik gördüğünüz veya anamadığınız yer olursa bu post altından daha açıklamalı halini yazmaya çalışabilirim. Ayrıca Andrew Ng'nin Coursera'daki Machine Learning kursununda da bunun açıklaması mevcut, o kursu da önerebilirim.İyi çalışmalar.",
2. ->  Çok teşekkür ederim..",
3. ->  Ufak bir sorum daha olacak : Feature Crosses bölümünün Playground Exercise kısmındaki Task 1'de learning rate'den bahsedilmiş, orada \"non-linear\" model'de learning rate'in yakınsama değerine etkili olmadığı mı anlatılmaya çalışılmış?.",
4. ->  ->  Merhaba,-> 'nün de dediği gibi lineer modelleme bu verisetini efektif bir şekilde modelleyemiyor. Learning rate değerini değiştirmemiz lossumuzu yine azaltacaktır yani etkileyecektir ama en kadar azaltırsa azaltsın loss halen kabul edilemez düzeyde yüksek bir değere yakınsar, optimum değere yakınsayamaz. Yani learning rate yakınsamamızı etkiler ama yeterli düzeyde etkilemez.İyi çalışmalar..",
5. ->  Merhaba Nil Hanım, bahsettiğiniz egzersizin \"Task 1\" bölümünde linear modeli verilen şekliyle çalıştırılması istenmiş. Verildiği şekliyle de \"feature\" olarak sadece x1 ve x2 var yani feature cross olmadan verilen data seti linear olarak modelleyebilir miyiz'in cevabını sorgulamamız isteniyor. Learning rate loss'u azaltsa da verilen data setten linear bir model çıkarmamız söz konusu değil anladığım kadarıyla..",
    
### soru 

> quest: "Merhaba, Representation: Cleaning Data bölümündeki, Scaling feature values başlığındaki maddeleri ve floating-point teriminin ne olduğunu anlayamadım, yardımlarınız için teşekkür ederim."

![image](image/12.jpg)

> comments:
  
1. -> Merhaba,1- Siz bir feature alanını scale ettiğinizde gradient descent algoritması, feature değeri daha da küçüldüğünden dolayı daha hızlı optimum değere yakınsar.2-Scale etmek NaN tuzağını engellemenize yarayacaktır. NaN tuzağı kavramı şudur, Eğer sizin verisetinizde NaN (gerçek hayatta sayılar sonsuzdur ama bilgisayar donanımının kısıtladığı bir sayı sınırı varıdr. Bunu aşması durumunda NaN olacaktır. Peki nasıl aşabilir? Büyük sayılarla matematik işlemleri yaparsa bu sayı NaN olabilir. O büyük sayıyı küçültmek ve belli bir range'e sokmak için scale yapılabilir.) bir veri var ise modeldeki diğer sayılarımızda eninde sonunda NaN olacaktır.3-Scale etmeniz modelinizin her bir feature'ınız için ilgili weight değerlerini öğrenmesi konusunda yardımcı olacaktır çünkü scale edilmemiş geniş aralığa sahip verilerde model weighr değerini öğrenmek için daha çok kaynak ve zaman harcayacaktır.Floating point kavramı aslında reel sayıların bilgisayar alanındaki adıdır. Bu kavrama reel değil de floating-point denmesinin sebebi ise sayı içerisindeki ondalık noktasının kayabilme özelliğinden dolayıdır. Gerçek dünyada sayılar sonsuza kadar giderken, bilgisayar ortamında bilgisayar donanımının getirdiği sınırlamalardan dolayı bütün sayıların gösterilmesi mümkün değildir. Bununla birlikte gerçekte sonsuza kadar giden birtakım değerler bilgisayar ortamında ortamın kapasitesine bağlı olarak yaklaşık değerlerle temsil edilirler. Bu sınırlamaların etkisini en aza indiren, sayıların maksimum miktarda ve gerçeğe en yakın şekilde temsilini sağlayan sisteme \"Kayan-Noktalı Sayılar\" sistemi denir. Kaynak ve daha fazlası için: [Kayan-Nokta](https://tr.wikipedia.org/wiki/Kayan_nokta) adresini inceleyebilirsiniz.İyi çalışmalar.
2. ->  ->  Açıklamanız için teşekkür ediyorum 🙂.",
3. ->  ->  peki scale işlemini nasıl yapıyoruz yani tam olarak neyi scale ettiğimizi ve neye göre sınırları belirlediğimiz anlayamadım.",
4. ->  ->  Feature olarak kullanacağımız veri aralıklarına bakıyoruz. Zaten tek bir feature'ımız var ise scale etmemiz gerekmiyor, birden fazla feature'ımız var ise değer aralıklarını karşılaştırabiliriz. Örneğin ev metrekaresi ve oda sayısı featureları değer aralığı olarak aynı düzlemde olmayacaktır. (ev metrekaresi 50-300 arası skalada diyelim, oda sayısı ise en fazla 10 olsun diyelim.) Bu durumda gradient descent fonksiyonumuzun daha hızlı çalışması için bu iki feature'ı benzer değer aralıklarına sokmamız performansı arttıracaktır. Benzer dememin sebebi örneğin ev metrekaresi feature'ını scale edip 50-300 aralığından -3Özellik Ölçekleme ve Normalleştirme (Feature Scaling and Normalization)",
5. ->  ->  Teşekkür ediyorum çok güzel açıklıyorsunuz ????galiba yarısı silinmiş yazının.",
6. ->  ->  Merhaba, yorumunuz için teşekkür ederim 🙂 evet edit diyince yazının tamamı geliyor ama save diyince yarısı gidiyor teknik bir problem var sanırım 🙂 Sizin için şöyle bir şey yapabilirim ama, edit dediğimde çıkan yazının tamamını ekran görüntüsü alıp atıyorum. Gözlerinizden şimdiden özür diliyorum, iyi çalışmalar diliyorum 🙂.",
7. ->  ->  olur mu öyle şey 🙂 saolun kafamdaki soru işaretlerini giderdiğiniz için, kolay gelsin..",
    
### soru 

> quest: "Merhabalar. Regularization bölümünün son kısmında check your understanding 2. sorusunda sıkıntı yaşadım. Cevap olarak 3 . şıkkı seçtim verilen bir feature içinde noise olduğu için. Anlatabilecek birisi varsa çok sevinirim.",

> comments:
  
1. ->  Merhaba Mehmet,Regularization ile kastedilen modelin karmaşıklığını azaltmaktır. Burada bahsedilen L2 ve daha sonra karşılaşacağın L1 regularization ile modeldeki weight değerleri mümkün olduğunca azaltılır. L2 regularization formülü gereği en yüksek weight değerini hızlı, düşük weight değerini ise yavaş şekilde 0'a yaklaştırır ancak tam olarak 0'a eşitlemez. İleride göreceğin L1 regularization ise bazı weightleri 0 yaparak model kompleksliğini azaltır.\"One feature will have a large weight; the other will have a weight of almost 0.0.\" seçtiğin şıkta bir feature'ın yüksek weight değerine sahip olacağı diğerinin ise 0'a yakınsayacağını söylüyor. Ancak L2 regularization yukarıda da açıkladığım gibi yüksek weight değerini daha hızlı bir şekilde azaltır ve modelde yüksek weight değeri kalmaz çünkü yüksek weight değeri modelin karmaşıklığına en çok katkı yapandır.",
    
### soru 

> quest: "Herkese merhaba. Crossing One-Hot Vectors kısmını tam oturtamadım yardımcı olursanız sevinirim 🙂",

> comments:
  
1. -> Merhaba,Öncelikle Feature Cross yapmamızın sebebi modelimizin verileri tek bir lineer çizgiyle ayıramamasıdır. Bu yöntemle yeni bir feature elde edip bu yeni feature modelimizin eğitim sırasında verileri daha etkili ayırıp daha etkili bir hipotez fonksiyonu elde etmesinde yardımcı oluyor. Örneğin elinizde \"dil\" ve \"ülke\" kategorik featureları olsun.Örneğin dil feature değerleri: \"Türkçe, İngilizce, Japonca\"Ülke değerleri de: \"Türkiye, İspanya, Kanada\" olsun.Bu iki kategorik veriyi One Hot Encoding kullanarak binary vector'e çevirdiniz ki modelimiz numerik veri üzerinde çalışabilsin.Eğer siz bu iki feature'ı yani iki binary vector değerini çarparsanız elinizde 9 elemanlı bir binary vector olur. Bu binary vector değerlerinden her biri bir ihtimali temsil eder. Örneğin:[Türkçe ve Türkiye,Türkçe ve İspanya,Türkçe ve Kanada, İngilizce ve Türkçe,.......] gibi.Siz ilgili ihtimalin olduğu indeksteki değere 1 koyduğunuz anda artık o eğitim örneği için o değer geçerlidir. Örneğin [1,0,0,0,0,0,0,0,0] yaptığınızda artık Türkçe ve Türkiye değerini o eğitim örneği için değer belirlemiş olursunuz. Buradaki amaç featureların tek tek tahmine katkısından daha çok katkı sağlamalarını sağlayabilmek. Örneğin dil ve ülke featureları kend başlarına feature olarak katkı sağlarlar ama iki feature'ı çarpıp elde ettiğimiz yeni feature tahminde daha çok katkı sağlayacaktır.İyi çalışmalar.1 month ago 19 people like this.Like ReportReply",
2. ->  ->  Doğru anlıyor muyum? Dil ülke örneği ile modelimizin verileri lineer olarak ayıramadığı durumda, featurelar çarpıldı ve yeni bir çıktı elde ettik(binary vektör). Dağınık halde olan verileri daha düzgün bir hale getirip tekrar bir doğru ile ayırıp ayıramadığımıza bakıyoruz bu şekilde. Peki bu yeni feature'ımız mı oldu yani diğer featurelarla birlikte nasıl değerlendiriyoruz bu durumu?.",
3. ->  ->  Evet, bu çarpılan feature sizin yeni feature'ınız oldu. x3 feature'ı olduğunu düşünelim. x3=x1 (x) x2 olsun. Yani x1 ve x2 feature'ımızın çarpımı ile x3 sentetik feature'ımızı elde edelim. Bu durumda y=w0+w1x1+w2x2 olan hipotez fonksiyonumuz artık y=w0+w1x1+w2x2+w3x3 olacak ve bu hipoetizimin doğru olarak gösterimi de bize verileri daha tutarlı ayırmış olacak. [Feature Crosses: Playground Exercises ](https://developers.google.com/machine-learning/crash-course/feature-crosses/playground-exercises) playground'ında w1 ve w2 ağırlıklarımızı 0, w3'ümüzü 1 olarak aldığımızda aslında yeni oluşturduğumuz feature'ın tek başına güzel bir veri ayırması yaptığını görebiliyoruz. Bu da yeni oluşturduğumuz feature x3'ün, x1 ve x2 featurelarından daha efektif çalıştığını gösteriyor.",
4. ->  ->  teşekkür ettim, çok iyi açıklıyorsun..",
5. ->  ->  teşekkür ederim 🙂.",
    
### soru 

> quest: "Herkese merhabalar. Feature crosses bolumundeki en son sorulan soruda (Check your understanding kisminda) neden one feature crossu diger featurelarinin binned halini alarak olusturduk. Binned hali olmadan alsak nasil bir sonuc dogurur? Binning konusunun uygulamasi kafamda tam oturmadi acikcasi. Bu konuda yardimci olursaniz cok sevinirim. Tesekkurler.",

> comments:
  
1. ->  Merhaba, bir şehirdeki boylam derecemiz 30 olsun ve kişi başına düşen oda sayısı da 1 olsun. Diğer şehirde de boylam derecesi 20 ve kişi başına düşen oda sayısı yine 1. Şimdi bunları binning yapmadan feature crossing yaparsak, derece ne kadar yüksekse biz ona daha çok değer biçmiş oluyoruz. Binning yaptığımızda, sadece şehrin bulunduğu boylamı 1, diğerlerini 0 şeklinde ifade ettiğimiz için, boylamın derecesine göre karar vermemiş oluyoruz.",
1. ->  ->  Ben de rooms per person özelliğinin neden bin yapıldığını anlamadım. Enlem ve boylam da gerekliliğini anlatıyor. Rooms per person diğer iki özelliğe göre daha stabil ve güvenilir. Bu kısmında 50 gibi bir outlier değer vardı onun etkisini kırmak için mi yine bin yapmamız gerekti ?.",
1. ->  ->  Sanırım o outlier değerlerini bu işleme gelmeden temizlemiş oluyoruz. Dediğiniz gibi rooms per person bin işlemi diğerleri kadar net görülemiyor. Bin yapılmayıp normal değerleri ile kullanılsaydı, roomsperperson = 1 ve roomsperperson = 2 değerleri için; 2 olan değere 2 kat önem vermiş oluyoruz ve ister istemez w değerine müdahele etmiş oluyoruz. Belki de fiyat olarak aralarında net olarak 2 kat fark yok. Onu modelin vereceği ağırlıklar ve hesaplayacağı hata ile kendisi tespit etmesini istiyoruz. Ağırlığa müdahale etmemiz bizi doğruluktan uzaklaştırabilir diye düşünüyorum.",
1. ->  ->  teşekkürler benim için faydalı oldu açıklama 🙂.",

### soru 

> quest: "Herkese Merhaba, umarım dolu dolu bir machine learning crash course haftası geçirirsiniz. Ben tam olarak Representation kısmındaki \"Account for upstream instability\" kısmını anlamadım. Yardımcı olursanız sevinirim.iyi akşamlar.",

> comments:
  
1. ->  Merhaba,Burada demek istediği feature'ınızın değeri zaman içerisinde değişiklik göstermemelidir yani Stationary durumda olmalıdır. Örneğin siz her şehirdeki oy oranına bakacaksınız ve her şehir içn o şehri tanımlayacak bir belirleyici feature'a ihtiyacınız var. Bu feature ismine sehir_id dediniz ve 1234567 değerini atadınız. Bu değer ilerleyen zamanlar değişkenlik gösterme potansiyeline sahip. Örneğin verisetinize yeni şehir eklendiğinde veya çıkartıldığında. Bunu yerine sehir_id feature değerinizi \"tr/istanbul\" yaparsanız bu değerin değişmeyeceği neredeyse kesindir (biri kalkıp da şehrin adını değiştirmezse).İyi çalışmalar.",
2. ->  ->  anladım teşekkürler. iyi çalışmalar..",
    
### 1. hafta sınavı 
  
    
### soru 

> quest: "Merhaba Arkadaşlar,  Representation: Feature Engineering bölümünde kategorik değerlerimize sayısal değerler tanımlayıp doğrudan modelimize dahil ettiğimizde sorunlu olabilecek bazı kısıtlamalar olduğundan bahsedilmiş. Buna istinaden eklediğim iki maddeyi tam olarak anlayamadım. Yardımcı olursanız çok sevinirim. Teşekkürler"

![image](image/13.jpg)

> comments:
  
1. -> Merhaba,1.Örneğin sizin sokak featureınız var ve string değerinde yani kategorik bir veri. Siz dediniz ki ben her bir sokak ismi için bir index kullanacağım yani ilk sokak için 0, ikinci sokak için 1 diye böyle gidecek. 10 sokağınız var ise son indexi 9 olur bu durumda. Burada ilk maddede diyor ki güzel index kullandın ama örneğin siz o feature değeri için ilgili weight'i 6 buldunuz bu sefer durum şöyle olacak 1.eğitim örneği için 6x0 2. eğitim örneği için 6x1......... 10.eğitim örneği için 6x9 (lineer regresyon formülündeki w (x) x kısmından). Burada sizin her sokak için bir weight değeri öğrenmeniz gerektiğinden bahsediyor çünkü sokakların hepsinin labelımıza farklı bir etkisi olur ama siz az önceki şekilde yaparsanız bütün sokaklar için aynı weight bulmuş olur ve sokakların labela etkisini gözlemleyemezsiniz. Buradaki indeksleme yöntemi her katgorik değeri bir int değere maplemekti. One Hot Encoding veya Multi-Hot Encoding kullanırsak bütün bu ihtimalleri featurelara ayırırız ve bu featurelardan her birinin değeri binary bir vector olur.2.Yukarıda sokak isimleri labelımız tek bir sokak ismi alyıyormuş gibi düşündük ve ona göre indeksledik. Bazen sokak featureımız string tipinde iki tane sokak ismi alabilir.(Örneğin evimiz iki sokağın köşesindeyse.) Eğer siz yukarıdaki gibi bunlar için de indeks kullanırsanız bu veriyi encode edemeyeceğimizden bahsediyor.İyi çalışmalar.",
2. ->  Çok teşekkürler ->  şimdi daha iyi anladım. İyi çalışmalar.",
    
### soru 

> quest: "Merhaba, Tensorflow'u pip ile yüklemek istediğimde böyle bir hata alıyorum. Çok yanlış bir şey mi deniyorum? Numpy ve Pandas'ı bu şekilde yüklemiştim. Pip sürümüm günceldir.  Yardımcı olabilir misiniz? Teşekkürler,",

> comments:
  
1. ->  sanırım versiyon söylemenizi istiyor.pip install tensorflow==2.0.0.",
2. -> eğer versiyon tanımı sorunu çözmez ise bunlara bakabilirsiniz python versionunuz nedir acaba? python 3.5 veya 3.7 versiyonlu olması gerekiyor. Şurada bir açıklaması var. [Link](https://www.tensorflow.org/install) ayrıca eğer bu versiyonları kullanıyorsanız \"pip3 install tensorflow\" olarak da bir dener misiniz? Bir de python2 kullanmayı tercih ediyorsanız ([Link](https://www.tensorflow.org/install/pip) şu link'te olduğu gibi tensorflow versiyonunu seçmeniz gerekiyor. Ayrıca şu siteden python3 indirebilirsiniz [Link](https://www.python.org/downloads/windows)
3. ->  Teşekkür derim Python 3.8.2 kullanıyordum o yüzden olmadı sanırım. Şimdi 3.7.7 indirdim ve pip ile versiyon belirtmeden yükleme yapabildim.Çok teşekkür derim,.",
4. ->  Anaconda kullanmak yeni başlayanlar için en iyisi. Kendisi paketler arasında uyumlulukları hallediyor ve gereklilikleri kuruyor..",
    
### soru 

> quest: "Merhaba arkadaşlar,  Geçen haftanın konusu ile ilgili bir yerde takıldım. Tecrübesi olanlar yardımcı olabilir mi?  Elimdeki bir veri setine eğitim gerçekleştiriyorum ancak bir türlü loss değerinde sürekli azalmayı yakalayamadım(converged olmuyor). Aşağıda denediklerimden 3 örnek görseli paylaşıyorum. Çok daha fazla denemem oldu. Geçtiğimiz haftadan nasıl yapmam gerektiğini tam anlayamadığımı fark ettim. Öğrenme oranını düşürdüm, epoch sayısını artırdım ve batch size ı azalttım. Grafik hep tırtıklı şekilde çıkıyor. Bunlardan kurtulmak için anladığım kadarıyla kesin geçerli bir yol yok. Parametre ayarlamalarını yaparken bildiğiniz dikkat edilmesi gereken başka noktalar nelerdir? Teşekkürler.

![image](image/14.jpg)
![image](image/15.jpg)
![image](image/16.jpg)

> comments:
  
1. ->  Batch-size'ı düşürmek osilasyon yapmasına yol açabilir. Stochastic gradient descent yani batch-size=1 aldığınızda daha fazla osilasyon yapacağını tahmini ediyorum. 128-256 gibi sayılar denediniz mi ?.",
2. ->  ->  Mantıklı geldi dediğiniz. Batch size büyüdükçe daha az tahmin gerçekleştirecek. Ama yine de olmuyor..",
3. ->  ->  Sürekli azalmasından ziyade, hatanın düşük olması daha önemli değil mi ? Hata miktarı yeterince düşük gibi geldi bana. Azalıp artmasından çok, en son ulaştığı noktada ne kadar az hata değerine ulaştığı daha önemli diye biliyorum. Pratik konusunda daha tecrübeli arkadaşlar belki daha iyi yanıtlayabilir, her zaman teorik bilgi ile pratik bağdaşmıyor. Belki sizin istediğiniz gibi daha iyi bir sonuç elde edilebilir 🙂.",
4. ->  ->  Şurada sürekli azalmalı diyor ama.",
5. ->  ->  Bence orada demek istediği genel olarak azalan bir grafiğe sahip olması, sizin grafik için konuşacak olursak 0.04 e gelip sonra 0.08 e tekrar atlıyorsa o zaman sorun var demektir. Ama 0.0310 dan 0.312 ye çıkarsa, bunda bir sorun yok diye düşünüyorum. Sizin çizdirdiğiniz grafiğin ölçeği de böyle gözükmesine sebep oluyor. Hatayı 0-100 arası çizdirirseniz büyük ihtimal bu titreşimler hiç gözükmeyecektir. Siz başka kaynaklarda gördüğünüz sürekli azalan grafikleri ölçeği 0 - 0.10 aralığına düşürüp incelerseniz büyük ihtimal onlarda da bu tarz dalgalanmalar olacaktır..",
6. ->  ->  steadily kelimesini sürekli olarak değil de yavaş yavaş ve düzenli bir şekilde olarak çevirebilirsin..",
7. ->  Modelinizin eğitiminde herhangi bir sorun görünmüyor. Bu kadarcık sizin deyiminizle tırtıklı olması çok normal. Epoch sayısı arttıkça azalma gerçekleşmiş ancak belli bir yerden sonra adımlar arası fark o kadar az ki grafikte net bir şekilde belli olmuyor. Bu azalışı net bir şekilde görmek istersen özellikle 0.04 değerinden düşük değerler için y ekseninde ki noktalara çok yakından bakabilirsin (yanlış hatırlamıyorsam matplotlib'de ticker fonksiyonu ile yapabilirsin )..",
8. ->  Evet bence de bazen daha kötü sonuçlara izin vermesi gerekir ki lokal minimumlardan kurtulabilsin. Örnekteki grafik sürekli azalan şeklinde olunca sormak istedim. Teşekkürler cevaplar için. Biraz araştırınca şu linke denk geldim. Çok faydalı oldu sizinle de paylaşayım: [Link](https://machinelearningmastery.com/learning-curves-for-diagnosing-machine-learning-model-performance/)
    
### soru 

> quest: "Merhaba.Representation kısmında verileri one hot encoding yöntemi ile kullanıyoruz fakat weight özelliğini nasıl kullanacağımızı anlayamadım.",

> comments:
  
1. ->  One hot encoding kullandığımızda örnekte verilen evin bulunduğu sokak değeri 1 oluyor, kalan sokaklar 0 oluyor. [0 0 0 1 0 0] şeklinde bir vektör oluşmuş oluyor (boyutları rasgele yazdım). Ve weight değerimizle çarparken sadece tahmin etmek istediğimiz evin bulunduğu sokak hesaba katılmış oluyor. 0 olan diğer sokakları değerlendirmeye katmamış oluyoruz (w * 0 = 0 olacağından). Umarım soruyu doğru anladım 🙂",
2. ->  ->  evet doğru anlamışsınız :). fakat bu değerleri sayısal olarak nasıl kullanıyoruz ?.",
3. ->  ->  evin fiyatı örneği için linear regression kullandığımızda, y' = w1*x1 + w2*x2 + .... + wn * xn + b şeklinde bir formulümüz vardı (n = feature sayısı). Bizim sokak ismi feature'ımız x2 olsun. (x2 * w2) -> sadece istediğimiz sokak için sayısal bir değer üretecektir. w2 değerimiz 5 ise oradan elde ettiğimiz değer yukarıda yazdığım formülde sadece o sokak için katkı sağlayıp modelin o sokağın ev fiyatını tahmin ederken ne kadar etkili olduğunu anlamasını sağlayacaktır. weight değerleri ilk olarak 0 veya random olarak belirleniyor. model training kısmında evin gerçek fiyatını görüp hatayı hesaplıyor. weight değerlerini hatanın azalması için gradient descent kullanarak güncelliyor. Biz belirlemiyoruz weight değerlerini. Son kısımları bütünlük oluşturması açısından yazdım, diğer arkadaşlar okurken daha faydalı olabilir.,
4. -> ->  Güzel açıklama teşekkürler. Çok ufak bir ekleme yapayım. Eğer bir ev 2 sokağın kesişiminde (2 sokağın kesiştiği köşede) bulunuyor ise vektörümüzde bahsi geçen 2 sokağın değeri de 1 oluyor. Vektör de örneğin [ 0 0 0 0 1 0 1 0 0] gibi bir şey oluyor. Ana regresyon denklemimizde de 2 farklı katsayının etkisini dikkate alıyoruz.,
5. ->  ->  teşekkür ederim gayet iyi açıklamışsınız 🙂.",
6. ->  Merhaba, benim anladığım kadarıyla; one-hot encoding yöntemi sayısal olmayan raw data ları feature a çevirip eğitimde öğrenilen weight ile çarpabilmek için kullanılan bir yöntem. Yani aslında önce verileri feature a çevirip ki bunu yaparken one-hot encoding yöntemi de kullanılıyor sonra eğitim esnasında weight leri buluyoruz. Umarım bende doğru anlamışımdır..",
7. ->  Çalıştığınız kısımda anlatılan, model weightlerinin anlamlı olması için neden kategorik değişkenlerde one-hot encoding yöntemini kullanmamız gerektiği. Bu anlayışla o kısma tekrar bakarsanız net bir şekilde anlayacağınızı düşünüyorum..",
    
### soru 

> quest: "Merhabalar herkese, keyifli ve verimli haftalar ???????? Ben sparse representation ı tam olarak anlamadım. Çok fazla farklı değer olan kategorik featurelarda multi-hot encoding yaparsak çok büyük bir vektörde çoğu 0, birkaç elemanı 1 olan bir gösterimin pek doğru olmayacağını, bunun için sparse representation yapılabileceği söyleniyor. Bunun için vocabularydeki her değeri indexliyor ve 1 milyon eleman tutmak yerine değeri 0 dan farklı elemanların değerlerini ve bu elemanların index ini tutuyor.  Benim anlamadığım index bilgisini training işleminde nasıl uyguluyor? Index için de ayrı bir weight mi kullanılıyor?  Teşekkürler ????",

> comments:
  
1. ->  Merhabalar,Doğru anlamışsınız. Yeniden bir indexleme yapılmıyor. Sadece elimizde tutacağımız dataları indexleri ile birlikte alınıyor. Ancak index bilgisi benzersiz olması sebebi ile ML için kullanılamamaktadır. Eğitime katkı sağlamaması sebebiyle veriden eğitim aşamasına gelmeden çıkartılır diye biliyorum.İyi çalışmalar. 🙂",
2. ->  Bu gösterimin teorik temeli seyrek matrisler (sparse matrices ). [Sparse matrix](https://en.wikipedia.org/wiki/Sparse_matrix) linkinden ekstra inceleme yapabilirsiniz.",
3. ->  Teşekkürler 🙂 ->  -> .",

### soru 

> quest: "Merhaba,  \"There's a Goldilocks learning rate for every regression problem. \" Goldilocks ile learning rate baglantisini, Goldilocks learning rate kavramini biraz acabilir misiniz?",

> comments:
  
1. ->  Merhaba,Goldilocks kavramı ve learning rate'in rolü [Link](http://community.globalaihub.com/community/status/1043-1043-1586253928/) postunun altında açıklanmıştı. Oradan bulabilirsiniz.İyi çalışmalar dilerim..",
2. -> gozden kacirmisim, tesekkurler..",
    

    
### soru 

> quest: "Merhaba. Binning ile feature'ı belirli sınırlara bölüp, bu sınır sayısı kadar yeni feature elde edip, bu featureları da binary vectorler ile mi ifade ediyoruz?",

> comments:
  
1. -> Merhaba,Bunu basit bir örnekle anlatabilirim. Örneğin sizin elinizde yaşları farklı 10 tane insan olsun. Bu insanların yaşları numerik veridir ve siz binningde bunu kategorik veriye çevirirsiniz ki feature ile label arasında lineer bir bağıntı oluşabilsin. Örneğin bu insanların yaşlarına göre diyabet olup olmadıklarını tahmin etmek istiyorsunuz bunun için ise elinizde her yaş grubu için belli diyabet risk değerleri var örmeğin 50-60 arası riskli siyabet 40-50 arası az riskli diyabet gibi. Burada yapmanız gereken şey bu yaşları kategorik veriye çevirmek. Bunun için Elinizde 10 tane insan var ise bu insanların yaş aralıklarını bulup bunları kategorik veriye çevirirsiniz (örneğin 19 yaş için 10lar, 28 yaş için 20ler.... gibi) Burada sınır sayısı kadar yani böldüğümüz kategorik veri sayısı kadar feature'ımız oldu. Bu featurelar binary vectorler ile ifade edilir örneğin 10lar feature'ımızın value'sı binary vectordür ve karşılığı sırasıyla [10lar,20şer,30lar,40lar,50ler,60lar,70ler,80ler,90lar] olacaktır. Eğer biz 19 yaşındaki bir insanı bu binary vectör ile ifade edeceksek [1,0,0,0,0,0,0,0,0] olarak yapmalıyız ki bu insanın hangi kategoride olduğunu belli edebilelim. (10- yaş arasında). Umarım açıklayıcı olmuştur eğer olmamışsa ekstra olarak şu linki de inceleyebilirsiniz: [Link](https://www.youtube.com/watch?v=iv_ec0EfXcE&t=204s) . Eksiğim veya hatam varsa düzeltilmelere açığım 🙂Machine Learning Tutorial 10 - Binning Datawww.youtube.comBest Machine Learning book: [Link](https://amzn.to/2MilWH0) (Fundamentals Of Machine Learning for Predictive Data Analytics). Machine Learning and Predictive Analyti...",
2. -> ->  Niçin yaş gruplarını 10-19, 20-29 vs. gibi gruplara ayırıyoruz. Örneğin; 10-14, 15-19, 20-24 vs. gibi ayırsak ne olurdu. Buna sezgisel mi karar veriyoruz?.",
3. ->  -> Merhaba, buna sezgisel karar verebilirsiniz. Örneğin yeni bir hastalık tipi beşerli yaş grupları için değiiklik gösteriyorsa bunun için yaşları beşerli gruplara ayırabilirsiniz..",
4. -> ->  O halde bir soru daha sorayım. Eğer bu aralıklarımızın sabit olduğunu (yani asimetri var) düşünmüyorsak yani 10-19, 20-29 gibi gruplar yapıyorken belli bir yaştan sonra (Örneğin 60 yaşından sonra grupları 60-64, 65-69 şeklinde yapmamız gerekiyor olsun) bu grupların daralması gerektiğini düşünüyorsak nasıl bir yol izlemeliyiz..",
5. ->  -> Merhaba, [Link](https://medium.com/hacktive-devs/feature-engineering-in-machine-learning-part-1-a3904769cd93) linkinde 3 şekilde binning yapabileceğimizden bahsediyor. Benim yukarıda bahsettiğim binning çeşidi Fixed-Width Binning çeşididir. Yani binning aralıklaları arasında sabit bir katsayı vardır. Sizin dediğiniz yöntem de gerçekleştirilebilir buna Binning By Instinct(İçgüdüyle binning) denir. Burada bin range aralıklarını siz belirliyorsunuz. Toparlamam gerekirse, dediğiniz yöntem için Binning By Instinct kullanabilirsiniz.",
"Feature Engineering in Machine Learning (Part 1)medium.comHandling Simple Numeric Data with Binning.",
6. -> buraya ek olarak bir şey sormak isterim, çok saçma olabilir ama kusura bakmayın lütfen, peki bunları bir sokakta yaşayan, yaş aralıklarını, diyabet durumlarını(az riskli, çok riskli, risksiz) ve yaşadıkları yerleri(latitude cinsinden) barındıran bir raw values olarak düşünürsek, yaş aralıklarını ve yaşadıkları yerleri binning ile diyabet olma durumlarını ise on-hot-encodingle kategorileştirip hepsi için yeni feature'lar (binary vectorler) elde ederek, bu sayede train modeli daha iyi tahmin verileri çıkarsın diye mi eğitiyoruz? Yoksa bunlar tamamen bu örnekten farklı, ayrı ayrı konular mı?1 month ago Like Reply Edit",
7. -> -> Makine öğrenme algoritmaları doğrudan kategorik veriler üzerinde çalışmamaktadır bu yüzden verilerimizin sayısal verilere dönüştürülmesi gerekmektedir.Öncelikle binning feature ile numerik verimizi kategorik karşılıklarına dönüştürüyoruz. Binning kullanarak yaşlarımızı 10-20,40-50 gibi kategorilere ayırıyoruz ki amacımız verideki gürültüyü ve non-linearity durumunu azaltıp modelimizin generalization oranını arttırmak. Yaş için konuşursak şu an elimizde 10-20,21-30.. kategorilerine karşılık gelen booelan featureları oldu. Yani bir kişi 18 yaşında ise 10-20 yaş aralığı feature'ı 1, diğerleri 0'dır. Bir eğitim örneği için bu yaş kategorilerinden aynı anda ikisi de 1 olamaz. Örneğin bir kişi hem 27 hem de 37 yaşında olamaz. Bu yüzden bu One Hot Encoding olarak geçer.Yaşadıkları yer için de tahminime göre bu değerleri de binning ile kategorik verilere sokup labelımızı bu kategorileşmiş veriler üzerinden yapmamız, yani regression problemimizi classification problemine çevirmemiz gerekiyor.Diyabet olma durumunu ise evet One Hot Encoding ile kategorileştirip bunu da numerik gösterime sığdırabilmek için binary vector elde ederiz ve tüm bu işlemlerimizin amacı kategorik verileri numerik veriye sokmak ve generalization'ı yani veri tahminini arttırmaktır. Gözden kaçırdığım veya eksik noktam varsa düzeltmekten eklemekten çekinmeyin 🙂 İyi çalışmalar.,
8. -> ->  çok teşekkür ederim,
    
### soru 

> quest: "Merhabalar, Validation and Test sets kısmında, loss eğrileri arasındaki farkı azaltmak için, veri setini shuffle etmemiz gerektiği anlatılmıştı(longtitude a göre azalan indekslendiği için, split ederken train ve validasyon setinin içeriği benzer olmuyor, bundan dolayı shuffle etmemiz gerekiyor), shuffle ettikten sonra da eğrilerin birbirine yakın konumlandığını görüyoruz. Ancak, shuffle edilmemiş kısımda validation_split i 0.4 e çekerek de aradaki farkı azaltabiliyoruz. Bu tamamen elimizdeki veri setine göre gerçekleşen rastlantısal bir durum mudur? İki yöntemin birbirinden farkı nedir teknik olarak? validasyona ayırdığımız veri miktarının yüzdesini artırmak genel olarak tercih edilmemesi gereken bir yol mudur? Teşekkür ederim şimdiden yorumlar için.",

> comments:
  
1. -> Merhaba,Shuffle edilmemiş veride calidation boyutunu değitirerek loss eğrilerini birbirine yakınlaştırabiliyoruz. İlk durumda train loss 70, validation loss 90 olsun, validation setin boyutunu arttırdıkça, train'in loss değeri artacak, validation'un azalacak yani 80 civarlarında bu iki eğri yaklaşık olarak birbirlerinin aynısı olacak. Ki bu durum loss da azalma yapmamakta aksine artışa sebep olmakta. Bu yüzden validation seti optimum boyutta tutarak, train seti olabildiğince büyük tutmayı amaçlıyoruz.Sıralı veri setinden rastgele örnek çekmiş olsak bile sıralı veri çekmiş oluruz. Bu durumda da çekmiş olduğumuz örnek ile train setimizi sağlıklı bir şekilde ifade edemeyiz.(Train ve Test setleri aynı dağılımdan seçilmeli - Generalization Assumption 3). Bu sebeple veri setimizi karıştırıyor sonrasında örneklem çekiyoruz. Böylece verimizi genelleyebileceğimiz bir örnek çekebilme ihtimalimiz sıralı veride olduğundan daha fazla olacaktır.Loss Eğrilerinin grafiğini inceleyerek bahsettiğim durumları gözlemleyebilirsiniz.İyi çalışmalar..",
2. ->  Validation ve train datasetleri icin shuffle = True dedigimizde randomization saglar ve modelin ezberlemesinin onune gecilmesine yardim eder.Tabiki ezberlemeyi yani overfitting i sadece shuffle = True diyerek engelleyemeyiz ve overfitting ortaya ciktiginda validation loss ile test loss arasindaki fark buyuk egriler uzak olur.Validation ve test loss egrilerini birbirine yaklastirmak demek validation sirasinda modelimizi check ederken aldigimiz iyi sonuclari(umarim iyi sonuclardir) test sirasinda da elde etmek, parallel sonuclara sahip olmak demektir.Yani bir anlamda overfittingi onlemek demektir.Iyi generalization demektir.Bunu yapmanin yolu validation setini buyutmek train setini kucultmek filan degildir..",
    
### soru 

> quest: "Merhaba arkadaşlar,  Öncelikle umarım herkesin sınavı verimli olmuştur, yeni hafta için de şimdiden iyi çalışmalar :)  Correlation matrix üzerine biraz düşündüm de, output ile yüksek corr. değerine sahip olanlar çok önemli onları kesinlikle traininge dahil etmeliyiz ama hem output hem de diğer feature’lar ile 0a yakın corr değerine sahip olan bir feature’ı traininge dahil etmeye gerçekten gerek var mı sorusu kafamı kurcaladı.  Ama sonuçta corr matrix bize aralarındaki bütün ilişkiyi vermiyordu sadece artış-azalış ilişkisi ile ilgili bir bilgi alıyorduk bu da feature’lar arasındaki farklı bir ilişkinin outputu etkileme ihtimalinin bu matrix ile keşfedilememe olasılığını ortaya çıkartıyordu.  Bunu bir örnekle açıklamam gerekirse mesela a b c featurelarımız ve y output olsun, (a-b)+c’nin outputa eşit olması gibi bir durumda a ya da b’nin bütün corr değerleri 0 olsa bile kendi aralarındaki farkın outputa etkisi olduğunu görebiliyoruz, yani corr matrix bir işimize yaramıyor. Peki bu gibi durumlarda hangi feature’ların elenmesi hangilerinin traininge dahil edilmesine nasıl karar vermeliyiz?  Zaman ayırdığınız için çok teşekkür ederim.

> comments:
  
1. -> Overfit'i engellemenin yollarından birisi \"removing useless/irrelevant features\". İlgisiz özelliklerin modelde kullanılması modelin verimini düşürebilir. Bu feature'lar ya veri setinden düşürülüyor ya da daha ilişkili olabilecek hale dönüştürülüyor.google crash course'da california housing data'yla çalışırken toplam oda sayısını nüfusa bölerek \"kişi başına düşen oda sayısı\" feature'ını oluşturduğumuzda bunu yaptık. Eğer modeli tek yerine çok sayıda özellik kullanarak eğitiyor olsaydık, \"evin kapısının rengi\"(mesela) gibi bir kolonu model dışında bırakmamız gerekirdi.Dolayısıyla, yanlış yorumlamıyorsam, bu üzerine çalışılan alanı iyi bilmemizi gerektiriyor. Çünkü, belki de kapının rengi bu bölge için zenginlik/lüks belirtisidir:Ör.Afrika'da birkaç sene önce yoksul hanelere yapılacak yardımların doğru kişilere ulaşmasını sağlamak için bir derin öğrenme çalışması yapıldı. Uydu fotoğrafları yorumlanarak yardıma talip bölgedeki evlerin ne kadarının metal çatılara sahip olduğu ayırt edildi. Böylece bu bölgenin diğer bölgelere kıyasla ne kadar fakir/zengin olduğu ayırt edildi. [Link](https://www.liebertpub.com/doi/pdf/10.1089/big.2014.0061) [Link](https://www.liebertpub.com/doi/pdf/10.1089/big.2014.0061www.liebertpub.com)
2. ->  Soruya detaylı bir yanıt veremeyeceğim fakat ufak bir ekleme yapayım; feature'lar arasındaki correlation çok yüksekse bunun tahminimiz üzerinde ekstra bilgi taşımadığını ve bu feature'ların elenmesi gerektiğini biliyorum. Mesela bir feature diğerinin 2 katı ise, correlation değeri 1 olur ve bir tanesini silmek verimlilik açısından faydalı oluyor..",
3. ->  ->  Bence de corr değeri 1 ise o feature'lardan birini silmek gayet mantıklı bir karar olur sadece ufak bir ekleme yapmak istiyorum, anladığım kadarıyla corr değeri bize artış/azalış katsayısını değil, olasılığını veriyor.(verdiğiniz örnekte bu yanlış anlaşılmalara yol açabilir diye eklemek istedim).",
4. ->  Merhabalar,Bu konu ile alakalı yakın bir geçmişte araştırma yapmak fırsatım oldu. Karşılaştığım yöntemlerin büyük çoğunluğu deneyerek buna karar vermekte. Diğer kısmı ise modelde kimin kalacağına istatistiksel testler yaparak karar verip sonrasında elde ettikleri modelleri deneyerek etkinlikliklerini ölçmekte.Sorunuza cevap verecek olursam: Deneyerek karar verebilirsiniz..",
5. ->  Herkese cevapları için çok teşekkür ederim, şu ana kadar yapılan yorumlardan elde ettiğim çıkarımlar; üzerine çalışılan alana göre bilgi sahibi olmak gerekebiliyor ya da deneyerek önemli/önemsiz feature'ları tespit etmek. Ben aynı zamanda şunu da merak ediyordum acaba bunları tespit etmek için belirli algoritma veya matematiksel yaklaşımlar var mı?.",
6. ->  ->  Istatistiki yaklasimlar var.kisa cevap: Sormus oldugun sorunun temeli Multiple Regression Analysis. Bu isimle google'laya bilirsin ya da talep edersen sana link gonderebilirim.Konuya ne kadar hakimsin bilemiyorum ama oncelikle sunu vurgu yapmak gerekiyor, verdigin ornek ve uyguladigin correlation matrix multiple regression konusunun ogeleri, yani coklu aciklayici degisken ile bir bagimli degiskenin tahmini. Cok degiskenli regresyonlarda regresyonun tahmin gucu ana konudur. Bunun da olcutu R-Squared denen bir parametre. Yalniz regresyona her buldugun degiskeni atma sansin yok ne yazik ki; kurgulanmasinda kistaslar bulunuyor. Ornegin aciklayici degiskenler arasinda dogrusal bir iliski bulunmamali (collinearity). Bu tarz degiskenlerden sadece birini denkleme katabilirsin ya da degiskenleri arindirdiktan sonra kullanabilirsin..",
7. ->  ->  Multiple Regression Analysis ile ilgili daha detaylı araştırma yapacağım. R-squared parametresi yukarıda anlattığınıza göre RMSE ile aynı görevi görüyor yani tahmin gücünü anlamamıza yarıyor, bir farkı var mıdır acaba soruyorum çünkü eğer pek bir farkı yoksa üzerinde durmanızın ve RMSE yerine onu kullanmanızın sebebini tam anlayamadım, teşekkürler..",
8. ->  ->  Fark var. RMSE mevcut durumu acikliyor. Verilen degerlere bagli olarak su kadar ya da bu kadar basarili demek icin var. R-squared dagilimin aciklayicilik gucu var. Yani (dagilimda bir degisiklik olmadigi surece) modelin yeni verileri de ne kadar aciklayabildigini ifade etmek icin kullaniliyor. Su soylediklerim birbirinin aynisi ya da ayni seyin laciverti gibi geliyor kulaga belki ama degil. Ikisi de modelin gucuyle mi alakali, evet. Ayni seyler mi veya ayni amaca mi hizmet ediyorlar, hayir. Yazilim gecmisinden geldigini varsayarak, su soylediklerimi icsellestirebilmek sanirim istatistik ile biraz hasir nesir olmayi gerektiriyor, onu da sana ben veremiyor olabilirim. Suraya iki link birakayim, benim anlattiklarimdan daha faydali olurlar sanirim.RMSE: [R-squared](https://www.statisticshowto.com/rmse/R-squared) [Coefficient-of-Detetmination](https://www.statisticshowto.com/probability-and-statistics/coefficient-of-determination-r-squared/)
9. ->  [Feature Selection](https://scikit-learn.org/stable/modules/feature_selection.html) [1.13. Feature selection — scikit-learn 0.22.2](https://scikit-learn.org/stable/modules/feature_selection.html)
10. ->  Charles Wheelan'ın çıplak istatistik kitabında bu gibi durumlar için Regresyon Analizini(diğer tüm değişkenlerin aynı şartlar altında sabit tutularak, elimizdeki değişkenin(feature) etkisini ölçmeyi) öneriyordu. İyice araştırmak gerekiyor sonuç olarak \"etkisi olabilir de olmayabilir de\" diye bir sonuca varıyorsunuz 🙂 Yine domain hakkında bilgi sahibi birisine danışmak da hızlı bir çözüm olabilir diye öneriyordu..",
11. ->  ->  teşekkürler, denemek ya da domain hakkında bilgi gerekliliği sorunun popüler cevapları 🙂.",
12. ->  ->  Yani elimizde birden fazla sayıda ve farklı modeller için farklı optimal feature selection algoritmaları var gibi görünüyor. Müsait bir zamanımda tabloda adı geçen algoritmalara bi göz atıcam, teşekkür ederim..",
13. ->  ->  Evet dogru.Rica ederim..",
14. ->  Merhaba, çoklu regresyon analizinde yeni bir bağımsız değişken(özellik) modele dahil olursa R^2 ya aynı kalır ya da artar. Ancak bu modelimizin daha iyi açıklama oranına sahip olduğu anlamına gelmez. Açıklayıcılığın artıp artmadığına bakmak için R^2 adjusted değerine bakmamız gerekir. R^2 adjusted modele yeni bir özellik eklendiğinde modeldeki parametre sayısını ve örneklem sayısını dikkate alarak R² üzerinde ayarlama yapar. Modele yeni parametre ekledikçe R² değeri yükselir ancak modelin karmaşıklığının azaltılması için modelin en az değişken ile açıklanması beklenir. Bu nedenle modele yeni değişkenler eklendiğinde gereksiz eklenen değişkenleri cezalandıran R_adj² kullanılması gereklidir. Şurada daha detaylı göstermeye çalışmıştım: [Kategorik Veriler ile Çoklu Regresyon Analizi](https://medium.com/@cerden/kategorik-veriler-ile-%C3%A7oklu-regresyon-analizi-minitab-uygulamas%C4%B1-e30f74a9b73d)
    
### soru 

> quest: "Selamlar, Bu soruyu yanlış yaptım açıklaması mevcut mu, teşekkürler.  Which of the following is prevent overfitting ? 1 - Cross-validation 2- Training with more data 3- Removing features 4- Ensembling",

> comments:
  
1. ->  [Link](https://elitedatascience.com/overfitting-in-machine-learning)Bu adreste oldukça iyi açıklanmış.",
2. -> Merhabalar,Overfit durumuna düşmemek için alınabilecek önlemleri sorulmuş, 4 maddede bu önlemler arasında mevcut.Her birini kısaca açıklamaya çalışacağım.1. Cross Validation: Veri setini k tane parçaya ayırarak eğitimi yapar, bu k parçadan 1 parçayı test için kullanır, bu parça her seferinde bir önceki iterasyondan farklı olur, bu yüzden modelimiz sürekli yeni test seti ile test edilmiş olur.2. Training with more data: Örnek sayımızı artırmak verimizdeki target ile feature arasında ki ilişkiyi daha rahat anlamamızı sağlamayabilmekte.3. Removin Features: Feature setimizden alakasız featur'ları çıkartarak target-feature ilişkisini daha net bir hale getirebilmekteyiz.4. Ensembling : Birbirinden ayrı modelleri bir arada kullanmamıza olanak sağlayan ML metodudur. Böylece modelimiz daha karmaşık yapılı örnekler ile overfit olmadan çalışabilir.Kısaca özetlemeye çalıştım daha ayrıntılı bir şekilde : [Link](https://elitedatascience.com/overfitting-in-machine-learning) adresinden inceleyebilirsin.İyi akşamlar.",
3. ->  ->  hocam şu soruma bakar mısınız rica etsem bir kaç saat önce post attım. ben de bu 4 adet şeyi seçtim ama yanlış cevap dedi.[Link](http://community.globalaihub.com/community/status/1468-1468-1586718659/) [Link](https://www.quora.com/Can-early-stopping-of-machine-learning-algorithms-lead-to-overfitting-of-validation-data)
4. ->  ->  İlk olarak Vermiş olduğunuz link çalışmamakta.Edit: Bahsi geçen soruda .Early stopping, .Regularization seçenekleri de bulunmaktaydı. Bunlarda overfit'i önlemek için alınabilecek önlemler arasında bulunmaktalar. Yani O soru için hepsi doğru olmalıydı. Sadece bu postta bahsi geçmiş olan 4 yöntem değil..",
5. ->  ->  profilime tıklar mısınız orada gözüküyor hocam.",
6. ->  ->  Maalesef görebildiğim bir post yok profilinizde 🙁.",
7. ->  ->  sorum şuydu hocam:Overfit i önlemek için eğitimin erken durdurulması doğru kabul edilmiş fakat internette şöyle bir yazıya rastladım. İki bilgi çelişir gibi geldi..",
8. ->  ->  Gördüm şimdi, Renklendirerek paylaşmış olduğun cevabı eğer yanlış anlamadıysam sadece ilk paragrafında soru ile alakalı renklendirdiğin kısım var cevabın kalanının daha çok veri kalitesi train- test verisinin dengesi (generalization) üzerine olduğunu görüyorum. Yani bu yazı ile bu yöntemler kesinlikle yanlıştır diyemeyiz. Devamında bulunan 2. cevabı okursan eğer durumu daha iyi anlayacağını düşünüyorum.Edit: Burada bahsi geçen yöntemlerin yanlış kullanımı halinde de yine overfit gibi bir durumla ya da daha farklı problemlerle karşılaşabiliriz. Alınabilecek önlemler olarak kabul edilmiş yöntem olmaları bu yöntemleri kullanmamız halinde \"oldu tamam ben artık overfit problemini aştım\" diyerek arkamıza yaslanabileceğiz anlamına gelmiyor tabi 🙂İyi çalışmalar..",
9. ->  ->  teşekkürnederim hocam 🙂.",
    
### soru 

> quest: "Bu konuda çok kafa karışıklığı var o yüzden buraya bir post yazayım dedim. Veriye validation set eklemezsek ne olur'la baksak çok daha iyi olacak.  Siz bir model geliştiriyorsunuz, training ve test seti ayırdınız, 100 örnekten 80'i training 20'si test. Ev fiyatlarını tahminlemeye çalışıyorsunuz. Bir regresyon modeli train ettiniz, sonra içine test verisinden 4 odalı ve iki banyolu bir ev koydunuz o da size bu evin fiyatının 100 bin lira olması gerektiğini söyledi, ama gerçekte o ev (test verisindeki ev fiyatı kolonu) 120 bin lira, buna göre hatanıza baktınız, parametrelerinizi değiştirip yeniden train ettiniz. Zamanla kendinizi bu test verisinden aldığınız hatalara göre adapte ediyorsunuz, yani test verisine overfit ediyorsunuz. Farkettiyseniz test verisiyle hem parametreleri değiştiriyoruz hem de test ediyoruz, bu yanlış, bu yüzden validation set ekliyoruz, hataya bakıp parametre değiştirme işlemini validation set'te yapıyoruz, ardından yeni çıkan modeli test verisiyle test ediyoruz, böylece modelin gerçekten iyi bir performans sergileyip sergilemediğini görebiliyoruz.",

> comments:
  
1. ->  Merve Hanim, diger post altinda da sormaya calismistim ama sorumu yeterince izah edemedim sanirim. Benim anladigim;\"burada train edilmis model icin test datasini bir sekilde modelin tekrar guncellemesi icin kullanildigini soyluyorsunuz. Bu durum, kullandigimiz takdirde validation verisi icin de gecerli.\"Demek ki bir \"update rule\" kullanimi var. Bu \"update rule\" nasil yapilandiriliyor (matematiksel olarak)?Bana bununla ilgili bir aciklama ya da kaynak gostermeniz mumkun mu acaba? Benim bildigim tek update rule backpropagation ve bu sizin soylediginiz test verisine \"tune\" olma probleminin onune gecmek adina validation verisi kullanildigi kurs programinda da bahsediliyor lakin ben bunun nasil oldugunu henuz kavrayabilmis degili..",
2. ->  Son bir ekleme daha yapabilir miyim;parametreler dedikleriniz \"agirliklar ve bias'lar\" mi yoksa \"learning rate, batch size, epochs\" mu?.",
3. ->  ->  tabiki de \"learning rate, batch size, epochs\" ... şunu demek istemiş anladığım kadarıyla bu parametreleri sürekli değiştirip iyiye yönelmek isterken test verisini overfitting yapıyorsunuz yani networku ezberletmiş oluyorsunuz bunu validation verisi ile test verisini doğrulatarak modeli eğitmenin daha doğru olduğunu söylemiş..",
4. ->  Evrim bence guzel soru sordun cunku anlam karmasasi yasiyoruz gercekten parametreler konusunda. zaman zaman dinlerken.Gradients denileni weights and biases olarak algiliyorum.Leraning rate, epoch, batch_size ise hyper parametrelerdir.Ancak konusmacilar weights ve biases icin parametreler ifadesini kullanabiliyor..",
5. ->  ->  weight ve bias parametrelerine siz müdehale etmiyorsunuz model katmanındaki optimizasyon algoritması(Örneğin: Stochastic Gradient Descent, Adam...) bu güncellemeyi yapar, değiştirir. Yanlışsam biri beni düzeltsin..",
6. ->  ->  Eger dediginiz gibi ise sunu aciklarmisiniz: diyelim ki model = vgg16(pretrained) ve for param in model.features.parameters()...param.requires_grad= False dersem ben neyi freeze etmis oluyorum?.",
7. ->  ->  Benim kafami karistiran terminoloji oldu. Sayet parametreden kastimiz sizin dediginiz gibi \"learning rate, batch size, epochs\" ise benim sorularim anlamsizlasiyor, cunku guncelleme metodu gerektiren seyler \"weights and biases\".Ilaveten Senay'a katilmak durumundayim, benim terminoloji bilgim su sekilde:hyperparamaters: \"learning rate, batch size, epochs\"parameters: \"weights and biases\".",
7. ->  ->  Ibrahim weights ve biases lere tabiki biz mudahale etmiyoruz biz sadece weightsleri optimizer.step() function kullanarak update ediyoruz.Bu konuda hemfikiriz..",
9. ->  ->  Eywallah güncelleme gerektiren yerler weight ve bias ... diyelim ki siz modeli eğitiniz sonuçlar kötü loss azaltıp accuracy değerini arttırmak için ne yapmanız gerekecek learning rate, batch size ve epoch değerlerini hatta optimizer da değiştirerek en iyi sonucu bulmaya çalışacaksınız bu denemeleri yaparken test veriniz overfitting olabilir.Bu yüzden veriyi 3 e ayırıp validation set ve test seti kıyaslamak gerekecek.",
10. -> -> :Öncelikle terminoloji ile alakalı sizinle aynı fikirdeyim eğer daha burada hata yapıyorsak aydınlatılırsa çok iyi olur.hyperparamaters: \"learning rate, batch size, epochs\"parameters: \"weights and biases\"Bu soruda; \"The regressor might overfit to test set if we don't use validation sets. \"Kafanıza takılanın bu olduğunu söylemişsiniz;Evet hatirliyorum. Benim sormaya calistigim, validation verisi kullanmak overfitting'i nasil onluyor? Nasil bir mekanik (bir gunceleme kurali ya da metodu) kullanilarak overfitting onleniliyor?Ve yanlış anlamadıysam net olarak sormak istediğiniz validation set işlemi için farklı bir matematiksel işlem olup olmadığı.Benim anladığım validation set ile test set arasında setlerin kendi çalışma mantığında hiçbir fark yok, yani ikisinde de amaç aslında test etmek. Ancak validation sette test ettikten sonra parametreleri güncelliyoruz(weights ve bias). Asıl Test setinde ise sadece bu ayarların nasıl sonuç verdiğine bakıyoruz. (parametreler önceden validation sette ayarlandı). Eğer bunu validation sette yapmayıp test sette yaparsak test set hem parametreleri ayarlamak için modele kendinden veri verecek, model bu verileri öğrenecek, sonra tekrar test setinde bu veriler test için kullanılıp \"overfitting\" olacak. Overfitting olacaksada validation sette olsun ki nasıl olsa test sette modelin hiç görmediği verilerle modeli son kez test edeceğiz. İşlemler sonunda Training loss ve Test loss değerlerine bakarak modelin tahmin gücünü anlıyoruz.Test setteki parametre ayarlama matematiksel işlemlerini Validation sette yaptık.Doğru anlamadıysam yüzüme vurun 🙂.",
11. -> ->  ilk basta sizin anlattiginiz gibi algiladim; yani validation veya test set uzerinden \"weights and biases\" icin bir update rule uyguluyoruz. Yalniz Ibrahim Ayaz kavram sorumun uzerine beni duzeltti. Bu durumda, yukurida da belirttigim uzere, benim sormaya calistigim sorular anlamsizlasiyor, cunku ben \"weights and biases\" icin bir guncelleme yaptigimizi saniyordum. Oysaki validation veya test seti \"learning rate, batch size, epochs\" uzerinde degisiklik yapmak icin kullanmaktan bahsediyormusuz. Bunlar bir \"update rule\"a bagli olmayan bizim elimizle girip degistirdigimiz degerler. Dolayisiyla validation set'in butun amaci bizim *training* sonuclarina bakarak kendimizce degisiklikler yapip modeli guclendirme amacimiza hizmet ediyor. Ki bu elimizle yaptigimiz degisiklikler de neticede bir cesit *tuning* olmasi sebebiyle test set uzerinde yapilirsa modelin genellenebilirligini zedeleyen bir unsura donusuyor, cunku model genel bir veri gurubu yerine test set verilerine duyarli hale geliyor, o yuzden de bunu validation set diye ayirdigimiz bir veri gurubu uzerinde yapmak daha mantikli.Kisa yorum: validation veya test set'in \"weights and biases\" ile dogrudan bir iliskisi yok. Amaci bizim gozlem (deneme-yanilma) yoluyla modelin gucune katki vermemize olanak tanimasi.Benim icin faydali bir tartisma oldu. Validation set'in islevi konusunda muglak fikirlerim vardi ve kavram tam olarak zihnimde yer etmemisti. Simdi tam olarak oturdu. Katki veren herkese tesekkur ederim.",
12. -> Merhabalar,->  Öncelikle paylaşımınız için teşekkür ederim. Ancak bazı noktalarda eksikleriniz bulunmakta.Örneğimiz Ev Fiyatlarının Tahmin Edilmesi (Regresyon Problemi), Hedef: Evin Fiyatı, Feature(Değişken): Oda Sayısı . Örnek Büyüklüğü 100, Train/Test büyüklüğü: 80/20 <- Sizin örneğiniz üzerinden açıklamaya çalışacağım.Eğitime başladıktan itibaren, her bir iterasyon sonunda bir Regresyon modeli tahmin edilir(örneğin: ev_fiyatı = 45.000(bias) + 1250(weight) * oda_sayısı). Bu model Test Setinin Tamamı ile(Ayrılmış olan 20 örneğin hepsi ile ) test edilir, çıkan sonuca göre katsayılar(weight(1250), bias(45.000)), belirlemiş olduğumuz learning rate'ye göre güncellenir(Batch Size verinin nasıl parçalanıp işleneceğini(batch_size'nin büyüklüğü iterasyon sayısını belirler.), Epoch ise verinin bir bütün olarak kaç defa eğitime tabii tutulacağını belirtir.). Sonraki iterasyon ile devam edilir. Belirlenen epoch sayısına ulaşılıncaya kadar bu işlem böyle devam eder. Ancak burada Test Set üzerine modelimizi overfit etmiş olmuyoruz. Modelimizin overfit olma olasılığı var. Böyle bir durumu sadece test set ile çalışarak gözlemlememiz mümkün olamamakta. Bu yüzden 3. bir set oluşturuyoruz: validation set olarak. Böylece eğitim boyunca bütün testleri validation set üzerinden yapacak ve eğitim bittikten sonra test setimiz ile 2. defa modelimizi test edecek ve eğitim sonuçları ile tutarlılığını gözlemleyebileceğiz. Eğer eğitim oranları en son yapmış olduğumuz test oranlarından çok büyük ise eğitim sırasında overfit olmuş diyebiliriz, ve buna göre modelimizi tekrar gözden geçirmek suretiyle gerekli değişiklikleri yapabiliriz.->  Dediğiniz gibi validation seti bütün eğitim boyunca kullandığımız için modelimiz validation set'e de overfit olabilmekte ancak en son Test Set'imizle yapmış olduğumuz deneme ile bu durumu tespit edebilmekteyiz.İyi geceler, iyi çalışmalar..",
13. ->  ->  Bilgilendirmeniz için teşekkür ederim. Ben de bu alana yeni başlayan biri olarak bir düşüncemi belirtmek istiyorum. Yazılarda ifade edilen terimler bazen olayları kafamda farklı şekillendirmeme sebep olabiliyor. Bundan dolayı bu terimler kullanılırken daha hassas olunursa anlam karmaşıklığının önüne geçileceğini düşünmekteyim.Terminolojiyi takip etmek isteyenler için;[Link]((https://developers.google.com/machine-learning/glossary)
"Machine Learning Glossary  |  Google Developersdevelopers.google.comCompilation of key machine-learning and TensorFlow terms, with beginner-friendly definitions..",
    
### soru 

> quest: "Soruların hepsini doğru yaptığımı belirterek kendimce nasıl yaptığımı tek tek anlatacağım. Yararlı olmasını umuyorum ve yorumlarınızı bekliyorum. Yazım hataları olabilir hızlıca yazdım kusuruma bakmayın. Hepsini yorum olarak paylaşacağım.",

> comments:
  
1. ->  Q1: Soruda validation setini kullanma sebebimizi soruyordu.1.1: Regressor ifadesi bana regresyon modelini çağrıştırdı.Train setinde eğittiğimiz verilerdeki hiperparametreleri test setine göre yaparsak model test setindeki verileri verdiğimizde iyi sonuç verip bizi yanıltabilir.Bu sebeple validation ile parametre ayarlayıp test setiyle overfitting'in oluşup oluşmadığını kontrol ediyoruz. 2 aşamalı değerlendirmeden sonra bunların loss değerleri birbirine çok yakın ise overfitting oluşmadığına kanaat getiriyoruz.Dolayısıyla validation set kullanmaz isek \"test setine\" overfit olabilir.1.2: Validation setin modelin fit performansınına etkisi yoktur. Sadece test setine overfitting olmasını önlemek için ara aşama.1.3: Modelin overfit veya underfit olmasını kontrol eden şey test setidir. Validation set sadece test setine olan overfiti denetler..",
2. ->  Q2: 6 resmin eşleştirilmesini istiyordu.1.resimde overfit durumu görüyoruz. Dolayısıyla train setin kaybı düşük, test setinin kaybı yüksek olmalı.Bu nedenle B şeklindeki son duruma baktığımızda test setin kaybı train sete göre oldukça fazla.2.resimde optimal modeli görüyoruz. Ne 1 gibi çok karmaşık bir model ne de 3 gibi çok basit.Dolayısıyla eğitim performansı iyi olmalı, test performansıda ona yakın olmalı.3.resimde ise model çok basit. Verilere çok iyi uymuyor. Dolayısı ile eğitim performansı düşük olmalı.Bu nedenle eğitim performansı diğerlerinden daha düşük olan A şekliyle eşleşir. Sonuç: 1-B/2-C/3-A.",
3. ->  Q3: Modelin kaç kez güncelleneceğini soruyordu.250 örneğimiz var. 80/20 oranında parçalarsak train set 200 örneğe sahip oluyor. Batch size ise 32.200'e tam bölünmüyor. 200 içinde 6 tane batch var ve bunları çıkarında 8 örnek kalıyor. Dolayısı ile bunların 7 iterasyon olacağını söyleyebiliriz.Buradaki düşüncem küsürat olamayacağı yönünde çünkü iterasyon tam sayılardan olmalı yarım iterasyon gibi bir şey mantıksız olacaktır.Batch 32 olduğundan kafa karıştırabilir ama ben o yeterince veri yoksa elindeki veriler kadar yapacağını düşünüyorum.Dolayısıyla cevaba 7*1000(epoch sayısı) = 7000 dedim..",
4. ->  ->  Bu sorunun net olmadığını düşünüyorum. Batch size 32 iken 8 örnek ile train etmek ne kadar doğrudur? Mevcut algoritmalar nasıl davranıyor bilmiyorum, bu konuda bilgisi olan varsa açıklayabilirse çok iyi olur.Ben de 200 / 32 = 6.25 çıkıyor ve batch size ımız 32 i olduğu için 8 sample ile train edilmeyeceğini düşündüm ve 1 epoch da 6 iterasyon olur, 1000 epoch için 1000*6 = 6000 iterasyon gerekir diye düşündüm. Tamsayı çıkan bir değer sorulsaydı daha açık ve net bir soru olurdu sanırım.,
5. ->  ->  Batch 32 diye 32 örneğe ihtiyacımız olmaması daha mantıksız değil mi? Şöyle düşünün batchte amacımız verileri parçalamak biz diyoruz ki aynı anda en fazla 100 veri işleyebiliriz. 8 veri gelmesi sizce mantıksız mı olur? Böyle düşünmekte fayda var..",
6. ->  ->  Ayrıca şunu belirtmeyi unutmuşum 8 örneği dışarıda bıraktığınızda tüm verisetini eğitimden geçirmemiş oluyorsunuz. Dolayısıyla bununla da çelişiyor dediğiniz yöntem..",
7. -> ->  elimizdeki veri sayısı batch size'a göre bölündükten sonra batch size'dan daha az sayıda kalan veriler ne kadar olursa olsun son iterasyon olarak training'e katılıyor. bu neden bu örnek için 6 iterasyon 32 veri ile, 7. iterasyon ise kalan 8 veri ile gerçekleşiyor. tam sayı çıkmaması aslında sorunun ufak bir trick'i olmuş.,
8. -> -> ->  Merhaba, yorumlar için teşekkürler. Ben 8 örnek ile update etmenin problem olabileceğini düşündüm, çünkü kalan 8 örnekten 4 tanesi iki kümenin tam sınırında kalan veya yanlış tarafta olan örnekler olursa az sayıda örnekten dolayı düzgün bir update işlemi olmayabilir, bu yüzden batch size a bir limit koyuyoruz, 32 lik datayla daha genel bir update işlemi oluyor.8 örneği bir epochda dahil etmemek çok büyük bir problem değil. Zaten siz datasetteki bütün datalarla öğrenme işlemi yapmıyorsunuz. Training ve test set olarak ayrılmamış tek bir dataset varsa yaklaşık yüzde 80 ini training e, yüzde 20 sini teste ayırıyorsunuz ve sadece training datası ile update yapıyorsunuz.Genelde her epochda datalarımızı random olarak seçtiğimiz için bir epochda kalan 8 örnek diğer epochda kullanılabilir, her epochda kalan 8 örnek aynı olmayacaktır.Bu yüzden son 8 örneği almamanın, alınacaksa da o epochda kullanılan datalardan random 24 tane daha datayı dahil etmenin daha uygun olacağını düşünüyorum.Mentorlarımızdan biri de konuyla ilgili açıklama yaparsa süper olur.Kolay gelsin...",
9. ->  ->  Teknik olarak algoritma son kalan 8 örneği yeni bir batch olarak değerlendirip 8 örnekle güncelleme yapıyor. Bu tip durumlarda dediğiniz gibi yöntemlere başvurulabilir mi ? İsterseniz bu fazlalıkları veri setinizden çıkarabilirsiniz veya random 24 tane veride verisetinden ekleyebilirsiniz (Ezberci eğitime karşıyız 🙂 ). Bunlar ek çözümler olur. Soruda sorulmak istenen algoritmanın nasıl davranacağı. Çözüm Enes beyin ifade ettiği gibi. Ancak sizin de ifade ettiğiniz gibi çok çok aşırı farklılıklar yaratmayacaktır..",
10. ->  Q4: Ev fiyatı için hangisinin iyi bir özellik olmayacağını soruyordu.Önceki sahibinin cinsiyeti iyi bir özellik olamazdı..",
11. ->  Q5: Soruda learning rate'i optimal seçmemekten dolayı oluşan problemlerden olmayanı soruyordu.Dolayısıyla öğrendiklerimizi düşünürsek kötü learning rate in 2 etkisi vardı: hızlı adım atıp minimumu kaçırma veya çok yavaş adım atıp uzun sürede minimuma ulaşma. Baktığımızda 1.şık ve 2.şık bunları sağlıyor.3.şıkka baktığımızda ise kayıp eğrisinde minimuma gitmek yerine test yöne hareket ettiği yazıyor. Bunun sebebinin gradyan hesaplarken yapılan hata olduğunu düşünerek bu cevabı seçtim..",
12. ->  Q6: Hangisinin lineer regresyon problemi olduğunu soruyordu soruda.Sınıflandırma ile aralarındaki temel fark ise regresyondaki output sayısal yani sürekli bir değer, sınıflandırma ise kategorik değerdir. Dolayısıyla (negatif-pozitif, kadın-erkek, yazar) sınıflandırma problemi oluyor. Satış tahmin etmek ise regresyon problemidir..",
13. ->  Q7: Korelasyon matrisi hakkında doğru olanı soruyordu.Bu tensorflow colabında geçiyordu. Her özelliğin ham değerinin diğerlerinin ham değeri ile ilişkisini veren değerlerimiz vardı.Bir de doğru hatırladığımı kesinleştirmek için şöyle düşündüm, Neden her özellik özellik değeri diğerlerinin ham değeriyle ilişkili olsun ki?.",
14. -> Q8: Hangilerinin overfittingi önlediğini soruyordu.1.deki cross-validation'a validation'ı öğrenmiştik ve bu da validation'un bir çeşidi gibi göründüğünden dolayı doğru dedim.2.deki daha fazla veriyle eğitim yapmanın veri miktarı ne kadar uzun olursa overfit olmanın o kadar uzun süreceğini düşündüm. Overfittingi kafamda ezberleme olarak kodlamıştım. Bir yerdeki insan sayısı artarsa onların hepsinin ismini ezberlemeniz daha uzun sürer mantığıyla.3.de özellik çıkarmaya doğru dedim çünkü ağırlık eksilmesi modelimizin karmaşıklığını azaltıyor. Buna şöyle bir örnek versem saçma olur mu bilemiyorum. Kalem verisetimiz olduğunu düşünelim. Kırmızı kalem örneği çok olduğundan iyi tahmin ediyor ancak mavi kalem örneği az o kadar iyi değil. Her boydan yeterince kalem olduğundan dolayı 100% tahmin etsin. Renk özelliğini çıkarırsak boylara göre tahminimiz yüksek olduğundan test setimizin doğruluğu artıyor. Overfitting düşüyor.4.de erken bitirme olayı overfit olmadan önce en iyi seviyede modelin eğitimini bitirebileceğimizi düşünürsek doğru geliyor.5.de regular kelimesinin düzenli anlamına geldiğini biliyordum ve bundan yola çıktım. Verisetini eğitim için düzenleştireceğini düşünerek overfittingi azaltabileceğini düşündüm ama buna pek gerek kalmadı.1,2,3 ve 4 ün olduğu tek şık vardı o da son şık olan hepsiydi..",
15. ->  ->  Overfitting önleme ile ilgili kursun hangi bölümünde anlatıyor? Ben bunlara bakmamışım.",
16. ->  ->  Validation set kısmında anlatılıyor..",
17. ->  Q9: Uyumu test etmek için test metadolojisi uygulanırken hangisi gereksizdir diye soruyordu.1.şıkta olan örneklerin setten bağımsız ve aynı şekilde çekilmesi kesinlikle lazım.2.şıkta olan dağılımın değişmez olması sonradan veri eklenmemesi gerekiyor. Çünkü örneğin kedi köpek sınıflayan bir modelimiz var. Bunu eğitirken köpek sayısı azsa yani köpeği tahmin etmesini geliştirmek için yeterince örnek yoksa ve biz sonradan abartırsak 10.000 köpek verisi eklersem tahmin oranı çok düşer.3.şıkta örnekleri seçerken rastgele seçmemiz gerektiğinden bahsediyor ve kesinlikle doğru.4.şıkta model test seti üzerinde eğitilmelidir diyor ve bu kesinlikle yanlış. Model train seti üzerinde eğitilir. Test seti üzerinde değerlendirilir.Dolayısıyla cevabı 4.şık işaretledim..",
18. ->  Q10: Nispeten diğerlerine göre kolaydı.Elbette atlamaların sebebi öğrenme oranının yüksek olması..",
    
### soru 

> quest: "İyi akşamlar arkadaşlar 🙂  Öncelikle sınav sorularından bir tanesini sormak istiyorum. Belki çok basittir ama anlayamadım. \"The regressor might overfit to test set if we don't use validation sets. \" buradaki ifadeyi açıklarmısınız?",

> comments:
  
1. ->  Bu soruyu ben yazdım. Eğer ekstra bir validation set kullanmazsak regressor overfit eder test setine demek. Bunun hangi kısmını anlamadığınızı söylerseniz açıklayabilirim..",
2. ->  ->  Devam niteliginde bir sorum olacak. Validation set'in islevi konusunda yerine oturtamadigim bir durum var. Yanlissam duzeltin;agirliklarin guncellenmesi icin sadece training verisinden gelen ciktinin label verisi ile karsilastirmasindan yararlaniliyor. Bu durumda validation verisi sadece gorsel bir karsilastirma yapmak icin var. O zaman overfit'i onleme processini nasil gerceklestiriyor? tam olarak yaptigi sey nedir?.",
3. ->  Merve Hanim ifadenzide hata oldugunu dusunuyorum. Oncelikle regressor un test setine overfit etme ifadesine dogru diyemeyiz..Test sirasinda overfitting probleminin ortaya cikabilecegini soyleyebilirsiniz.Ancak bu problem de validation kullanilmadigi icin ortaya cikmaz..",
4. ->  ->  bununla ilgili bir post yazdım..",
5. ->  ->  overfit olayı olunca illa ekstra validation set eklememize gerek yokki başka yollarla da overfitting önlenebilir , bide benim eğitimden de anladığım validation set testten once modeli anlamak için kullanılıyor..",
6. ->  ->  overfitting'i regularisation'la engelliyoruz, validation seti eklemek ve ayrı bir test seti kullanmak bizim overfit edip etmediğimize bakmamızı sağlıyor..",
7. ->  Overfitting oneleme nasil olur sorusunun cevabi sinavin iceriginde mevcuttu hatirlarsaniz;Cross-validation, early stopping, train with more data, remove features ,regularization, dropout..",
8. ->  ->  Overfitting nasil onlenir sorusunun cevabi sinav sorularindan biriydi.Hatirlarsaniz, cross-validation, remove features, train with more data, regularization, early stopping, dropout...",
9. ->  ->  hmm simdi anladım 😀 teşekkür ederım oturmamış bu konu bende 🙂.",
10. ->  ->  Evet hatirliyorum. Benim sormaya calistigim, validation verisi kullanmak overfitting'i nasil onluyor? Nasil bir mekanik (bir gunceleme kurali ya da metodu) kullanilarak overfitting onleniliyor?.",
11. ->  ->  Overfitting engelleme ile ilgili kısımlar eğitim serisi içinde nerede acaba? Gözden kaçırdım sanırım..",
12. ->  Regresör regresyon yapan modele dendiğini düşündüm ben yaparken. Eğitimde de gördük ki validation kullanma amacımız test setine overfitting oluşmaması. Dolayısıyla kullanmaz isek test setine overfit oluşabilir..",
13. ->  Aklımda check ve evaluate kavramları kaldığından ben soruyu yanlış cevapladım..",
14. ->  Validation kullanmazsan test sirasinda overfitting problemi ortaya cikar diye bir kural yok.Ortaya cikabilir.Validatin kullanmamizin sebebi modelimizi check edip karar vermek, teste hazir olup olmadigina..",
15. ->  ->  Düşündüğümü aktaramamışım kesinlikle öyle oluşmaması için bir önlem validation..",
16. ->  Aslinda daha onemlisi su: Validation set kullanmadigin icin overfitting problemi ortaya cikmiyor, validation set kullanmak, test surecinden once modeli degerlendirmeni saglar.Dolayisiyla Merve Hanim ifadenizin dogrulugundan supheliyim..",
17. ->  ->  might diyor orada zaten.",
18. ->  ->  might diyor ama eger validation kullanmazsak diyor.Yanlis hatirlamiyorsam bu dogru yanitti.Ve test sirasinda ortaya cikabilecek olan overfiiting probleminin validation set kullanmamaya baglanmasina dogru diyemeyiz..",
19. ->  Bununla ilgili bir post yazdım..",
20. -> Soruda herhangi bir hata görmedim, ancak burada yazılanlar biraz fazla terimsel olduğu için konunun anlaşılmasını engelliyor olabilir.Herhalde şunu biraz açmak bu meselenin anlaşılmasını kolaylaştırabilir:İki parçalı veriyle(train+test) çalışırken ürettiğimiz modelin ne kadar başarılı olduğunu görmek için model üzerinde değişiklikler yapıyoruz. yaptığımız her değişiklik modelin \"kullandığımız test seti için\" düzenlenmesine sebep oluyor. Yani bir süre sonra bu sisteme overfit olmaya başlayabilir. Bu da gerçek datayla çalışmaya başladığımızda hata miktarının öngöremediğimiz şekilde artmasına sebep olabilir.Bunu engellemek (yani tarafsızlığımızı korumak) adına yapabileceğimiz bir şey araya bir validation set ekleyip (train+validation+test) modeli validation set üzerinde elde ettiğimiz sonuçlara bakarak düzeltmek. Modelle işimiz bittikten sonra performansını test sete bakarak belirleriz. Böylece tarafsızlığımızı daha iyi korumuş, çalıştığımız veriye overfit olma riskini de azaltmış oluruz.",
21. ->  ->  Şöyle bir örnek daha net anlaşılmasını sağlayabilir belki. Boksör olduğumuzu düşünelim. Güçlü biriyle (test) turnuva maçına çıkacağız. Antrenman yapıp (train) direkt adamın karşısına çıkmak yerine öncesinde orta seviye biriyle (validation) maç yapıp gücümüzü test ediyoruz. Bu karşılaşmaya göre daha iyi hazırlanıp turnuvadaki maçımıza (test) gidiyoruz. Orta seviye adama göre çalışmaya çok fazla odaklanırsak (overfitting) turnuvada çok fena dayak yiyebiliriz 🙂",
22. ->  ->  Harika! Bu örneğe bir düzeltme; rakip boksörlerin ne kadar güçlü olduğunu bilmiyor olmalıyız. Aksi takdirde tarafsızlığımızı kaybetmiş ve gene overfit riskiyle karşılaşmış oluruz..",
23. ->  ->  Doğrudur o zaman ikisinide bilmediğimizi ama birbirlerinden farklı olduklarını düşünebiliriz sanırım..",
24. ->  Overfitting, training esnasinda cok iyi sonuclar alip, test/validation sirasinda poor sonuclarin olusmasidir.Modeliniz training sirasinda ogrenmekten ziyade ezberlemistir..Dolayisiyla modelinize ezberledigi training dataset disinda farkli bir dataseti verdiginizde sonuclar icacici olmamistir,Peki neden olmustur bu? Yani overfittingin sebebi nedir? Kucuk bir dataseti ile cok katmanli complex bir model kullanmissinizdir.Nasil onleriz? Dropout, regularization, remove features, cross-validation, early stopping, train with more data.(Bazi yontemlerin ayni anda kullanilmasi tavsiye olunmaz).",
    
### soru 

> quest: "Veri setinden veri setine değiştiğini bilmekle beraber en iyi epoch, learning rate ve batch size kombinasyonu (işlem hızını da gözeterek) learning rate ve batch size'ı mümkün mertebe düşük tutup epoch sayısını yüksek tutmak mıdır? Çünkü anladığım kadarıyla batch sayısını ne kadar küçültürsek ana örneklemimizi o denli küçük parçalara ayırıyor ve her seferinde learning rate'i kullanarak ağırlıkları update ediyor. Bu durum da işimize gelmeli.  Epoch tarafından bakacak olursak da, 1 epoch sanıyorum ki tüm mini batchlerimizin işlenmesi anlamına geliyor (Yani ana verideki örneklem sayısı/mini batchteki örneklem sayısı kadar iterasyon). Bu sayıyı ne kadar yüksek tutarsak da defalarca kez veriyi işleriz ve sonucunda düşük bir MSE değerine ulaşırız.  First Step with TF dersinin 'Linear Regression with a Real Dataset' egzersizinde learning rate'i düşürürken epoch ve batch size'ı büyütmenin çoğunlukla en iyi kombinasyon olduğu yazıyor ancak ben batch size kısmına katılmıyorum.

> comments:
  
1. -> Merhaba,Batch size'ı çok küçük bir değere ayarlamak kararsızlığa neden olabilir. Öncelikle batch size'ı büyük bir değere ayarlayıp azalmayı görene kadar ufak ufak küçültmeniz daha iyi olacaktır. Batch size'ı büyük bir değer almamızdaki dezavantaj memoryde kaplayacağı yer ve zaman karmaşıklığıdır(complexity.) Avantajı ise büyük bir batch size'da daha doğru ve tutarlı eğimler elde edersiniz çünkü daha büyük bir veri grubu (batch sayısı kadar veri) içindeki kaybımızı optimize ediyoruz. Yani batch size'ınız düşükken daha sık güncelleme yapıyor olsanız da bu sıklık optimizasyonun daha iyi yapılacağı anlamına gelmez. Bunu bir çok kötü güncelleme vs çok az iyi güncelleme olarak düşünebilirsiniz. En ekstrem örnekte zaten batch_size training size'a eşit olacak kadar büyüktür. Kaynak olarak: [Link](https://forums.fast.ai/t/disadvantages-of-using-very-large-batch-size/29177/3) linkini verebilirim. İyi çalışmalar dilerim 🙂",

### soru 

> quest: "Merhabalar,  Validation and Test Sets Collab kısmında task2 de train ve validation setindeki verilere yeterince benzemiyor diyor ve head ile incelenmesini söylüyor.Tam olarak benzememeden kastı ne ve de bu sette bunu nasıl gözlemleyip evet benzemiyor diyebilirim?  Teşekkürler:)",

> comments:
  
1. ->  dataframe ismi ile birlikte .head(1000) diyerek kullanırsan ilk 1000 veri geliyor örneğin. Burada verilerin hepsinin farklı olduğunu gözlemliyoruz hepsi bu 🙂.",
" -> Merhaba Buse,Çözüm kısmına baktığında, 0-4 ile 25-29 arasındaki değerleri incelemeni söylüyor. Kodu yazıp baktığımızda, bazı farklı ve aşırı değerler görebiliriz. Birisi 1000-7000 arası değişirken, diğeri 700-2000 arası değişmekte.",
" -> Merhaba, sanırım \"validation seti train setine yeterince benzemiyor yani rastgele dağılmamış\" bunu keşfetmemizi bekliyor Task 2. Longitude değerine bakarsak verinin sıralandığını görüyoruz.",
1. ->  Sanırım keşfetmemizi istediği şey train setindeki verilerin longitude değerlerine göre sıralı olması. Bu durum veri setini train ve validation olarak ayırırken, validation setinin sadece belirli bir aralıktaki longitude değerlerine içermesine sebep oluyor. Bu durumun problem olmasının asıl sebebi de longitude değerinin median_house_value değerini etkilemesi olduğunu söylüyor colab'deki açıklama. Bu etkiyi yani feature'ların birbirleri arasındaki ilişkinin derecesini bir önceki exercise'larda correlation matrix ile görebileceğimizi öğrenmiştik fakat corr matrisine göre aralarında(median_house_value ve longitude) bir ilişki olmaması gerekiyor. Teğit etmek için datayı incelediğimde de aralarında bir ilişki göremiyorum. Bu durumun sebebini anlayamadım, birisi aydınlatabilirse sevinirim, şimdiden teşekkürler..",
    
### soru 

> quest: "Son aşamada (Validation Set)  Şimdi ilk önce training set ile veri eğitiliyor, ilk epoch sonunda test ediliyor değil mi? eğer iyi bir tahmin yoksa, ikinci epoch'a geçiyor sonra test set ile yine test ediliyor. Bu yüzden overfitting olmaması için Validation Set kullanıyoruz. Buraya kadar doğru anlamış mıyım? Bir diğer sorumda Training Set içinde Test Set verileri yok, ayrı yani bu veriler, overfitting nasıl oluyor? Burayı tam anlayamadım. Teşekkürler:)",

> comments:
  
1. ->  Benim anladığım training setinden verilerimizi eğittikten sonra her aşamada test setteki veriler ile test edince modelimiz verileri ezberlemesi duruma overfitting oldu deniliyor. Modelimiz overfitting olduğunda ise yeni bir veri eklediğinde düzgün bir şekilde çalışamıyabiliyor. Bu yüzden bu durumu engelleyebilmek için modelimizi eğitirken validation seti kullanarak hyperparameterleri ayarlıyoruz ve modelimizin hazır olduğuna karar verdiğimizde son aşama olan test için ayırdığımız test seti ile modelimizi test ediyoruz.",
1. ->  yani modelimizi overfitting olmaması için validation seti kullanarak test ediyoruz en son aşamada test seti kullanarak modelimizin iyi olup olmadığını belirliyoruz. Umarım daha çok kafanı karıştırmamışımdır.",
" ->  Merhaba, [Link](https://medium.com/data-science-tr/overfitting-underfitting-cross-validation-b47dfda0cf4e) bu sitede overfitting durumu çok basit bir dille anlatılmış belki faydası olabilir",
"Makine Öğrenmesi Dersleri 8: Cross Validationmedium.comOverfitting (High Variance)",
1. -> Çok teşekkür ederim cevaplarınız için.",
1. ->  Şöyle ki eğitim setinde test verileri bulunmuyor ama eğitme işleminde hiperparametreleri kullanarak test setindeki verilerin doğruluğunu iyileştirmek için kullandığımızdan dolayı model sadece test verileri için iyi değer vermiş oluyor. Yeni veri gelince patlıyor..",
1. ->  Verini üçe bölüyorsun, eğitim, değerlendirme ve test. Değerlendirme de aslında bir test seti. Verini batch'ler halinde eğittin, her batch'te değerlendirme setindeki verilerle hata hesaplayıp hiperparametrelerini güncelliyorsun. Yani bu her epoch sonu değil her batch sonu olan bir şey. Verini eğittikten sonra hiperparametrelerini güncellediğin değerlendirme setiyle kontrol edersen modelinin çok iyi bir performans sergilediğini görürsün, ama aslında modelin kendi hatasına baka baka değerlendirme setini ezberlemiştir, biz buna overfitting diyoruz, yani modelin çok iyi gibi gözüküyor da aslında değil. O yüzden ikinci bir test seti kullanıyoruz test etmek için.",
1. -> Cevaplar için çok teşekkür ederim. Şimdi çok daha iyi anladımmm...",
    
### soru 

> quest: "Merhaba. Benim bir sorum olacaktı. Şimdi biz veri setini küçük gruplara ayırarak öğrenme işlemini bu küçük gruplar yani mini-batchler üzerinden devam ettiriyoruz. Peki bu mini-batchler üzerinden giderek bulduğumuz cost function'ı ve bu cost'u minimize etmek için kullandığımız gradient descent batchlere göre değişiklik mi gösteriyor. Yani her bir batch için ayrı bir cost function mı buluyoruz o kısmı tam anlamadım. Anlamadığım için de karıştırmış olabilirim ???? Düzeltilmeye açığım, vereceğiniz cevaplar için şimdiden teşekkür ediyorum. ????",

> comments:
  
1. ->  Merhaba, evet mini-batch için belirlediğimiz sayıda örneği kullanarak loss'u (yani gerçek sonuçlardan ne kadar uzak olduğumuzu) hesaplıyoruz ve parametrelerimizi (W, b) mini-batch üzerinde bulduğumuz loss'a göre güncelliyoruz. Seçtiğimiz farklı örnekler, farklı feature değerlerine sahip olduğu için, mini-batch'lerimizin loss'ları da doğal olarak farklı olacaktır. Umarım doğru anladım soruyu 😀",
2. ->  Teşekkürler Mert cevabın için. Peki genel olarak modelden dönecek olan weight ve bias, mini-batchlerden hesaplanan weight ve biasların ortalaması olarak mı olur? Yani nasıl formülize edilir? Bunun da yanıtını verebilirsen çok iyi olur 🙂.",
3. -> Sorunu tam olarak anlamadım ama baştan itibaren kendimce mantığını açıklamaya çalışayım. Linear Regression üzerinde çalıştığımızı düşünelim. İlk olarak w ve b değerlerimizi 0'a eşitliyoruz. y'(tahminimiz) = w.x + b gibi bir formülle ifade ediliyor. X ise feature vektörümüz, yani ev fiyatı tahmin etmek istiyorsak feature'larımız x = [oda sayısı, metrekare, evin yaşı, vb.] , ardından bu y' değerimizi hesapladığımızda fotoğrafta olduğu gibi bir doğru tahmin etmiş oluyoruz. Doğrumuzun örneklerimizin bulunduğu noktalara göre olan uzaklığı loss değerimizi ifade ediyor. Örneğin mini-batch-size=5 için kullandığımız örneklerin rastgele çok lüks evlerden denk geldiğini düşünelim. Fakat ortalama olarak o mahalledeki evler, lüks evler kadar pahalı olmayacağından çizgimiz noktalarımızla alakasız çıkacak ve loss yüksek çıkacak. Sonraki 5 örneğimize geçtiğimizde ise ortalama fiyatlara sahip evler geldiğini düşünelim. Bu sefer bu evler için yaptığımız tahminler gerçek sonuçlarımıza yakın olduğundan çizgiyi verilerimize daha uygun şekilde çizebileceğiz. Bu şekilde mini-batch'ler üzerinde işlem yaparak çizgimizi, noktalardan en az uzak olacak şekilde çizmeye çalışıyoruz. Bu kursta bahsedilmemiş terimler kullandıysam ve anlaşılmadıysa üzgünüm, açıklamaya çalıştım, umarım faydası olur :)",
4. ->  ->  Aklıma cevabınızı okuyunca bir soru geldi. Batch'in alacağı veriler rastgele seçiliyor dediğiniz gibi. Bu rasgelelik seçilen öğenin bir daha seçilmemesi üzerine mi yoksa rastgele veri çektiğimiz havuza her zaman veriseti içerisindeki tüm veriler dahil oluyor mu?.",
5. ->  ->  Bizim şu ana kadar kursta ilerlediğimiz kadarıyla epoch kavramının açıklanışı ve genel olarak bilinen yöntem, seçilen örneğin tekrar seçilmemesi diye biliyorum. Fakat yapay sinir ağları uygulamalarında seçilen örneğin tekrar havuza geri atılması kullanılıyormuş. With replacement - without replacement şeklinde geçiyor. Attığım linkte bahsettiğin durum ile alakalı bir değerlendirme var.[Should training samples randomly drawn for mini-batch](https://stats.stackexchange.com/questions/235844/should-training-samples-randomly-drawn-for-mini-batch-training-neural-nets-be-dr)
6. ->  ->  Anladım teşekkür ederim..",
7.   ->  ->  Her mini-batch için bir y' tahmin ediyoruz. yani örneklere en az uzak olucak sekilde çizgi çiziyoruz. nihai çizgiyi, nasıl elde ediyoruz? elimizde bir çok y' olucak. mesela arasındaki en iyisini mi alıyoruz? yoksa bütün y' alıp ortalamasını alarak en iyi tahmin edecek modeli mi elde ediyoruz? yada başka birşey mi.",
8. ->  ebubekir ceylan y' dediğimiz şeyler tahmin ettiğimiz değerler bunun modeli çizmekle dolaylı bir ilişkisi var. Modelin doğruluğunu ölçmek için Cost Functiondan kayıp değeri hesaplamamıza yarıyor. Bizim batchler sonunda elde ettiklerimiz bias ve weight değerleri. Ki bu da birden fazla olmuyor bir denklemdeki ağırlıkları değiştirerek kaybı azaltmaya çalışıyoruz. Ama tek bir denklem var elimizde. Ortalama vs söz konusu değil..",
9. ->  ebubekir ceylan Nihai çizgi, en son elde ettiğimiz weight ve bias değerlerini kullanarak çizdiğimiz çizgi oluyor. Önceki çizgileri sadece hatamızı hesaplayıp w ve bias değerlerimizi güncellemek için kullanıyoruz. Zaten mini-batch sayısı çok çok küçük belirlemediysek, her adımımızda daha iyi bir çizgi çizmemiz yani daha az cost'a sahip olmamız beklenir..",
    
### soru 

> quest: "Validation Set kısmında oluşan yeni iş akışında validation ile en iyi modeli seçin sonrasında test setine göre 2 kez kontrol edin diyor neden iki kez?Birde bu iş akışının test setine daha az exposures oluşturur derken ne demek istiyor anlayamadım?  Teşekkürler:)",

> comments:
  
1. -> Sanırım ingilizce olmasından ötürü farklı anlaşılıyor ????Ben şöyle anlamıştım, burada bir validation set üzerinde en uygun yaklaşımı aldık. Bu bizim için, öğrenim sonrasında validation set’e göre en iyi sonuç veren yapı. Bunu, test setinin üzerinde tekrar kontrol ederek, validation set’e overfit olup olmadığını kontrol ediyoruz. Yani ilk check, validation set üzerinde, ikinci check ise test seti üzerinde. Double check’i böyle anladım ben.",
2. -> Exposure'dan kastı da şöyle ki, Verimizi \"train-validation-test\" olarak bölmek ile \"train-test\" şeklinde bölmenin kıyasını yapıyor anladığım kadarıyla. Böylece, test üzerinde ustalaşmayan yapının doğruluğun, görseldeki yolla daha iyi görebiliyoruz..",
3. ->  Merhaba,Anladığım kadarıyla oradaki double check demesinin sebebi veriyi daha önceden validation set ile kontrol etmemizdi yani oradaki double, test setini iki kez kontrol et demek değil ikinci kez test verisinde kontorle t demek. Az exposure kısmı için validation set'i işin içine sokmamızın amacı ise, validation setin olmadığı iş akışında modeli optimize şekle ayarlamak için model eğitildikten sonra test setimizi deneyerek o anki optimumluk değerine göre bu workflow döngüsünü sürdürüyordu. Validation set ise training set içinden seçileceği için test ve train datasını ayırarak training sonrası test datasının unseen data olarak kalmasını sağlar. Yani test datasının exposure'luğu önlenmiş olur.İyi çalışmalar.",
4. ->  Oğuzhan ve Fethi'nin dediği gibi validation datası ile yapılan teste ek olarak test datasıyla da test edilmesi double check olmuş oluyor.Burada datamızı üç farklı parçaya bölmemizdeki amaç şu, train datası ile eğittik ve validation datası hiperparametrelerimizi ayarladık ve en iyi modelimizi seçtik. Peki bu ayarlamadan sonra ya modelimiz validation datasına overfit olduysa? İşte burada test datası devreye giriyor ve bir kez daha modelimizi test etmiş oluyoruz.",

### soru 

> quest: "Merhaba,   Anladığım kadarıyla Epoch, learning rate ve batch kavramlarını göstermeye çalıştım. Sizden talebim, hatalarımı düzeltmeniz çünkü sebebini bilmediğim bir şekilde anlamakta zorlandığımı düşünüyorum.  Veri setimizi, tüm veriyi tek seferde işlemek zorunda kalmasın diye batch'lere ayırıyoruz. Burada veri setimiz 10, batch boyutumuz ise 2.   10 veriyi, 2'li parçalara ayırdığımız için, toplam 5 kez (her seferinde 2 veri olacak şekilde) veriyi alıyor ve kendisini eğitiyor. bu işlemi 5 kez yaptıktan sonra yani tüm seti parça parça aldık böylece bir epoch bitti. Bu epoch'un sonucunun, feature'u ve bias'ı ne kadar değiştireceği de bizim learning rate'imize bağlı.  0,5 veya 1'se bu sonuç fazla etkileyecekken, 0.001 gibi bir değerse etkisi çok az olacaktır.  Burada bir hata var mı? Şimdiden teşekkür ederim.",

> comments:
  
1. ->  Doğru anlamışsın. Her batch'ten sonra modelin weight'leri ve bias'ları güncelleniyor, senin örneğine göre her epoch'ta 5 kez güncelleme yapılıyor, yani parametre güncellemesi her epoch sonunda yapılmıyor, batch bitince yapılıyor. Ama epoch sayısını istediğin kadar belirleyebilirsin bunlardan bağımsız olarak. Senin modelin veri setine epoch sayısı kadar defa maruz kalır. Umarım açıklayabilmişimdir.1 month ago 18 people like this.Like ReportReply",
2. -> ->  Çok teşekkür ederim, gayet iyi açıkladınız..",
3. ->  ->  Epoch sayısının çok fazla yaptığımızda overfitting oluşmuyor mu acaba ?.",
4. ->  ->  learning rate'in ve parametrelerin yeterince düşükse overfitting'e yol açmaz, aynı şekilde regularizasyonla da bunun önüne geçebilirsin, ama evet, mantıken overfitting'e müsait bir yapı varsa epoch sayısını arttırmak overfitting'i arttırır.",
5. ->  ->  Teşekkürler. Learning rate'in overfitting ile ilgili olduğunu bilmiyordum. Ama nasıl bir ilgisi var onu anlayamadım..",
6. ->  ->  demeye çalıştığım şey eğer learning rate’in ve parametrelerin düşükse overfitting olması için çok fazla epoch olması gerek, yani çok küçük adımlar atıyorsun sonuçta gibi düşünebilirsin. düşük learning rate de overfitting’e yol açabiliyor bazen, burada açıklaması var learning rate’ini probleme göre nasıl seçmen gerektiğinin [Link](https://mlexplained.com/2018/01/29/learning-rate-tuning-in-deep-learning-a-practical-guide/)
7. ->  ->  İnceliyorum teşekkür ederim..",
8. ->  \"tüm veriyi tek seferde işlemek zorunda kalmasın diye\" yerine \"tek bir parametre güncellemesi(weight update(iteration)) için bütün sample'ların üzerinden geçmesini bekleyerek zaman kaybetmemek amacıyla\" olarak güncelleyebiliriz sanırım..",
9. ->  First Steps with TF bölümündeki Linear Regression with Synthetic Data kısmının 5. görevinde sorduğun sorunun cevabı var ->  kısaca güzel açıklamış..",
10. -> Batch (küme) lere ayırmak istememizin nedenlerinden biri de dataset çok büyük olduğunda tek seferde bu kadar datayı tutarak işlem yapabilecek memory mizin olmayacağından dolayı. Ayrıca tek seferde ne kadar çok data olursa bu hesaplamalar matris çarpımlarına dayalı olduğu için matrisin satır sayısı artmış oluyor ve işlem süresi de baya artıyor.Bir tane data alıp loss hesaplayarak parametreleri güncellerseniz hızlı işlem yapıyorsunuz fakat kümeden çok farklı noktalar seçerek updatelediğiniz için stabil olmayan bir öğrenme gerçekleşiyor. Örneğin ders içeriklerinde örnek verilen sick-healthy trees datasetinde healthy lerin içinde bir tane bulunan sick örneği ile train ederken model parametreleri sapabilir. Bu yüzden daha fazla örneği kapsayan bir güncelleme daha doğru sonuç veriyor.Çözüm olarak da imkan verdiğince yüksek batch size ile başlayın, hızlanmak ve daha hızlı train edebilmek adına yavaş yavaş batch size ı düşürün, sonuçlar bozulup loss zigzag çizene kadar batch size ı azaltabilirsiniz öneriliyor.",
11. ->  Bu ayrımı yapmakta çok zorlanıyordum. Çok açık ve faydalı bir anlatım olmuş. Destekleriniz için çok teşekkürler..",
12. -> ->  Rica ederim, ben de uzun uğraşlar sonucu da oturtabilmiştim, size faydası olduysa ne mutlu 🙂.",
    
### soru 

> quest: "Merhaba,   Validation &amp; Test Sets Programming Exercise kısmında veri train ve validation olarak 2'ye ayrılıyor. 3'e ayırıp test seti kesinlikle train içinde kullanmamamız gerekiyor diye anlamıştım. Exercise'da en son problemleri çözdükten sonra test set kullanılıyor. Bu durum modeli başarılı yorumlamamıza sebep olmaz mı ?   Birde bu exercise'da veriyi karıştırararak, yeni index atayarak sanırım problemi çözmeye çalışıyor. Mantığı anlayamadım. Kısaca bahsedebilirseniz çok sevinirim .   Teşekkürler 🙂",

> comments:
  
1. -> Bu örnekte yanlış hatırlamıyor isem,verilerimizi okuyup scale ederken zaten test verimizi ayırıyorduk.Ancak anlatımda da bahsedildiği üzere,test verilerini modelin eğitimi sırasında validation için bile kullanmamız modeli etkileyecektir.Bu yüzden,train setin içinden bir validation set ayırıp eğitimimizi öyle gerçekleştiriyoruz.Ve evet modelin tahmin yeteneğini arttırır bu işlem.İkinci sorunuza istinaden,train set içerisinden validation setlerini ayırırken 0.8-0.2 oranını kullanıyoruz.Yani datanın ilk %80 lik kısmı (100 satır içinden ilk 80 satır gibi) train,%20 (son 20 satır gibi) validation için kullanılıyor.Ancak verilerimiz,ilk kolona göre sıralı olduğu için,modelimiz eğitim aşamasında yeteri kadar farklı örnek göremiyor.Bu yüzden,indexleri karıştırarak,yani veri setinin küçükten büyüğe olan sıralaması rastgele hale getirerek,elimizdeki veri çeşidini daha farklı örneklere kavuşturuyoruz mantığı bu.Kod yapısı için pandas dökümantasyonunu incelemenizi tavsiye ederim.Eksik veya yanlış söylediğim bir şey varsa düzeltin lütfen 🙂",
2. ->  ->  çok teşekkürler bazı kısımları kaçırmışım 🙂.",
3. ->  Merhaba,En başta internetten aldığımız 2 tane csv var. Bunlardan bir tanesi test için. Diğeri de training ve validation'ı barındıran csv. Yani test verisi training-validation verisi içinde yer almıyor.Diğer sorunuza gelince de şöyle örnek vereyim;- Elinizde bir veri var ve o veri bir sütuna göre sıralanmış ve siz bölmek istiyorsunuz. Peki böldüğünüzde elmalar bir tarafta armutlar bir tarafta kalırsa ne olur? Sadece elmalara göre eğitmiş olursunuz modelinizi ve armut görünce sapıtır. Bu yüzden sıralama olmadan karışık bir şekilde bölme yapılırsa daha anlamlı bir dağılım elde etme şansı artar. Umarım yardımcı olmuşumdur..",
4. ->   ->  teşekkür ederim.",
5. -> Merhabalar,İlk sorun için, 3 sete ayırmamızın sebebi aslında özet olarak Validation Set: Check Your Intuition başlığı altında bulunmakta, kısaca özetleyecek olursam:test setimizi her iterasyonda modelin verimliliğini test için kullanmamız halinde modelimiz test setimizin içermekte olduğu ya da olabileceği kendine has durumlara adapte olmasına sebep olabilmekte.(modelin test sete aşırı uyumu/ overfitting).Bunun yerine bir valiadation set ile her eğitim sonunda model etkinliğini test edip, nihai modele karar verdikten sonra test seti ile test ederek, modelimizin etkinliğini daha sağlıklı bir şekilde gözlemleyebiliriz. Bu şekilde modelimizin validation setimize aşırı uyum sağlayıp sağlamadığını tespit edebilir ve devam eden aşamalara daha sağlıklı karar verebiliriz.Veriyi karıştırmasının sebebi ise elimizde bulunan verinin longitude alanına göre küçükten büyüğe sıralanmış olması, bu durum her ne kadar veriden rastgele seçim yapsakta sıralı veri seçmemize sebep olmakta ve sonuçlara baktığımız zaman train ve validation loss değerleri arasında ki farkın fazla olduğunu görmekteyiz. Verinin sıralı olması yapılan ayrım sonucunda train ve validation setlerinin dağılımlarının birbirlerine yakın olmadığını göstermekte. Bunu düzeltebilmek için sıralı olan veriyi setini, karıştırmayı çözüm yolu olarak öneriyor ve uygulayarak sonuçları gözlemliyor.İyi çalışmalar ...",
    
### soru 

> quest: "Merhaba epoch batch size ve iteration kavramlarını kafamda pek oturtamadım bunları biz neden kullanıyoruz?",

> comments:
  
1. ->  iteration: yineleme demek ya Türkçesi ordan bakarsak olaya, modelin bir kere işlemesi (tabi diğer değişkenlere göre) 1 iteration oluyor anladığım kadarıyla. Batch size ise kaç örnekte bir iteration yani modeli baştan çalıştıracak onun sayısı. Örneğin 12 örnekli bir datasetinde batch size 6 ise burdaki 1 epoch ta 2 yineleme (iteration) olacak demektir. Bilmem anlatabildim mi? 🙂.",
2. -> Batch size: Bir kerede işlenen veri sayısı.Iteration: Tüm verileri işlemek için gereken batch size sayısı.Epoch: Tüm verisetinin iyileştirme aşamasından kaç kez geçirileceği.İterasyonları oluşturma yani batch size ayarlama sebebimiz tüm verilerin aynı anda işlenmesinin maliyetli olması ve daha uzun zaman alması. Çok fazla epoch olması durumunda hem eğitim süresi çok uzuyoor hem de overfitting sorunu baş gösteriyor.Anladıklarımı kabaca bir senaryoya dökeyim. 10.000 verimiz var ve batch size 200, tüm bu verileri işlemek için 50 iterasyona ihtiyacımız var. 1.epochun 1.iterasyonunda 200 veri geliyor modelimizin ilk parametrelerini oluşturuyoruz. Ve cost functiona sokarak kaybı hesaplıyor. 2.iterasyonda ise bu kayıp değerini azaltmak için parametrelerde değişiklikler yapıyor yeniden cost functiona sokarak kaybı hesaplıyor. 1.epoch sonunda tüm verileri işleyerek elde ettiğimiz bir modele sahip oluyoruz. 2.epochta amaç yine tüm verileri iterasyonlara bölerek yeniden işleyerek modeli daha da iyleştirmek..",
3. ->  Eğitmenlerimizden senaryonun doğruluğu hakkında onay almak güzel olacaktır 🙂.",
4. ->  1000 verimizin olduğunu düşünelim. Modele vereceğimiz her BİR veri \"iterasyon\"dur.Tüm iterasyonlar tamamlandığında yani 1000 verimiz tamamen modele verilip, loss değerlerinin hesaplanması, BİR \"epoch\"tur.Batch-size ise bu loss değerlerimizin işleme alınıp, ağırlıkların güncellemesinin sıklığını ifade eder.Yani Batch-size =100 yaparsak; her 100 iterasyonda (her 100 verinin işlenmesinde) modelin ağırlıkları güncellenecektir.",
5. ->  ->  Sanırım iteration tam olarak bu değil. Anladığım kadarıyla iteration parametre güncellemesi anlamına geliyor. Yani her weight update bir iteration'dır diyebiliriz. Bu durumda eğer 1000 sample varsa eğer SGD için iteration sayımız bir epoch için 1000 olacaktır ama mini-batch(batch-size=100) tercih edersek bu kez bir epoch için iteration sayımız 10 olacaktır..",
6. ->  ->  Haklısın. Yinelemenin tam tanımını şöylede düşünebiliriz; \"Yineleme, bir epoch tamamlamak için gereken batch sayısıdır\". Böylelikle 1 epoch yani toplamda 1000 veriyi 100'er veri içeren partilerle tamamlamak için 10 yineleme yapmamız gerektiği ortaya çıkıyor..",
7. ->  ->  \"iterasyon\" düzeltmesi için teşekkürler. Ben de şunu düzelteyim örneğinizde; SGD random seçtiği bir örnek üzerinden güncelleme yapar. Yani 1000 örneğimiz için bir epochta 1 güncelleme olur, \"iterasyon\" sayımız da 1 oluyor haliyle. Eğer GD yapsaydık o zaman 1000 örnek için bir epochta 1000 güncelleme olacağından, \"iterasyon\" sayımız da 1000 olacaktır. Kısaca GD-SGD farkına değinmek istedim ????.",
8. ->  teşekkürler.",
9. -> Arkadaşlar merhaba,Burada küçük bir karışıklık sezdim, onun için ek bir açıklama yapmak istiyorum :)İterasyon dediğimiz şey işlem sayısıdır, ML konularında bu işlem weight update'dir. Yani modelimizde bulunan weight ve bias'ın kaç kere güncellendiğidir.Epoch, modelinizin tüm data ile kaç kere eğitileceğidir. Eğer elinizde 10.000 adet veriniz varsa, modeliniz bir epoch sonunda tüm datayı görmüş olur.Batch ise çok yüksek sayıda veriyi aynı anda modele vermemek ve bu sayede oluşabilecek memory sorunlarına önlem olarak kullanılan, bir iterasyonda kullanacağınız veri miktarıdır. Örneğin batch_size 1000 alınırsa, 10.000 adet verinin içinden 1000 adet veri alınır, 1 kez weight update yapılır, ardından diğer veriler 1000er 1000er modele verilir ve her biri ile yine weight update yapılır. 10.000 verinin tamamı verildiğinde 1 epoch tamamlanmış ve veriniz 10 kere güncellenmiş olur..",
    
### soru 

> quest: "Validation test kısmındaki check your intuition bölümünü tam anlayamadım.Şöyle anladım,modeli eğitim setiyle eğiticez bu eğitim kısmında hiperparametreleri düzenleyeceğiz.En son test setiyle değerlendireceğiz.Yani her iterasyonda eğitim olur ama test en sonda olur.Doğru mu anladım?",

> comments:
  
1. ->  Benim anladığım kadarıyla şöyle bir şeyden bahsediyor orada -ki cevapla da örtüşüyor-- Bir training set ve bir test set var elimizde,- Her bir çevrimde önce train edip sonra test sete uyumuna bakıyor,- Haliyle aslında test setini de bir bakıma training set olarak kullanıyor ve overfitting oluşuyor..",
2. ->  Merhabalar, Kısaca özetleyecek olursam, umarım yanlış anlamamışımdır :)Ana başlığa göre:Modelimizi her bir iterasyonda eğitiyoruz ve her eğitimin sonunda test setimiz ile ölçümlerimizi yapıyoruz. Elde ettiğimiz sonuçlara göre parametrelerimizde değişiklik yapıp yapmayacağımıza karar veriyoruz.Burada ki problemimiz: Her bir iterasyonda test setimizi bir ölçüm seti olarak kullandığımız için, modelimizin test setinde oluşabilecek garipliklere uyum sağlayarak bizi aldatabilecek sonuçlar elde etmemize sebep olabilmekte.Bu sebepten ötürü de setimizi 3'e bölerek bir de validation_set oluşturuyoruz. Böylece Bütün eğitim sonunda elde ettiğimiz modelimizi test seti ile gözlemleyerek modelimizin validation_set'imize aşırı uyum sağlayıp sağlamadığını gözlemleyebilmekteyiz.İyi çalışmalar.",
3. ->  eğitim setiyle belirlediğimiz en uygun parametreleri test setinde tahmin edeceğiz diye biliyorum..",
4. -> Merhaba, benim anladığım kadarıyla amaç overfitting'i azaltmak ve modelin kalitesini değerlendirmek. Bu nedenle önce training setin bir kısmını split ederek validation_set'i oluşturuyor. Eğitim aşamasından sonra sonuçları değerlendirmek için doğrulama setini kullanıyor, doğrulama bittikten sonra test seti üzerinde test ediyor..",
5. ->  Eğitim verisetini eğittiğimizde model kendi başına overfitting oluştururken, test seti işin içine girdiğinde, test setinde kaybı düşürmek için yaptığımız müdahaleler sonucunda overfitting oluşuyor diye düşünüyorum. Yanlış mı düşünüyorum acaba?.",
6. ->  Merhaba, modelimiz eğitilirken her bir çevrimde test seti ile karşılaştırma yapılıyor. Burdan Loss alınıp model eğitiliyor. Bu durumda test setindeki veriler eğitimde kullanılmış oluyor. Daha sonra test aşamasında bu değerler eğitimde kullanıldığından dolayı overfitting ihtimali artıyor. Bu nedenle eğitim verilerimizi eğitim ve validation olarak ikiye bölüyoruz. Artık modelimiz eğitilirken validasyon dataları ile karşılaştırma yapıyor. Böylece test aşamasında test datalarımız eğitimde kullanılmadığı için daha önce karşılaşmadığı datalarla modeli test etmiş oluyoruz. Hem overfitting azalıyor hem de daha doğru bir test yapmış oluyoruz diye anladım ben. Yanlışım varsa mentör hocalarımız düzeltsinler lütfen. İyi çalışmalar dilerim.",
7. ->  ->  Merhaba, buradan anladığım kadarıyla ilk başta eğitim setimizi eğitim ve validation set olarak ayırıyoruz. Modelimizi eğitim seti ile eğitip validation dataları ile karşılaştırma yapıyoruz. Validation datalarımız ile karşılaştırma yaparken modelimizin daha doğru çalışması için parametrelerimizi en doğru sonuca ulaşana kadar tekrar tekrar değiştirip validation datalarımız ile test ediyoruz (Bu kısım doğru mudur?). En sonunda da test datalarımızı modelimizde kullanıp karşılaştırma yapıyoruz. Özet olarak sormak istediğim eğitim setimizi böldükten sonra modelimizi eğitim setiyle eğitip en uygun sonuca ulaşana kadar parametreleri değiştirip tekrar tekrar validation datalarımız ile mi karşılaştırıyorz? Yanlış anlamış olabilirim düzeltebilirseniz sevinirim. Teşekkür ederim..",
8. ->   ->  ben de sizin gibi düşünüyorum. Ancak kursta öğrenciyim yanlış bir bilgi verip kafa karıştırmak istemiyorum. Aşağıda paylaştığım linkte validation set kısmındaki açıklama ile de örtüşüyor dediklerimiz. Ek olarak eğer validation set olmasa bu işlemde test seti kullanılacak. Böylece modelimiz test setindeki verilerle çalışmış olacak. Modeli en son test ettiğimizde modelin daha önce görmediği verilerle test etmemiz bize daha doğru sonuçlar verecektir. Bir yanlışlık varsa mentör hocalarımız yardım etsinler lütfen. İyi çalışmalar dilerim.["About Train, Validation and Test Sets in Machine](https://towardsdatascience.com/train-validation-and-test-sets-72cb40cba9e7)
9. ->  ->  Teşekkür ederim bilgiler için. İyi çalışmalar..",
10. ->  Modelimizi training data üzerinden eğitirken test ile de modelin genelleme yeteneğini ve ne kadar iyi çalıştığını test ediyoruz. Ancak burada şundan bahsetmiş, peki train datası üzerinde eğitim yaparken test sonuçlarına bakarak learning rate gibi parametreleri daha iyi bir model oluşturmak için değiştirebilir miyiz? Cevap olarak ise bu da test datasında iyi bir sonuç alınabilir ama bu seferde seçilen test verilerine özgü bir modelleme olur ve genelleme yeteneği düşük olur..",
    
### soru 

> quest: "Merhaba, Generalization kısmında ki ML Fine Print altında anlatılanı anlayamadım.Tam olarak ne demek istiyor acaba?  Teşekkürler",

> comments:
  
1. -> Merhaba,Burada Generalization yani modelimizin daha önce görülmemiş bir veriyi tahmin etme yeteneğine rehberlik ve etki eden 3 varsayımdan bahsetmiş:1.(Independently and Identically): Burada verilerinizi random olarak almanız gerektiğinden bahsediyor. Örneğin elinizde insanların yaş, boy, kilo, cinsiyet ve ülke bilgilerini içeren bir veriseti var. Burada ülke sizin labelınız, diğer alanlar ise featurelarınız olsun. Eğer verisetinizi eğitime sokarken random olarak almazsanız eğitim-test bölme işleminde eğitim verisinde sadece tek bir ülkeye ait veriler gelmiş olabilir. (Örneğin elinizde TR-US-FR ülkeleri olduğunu düşünün. Eğitim setinde sadece TR verileri olursa modelimiz FR ve US verilerini tahmin etmekte efektif çalışmayacaktır.)2. (Stationary): Burada da verilerimizin dağıtımının sabit olduğundan bahseder. Örneğin verisetinizde bir şemsiye şirketinin satış listesi olsun. Şemsiye satışları mevsimsel olarak farklılık göstereceğinden bu dağıtım sabitliğini ihlal eder.3. Bu kısmı tam anlayamamakla birlikte anladığım kadarıyla örneklerimizi aynı dağıtımın içindeki bölümlerden çiziyoruz bunu nedeni test ve eğitim verilerimizi birbirinden farklı distributionlarda olursa test loss'umuzun fazla olacağı yani tahminleri doğru yapamayacağıdır.İyi çalışmalar.",
2. ->  ->  3. Kısım Dağılımların aynı olmasın bahsetmektedir. Çünkü dağılımın değişmesi ile verinin karakteristiği değişir, ve elinde iki farklı veri seti olmuş olur. (Her dağılımın kendisine has olan çözüm yönetim vardır.) Bu durumda biri ile eğitim yaparak diğeri ile test etmen sağlıklı sonuçlar vermeyecektir.,
3. ->  ->  Açıklama için teşekkür ederim 🙂 İyi çalışmalar dilerim..",
4. -> ->  Stationary'den kastımız verinin ortalamasının, varyansının ve kovaryansının sabit olması değil midir?.",
5. ->  -> Evet, ortalama, kovasyans ve varyans değerleri Stationary'de sabittir. Bu konu ile alakalı [Link](http://people.duke.edu/~rnau/411diff.htm) linkinde yazılmış çok güzel bir yazı mevcut, okumanızı tavsiye ederim 🙂.",
6. ->  Çok teşekkürler şimdi daha iyi anladım:).",
    
### soru 

> quest: "Merhabalar. Bazı terimleri Türkçe nasıl açıklayacağımı bulamıyorum. Örneğin tuning ve fit ifadelerinin bu işin içinde çalışanlar nasıl Türkçeleştiriyor? Fit için \"uydurma\" Tuning için \"ayarlama\" ifadesini kullanmak anlamının kaybolmasına yol açıyormuş gibi geliyor. Mediumda yazı yazmak gibi bir hedefim var. Öğrendiklerimi en iyi şekilde açıklamak istiyorum.  Bir de \"offset\" tam olarak nedir?",

> comments:
  
1. ->  tuning final modeli oluyor fit etmek train verilerini eğitmek oluyor diye biliyorum..",
2. ->  Bence her şeyi Türkçe olarak yazmak zorunda değilsiniz. Eğer kelimeyi Türkçe yazdığınızda daha zor anlaşılacaksa, Türkçe yazmaya çalışmanın çok da bir anlamı yok. Bazı kavramlar oturmuş ve sürekli İngilizce karşılıkları karşımıza çıkıyor. Ayrıca bazı kelimelerin de tam karşılıkları yok. Tabii bu konuda net bir doğru yok, dilimize oturması için sürekli Türkçe yazılması daha iyi olur diyenler de olabilir. En azından 2020 yılında tamamen Türkçe içerik yazmak zor olabilir 🙂 Belki 10 yıl sonra dilimizde tam karşılıkları olur ve insanlar Türkçe olarak bu kavramlara aşina olur.",
3. ->  ->  Anladım. O zaman olduğu gibi kullanmak dediğiniz gibi en mantıklısı olacaktır. Teşekkürler..",
4. ->  [Link](https://github.com/deeplearningturkiye/turkce-yapay-zeka-terimleri/blob/master/ingilizce-turkce.md) Burayı kullanabilirsin. Ethem Alpaydın Yapay Öğrenme kitabında kullandığı Türkçe karşılıklar oldukça değerli. Türkçe anlaşabilmemiz bu kavramları Türkçeleştirebilmenin her bilim dalında olduğu gibi çok önemli olduğunu düşünüyorum. Ian Goodfellow, Yoshua Bengio ve Aaron Courville'nin Deep Learning kitabının Buzdağı Yayınevinden çıkan çevirisinde çeviri ekibinin kullandığı terimleri de mutlaka kullanmalı..",
5. ->  ->  Dediğiniz kaynağa bakıyorum kafama takılan bir şey olunca. Ama fit yerine \"uydurma\" kelimesini kullanmak biraz zorlama gibi durduğu için acaba kullanılıyor mu diye öğrenmek istemiştim. Teşekkür ederim..",
6. ->  Ben de Türkçe kullanmaya gayret eden biriyim. Eğer Türkçe karşılık olarak tam ifade eden kelime yoksa İngilizce kullanmakta sakınca olmayacaktır. Bu durumu da yazınızın sonunda bir dipnot şeklinde belirtebilirsiniz yazarlar genelde öyle yapıyor",
7. ->  ->  Dediğiniz gibi mini bir sözlük eklemeyi düşünüyorum yazdığımda. Teşekkür ederim..",
8. ->  Fit kelimesinin karşılığı olarak uyarlama kullanılabilir..",
    
### soru 

> quest: "Merhaba, Scale işleminin tam olarak ne yaptığını anlayamadım. Biraz basit bir soru oldu galiba, özür dilerim.",

> comments:
  
1. ->  Scale işlemi genelde verileri yorumlamayı kolaylaştırıyor. Örneğin nüfusun yaşa göre dağılım verilerimiz var. 25 yaş birey sayısını toplam sayıya bölerek 0 ve 1 arasına yani olasılıksal yorumlanması için gerekli aralığa düşürmüş oluyoruz. Öte yandan grafik çizerken bazı değerlerin 100000 lerde olduğunu (örneğin ev fiyatı) bazılarının ise 10 lardan daha küçük olduğunu (örneğin oda sayısı) düşün. Görselleşirmek zor olacaktır. Eğitmenlerimizden gerekli düzeltmeleri bekliyorum.",
2. -> Andrew NG de bu şekilde açıklıyor. Özetleyecek olursak convex şeklin daha düzgün oluşması için scaling yapıyoruz. Daha düzgün oluşması ise şekilde görüldüğü üzere daha az ve doğru yönden sapmayan adımlarla global minimuma ulaşmamızı sağlar..",
" -> Yani anladığım kadarıyla, kıyasın daha doğru yapılabilmesi için, değeri belirli bir sayıyla çarpıyor veya bölüyoruz. Böylece görselleştirme olsun, bizim modelin gidişatını anlamamız olsun bu konularda yararlı oluyor..",
3. ->   -> Aynen öyle..",
4. -> ->  Çok teşekkür ederim..",
5. ->   -> evet ölçeklendirme yapıyoruz..",
6. ->  Verilerimizi ölçeklememiz featurelar arasındaki uçurumu azaltacaktır. Andrew Ng'den alınan -> 'ın paylaşatığı grafiğe bakarsanız elimizde ev boyutu ve evdeki lavabo sayısı adlı featurelarımız var. Bu featureların değer aralıkları ev boyutu için (0-2000), lavabo asyısı için (1-5). Bu değer aralıklarının çok farklı olması gradient descent fonksiyonumuzun yavaş çalışmasına neden olacaktır çünkü resimdeki soldaki garafiğimizde optimum değer olan en iç değere yaklaşması uzun sürecektir. Bu problemi çözmek için feature scaling yapabilirsiniz. Feature scaling aslında tüm featurelarınızın belli bir değer aralığına alınmasıdır. En optimum değer aralığı diye bir şey yoktur ama olabilrdiğinde birbrine yakın küçük değerler arasına alınmaya çalışılabilir. Örneğin -1About Feature Scaling and Normalizationsebastianraschka.comSections",
7. ->  Arkadaşların teknik olarak açıklamasının yanında bana hitap eden kısmı şöyle. Görselleştirme ve grafiklerde çok büyük rakamlar bu mantıkla bin ya da milyon ölçeğinde gösterilir ki veri okuması kolay olsun, görsel çirkin gözükmesin. En basit haliyle model yazmaya başlamadan önce define gibi komutlarla veriye genel bir bakış açısı ile baktığınızda, her bir feature milyon seviyesinde olursa okuması bakan kişiyi yoracaktır ve tek bir sayfaya sığmasını zorlaştıracaktır. Veri Biliminde en önemli şeylerden birisinin de kodunuzun okunabilir olması. Çalışmanıza bakan diğer insanları düşünerek en sade ve yalın şekilde olması önemli. Bu nedenle tekniğin de ötesinde bu sebeplerle ölçeklendirme önemli bence..",
8. ->  Verileri aynı dünyaya indirgeme işlemidir. Böylece modelimiz daha iyi ve hızlı öğrenir. Yapay zeka da yığın normalizasyonu denilen işlem vardır. Veriler bir sonraki katamana gönderilmeden önce açık bir şekilde normalize edilir. Çünkü sonraki katman, daha önce bu katmana gönderilen dağılıma benzeyen bir veri yığını beklemektedir..",
9. ->  Scale yani ölçeklendirme işlemi biraz daha okunaklı hale getirmek için kullanılmış. Yani 1000 e bölüyor ev fiyatlarını daha okunaklı bir değer ortaya çıkıyor. \"/=\" ifadesi zaten pythonda aynı değeri yani median_house_value değerlerini 1000 e böl ve tekrar eşitle anlamında..",
    
### soru 

> quest: "Merhabalar, teorik olarak anladığımı düşündüğüm şeylerin Playground Exercise lar esnasında yerine oturmadığını fark ediyorum. Örneğin Training and Test Sets: Playground Exercise kısmında taskleri yerine getirip, gözlem yapsam da bazı neden sonuç ilişkilerini kuramıyorum. Task 2 ve 3 teki  \"Is the delta between Test loss and Training loss lower or higher with this new Learning rate? What happens if you modify both Learning rate and batch size?\" ve \"Does altering the training data percentage change the optimal learning settings that you discovered in Task 2? If so, why?\" sorularının cevaplayamadım.  Yardımcı olabilirseniz sevinirim ve aynı zamanda kafamda daha iyi oturtabilmek adına gelebilecek tavsiyelere de açığım. Teşekkürler.",

> comments:
  
1. -> Task 1’de modeli eğittiğimizde overfitting(test loss >> training loss) olduğunu görüyoruz. Yani modelimiz eğitim verilerine çok iyi uyuyor, ancak yeni bir veri geldiğinde veriyi doğru şekilde genelleştiremiyor.Task 2’de learning rate’i azalttığımızda, test loss değerinin training loss değerine yaklaştığını görüyoruz. Batch size’ı arttırdığımızda, test loss değerinin traning loss değerinin birazcık altına düştüğünü görüyoruz. Peki bunu niye yapıyoruz? Amaç burda perfect fitting’i(test loss ~ training loss) yakalamak. Yani her iki değerin de kabaca aynı veya birbirine yakın değerler olmasını istiyoruz. Task 3’te ise training data percentage oranını %10 olarak ayarladığımızda, verilerin %10’u training set için, kalan %90’ı test set için kullanılıyor. Training setindeki veri yüzdesini bu kadar azaltmak veri noktalarının sayısını büyük bir oranda azaltıyor. Çünkü eskiye göre daha az veriyle eğitim sağlamış oluyoruz. Learning rate ve batch size’a yüksek bir değer verdiğimizde, eğitim modelindeki düzensiz atlamaları görüyoruz. Loss curve deki minimum pointe asla ulaşamıyor, tekrar tekrar üstünden atlıyor. İyi bir model için yeterli ölçüdeki bir training sete ihtiyacımız var. Ben öğrendiklerimle bu şekilde yorumladım, yanlışım varsa arkadaşlar düzeltirse sevinirim.", 
2. ->  ->  Sanırım Task1'de overfitting gerçekleşmiyor aksine learning rate çok yüksek olduğu için modelimiz training loss'u yeterince düşüremiyor yani yeterince iyi öğrenemiyor ve bu da delta'nın (test loss - training loss) yüksek olmasına sebep oluyor çünkü iyi öğrenememesi sebebiyle yeni gelen test datasında iyi tahmin gerçekleştiremiyor. Yani çok iyi öğrendiği(overfit) için değil yeterince iyi öğrenemediği için (test loss >> training loss).",
3. ->  ->  Düzelttiğiniz için teşekkür ederim, ben de doğrusunu öğrenmiş oldum böylece..",
4. ->  Teşekkür ederim şimdi daha iyi anladım..",
    
### soru 

> quest: "Merhabalar. Öncelikle yardımlarınız için teşekkür ederim. Sorularımı çalışmam bittikten sonra teker teker sormak istedim. Gradient Descent optimizasyonunu önceden tek seferde sürekli yineleme yaparak optimum değerleri buluyor gibi düşünüyordum. Ama epoch kavramı işin içine girdi ve kafam biraz karıştı. Tek veya birden fazla epoch arasında ne fark var anlayamadım. Batch_size olarak verileri rastgele almadığımızı, tüm veriseti aldığımızı düşünürsek sürekli aynı sonucu üretmez mi? Epochlarda ne oluyor ki değerlerimizi sürekli optimize edebiliyoruz? Bir de diyelim ki her epochda ikişer iterasyonumuz var. Resimde gördüğümüz şekilde 1. epochta 2 nokta ilerleyip ikincisinde kaldığımız noktadan devam etmiş mi oluyoruz? Epochlar arası geçişlerde elimizde olanları kafamda tam olarak  oturtamadım.",

> comments:
  
1. ->  Benim de burada oturtamadığım şeyler vardı. Bu linkte tartışmıştık kafandaki soru işaretleri gidecektir. İyi çalışmalar dilerim. [Link](http://community.globalaihub.com/community/status/1377-1377-1586334163/?t=1586558632#comment.2492.2559.2585.2606)
2. ->  ->  Hala gitmedi maalesef. Epoch artınca belirli bir seviyeye kadar maliyet azalıyor konuşmalardan sadece bunu çıkarabiliyorum ki zaten eğitimde bundan bahsediliyor oradan biliyorum ancak öğrenmek istediğim tam olarak bu değil..",
3. ->  ->  1 epoch'ta bütün weightler 1 kere güncelleniyor. 10 epoch yaparsan bütün hepsini 10 kere güncellemiş olursun. Yani daha fazla yakınsarsın..",
4. ->  ->  Her epochun içinde ağırlıklar iterasyon başına güncelleniyor..",
5. -> ->  ağırlık iterasyon başına güncelleniyor diye biliyorum hocam.",
6. ->  Elinde 200 sayfalık bir kitap var diye düşün. Amacın bu kitapta yazılanları öğrenmek. Bunun için bir strateji belirledin kendine. Dedin ki ben bunu 5 sayfalık parçalara ayırıp her bir parçayı çalıştıktan sonra bir sonraki kısıma geçeceğim.Batch size dediğimiz şey modelin parametrelerini güncellemeden önce üzerinde çalışılacak eğitim verisi sayısı. Yani kitap örneğinde bu sayı 5. Modelin y = w.x + b olduğunu kabul edelim.İlk olarak w ve b parametrelerine rasgele değerler veriyoruz. Sen 5. sayfayı okuduktan sonra w ve b parametrelerini güncelliyorsun. Kitabı bitirmen için 200/5 = 40 adet batch'in mevcut. Yani model parametreleri 40 kere güncellendi ve kitabı tamamladın. Epoch tüm kitabın tamamlanma sayısı. Yani şu ana kadar 1 epoch tamamlanmış oldu. Kitabı kaç kere okuyacağımız bize kalmış. 100 kere okursak kitabı 100 epoch'a ihtiyacımız var.",
7. ->  ->  Epochu tamamlayıp 2. epocha geçtiğimde kitaptan 1.epochta öğrendiklerimi pekiştirerek devam ediyorum diyebilir miyiz? 2.Epocha geçerken birinciden elde ettiğimiz modele göre loss belirleyip her epochta bunu tekrar mı ediyoruz? Ve 2 nokta atlama olayı hakkında bir fikir verebilir misiniz acaba? Bu şuna mı benziyor bir kitabı bir gün okuyup bitirmek yerine bugün 2 kez yani iter 5er sayfa okuyup devamını ertesi gün yani 2.epochta okumak?.",
8. ->  ->  Birinci sorunun cevabı evet. 2. Epocha geçerken birinciden elde ettiğimiz modele göre loss belirleme ifadesi yerine şöyle diyelim: Loss fonksiyonu en başta tanımlanır. Örneğin regresyon problemi için ortalama kare hata çok kullanılan bir loss fonksiyonudur. Bunu en başta belirlersin. Dolayısıyla her epoch'ta aynı loss fonksiyonu kullanılır. Loss fonksiyonunun değerlerine göre modelin parametreleri güncellenir. Bu güncelleme her bir batch_size tamamlandığında gerçekleşir. Kitap örneğinde senin ifadenle nokta atlama 5 sayfalık dilimi tamalayıp diğer 5 sayfaya geçtiğinde oluyor. 1 epoch kitabı bir kere bitirmek demek..",
9. ->  Kitap örneğinin doğru bir örnek olabilmesi için tabii ki bilgilerin üst üste giden bilgiler olmadığını da belirtmem lazım..",
10. ->  peki epoch sayısını çok fazla artırınca ne olur? sanırım loss değeri artıyor ama nedenini anlayamadım?.",
11. ->  ->  Epoch sayısını çok fazla artırırsanız mutlak sıfıra yakınsarsınız. Eğer loss değeriniz artıyorsa learning_rate değerinizi çok büyük seçmişsinizdir. Dolayısıyla learning_rate değerinizi küçültmeniz gerektiğini anlayabilirsiniz böyle bir durumla karşılaşırsanız.,
12. ->  Kurs içerisinde de şöyle bir ifade geçiyordu: \"Epoch sayısını ve batch size arttırırken learning rate azaltmak genelde iyidir.\".",
13. ->  ->  cevap icin tesekkur ederim, simdi daha iyi anladim..",
14. -> Merhabalar,[Link](http://community.globalaihub.com/community/status/190-190-1586531975/?t=1586595736#comment.2872.2860.2900.2877) Burada makina öğreniminin rastgeleliği ile alakalı bir paylaşım olmuştu. Bu postu işaret etmemin sebebibu rastgelelik 1 epoch için de geçerli 100000 epoch içinde. Epoch kavramını basitçe tanımlamak gerekirse: elinde bulunan veri seti'ni epoch olarak düşünebilirsin. batch_size ise elinde ki veri setini nasıl alt kümelere ayıracağına karar verdiğin değer, iterasyon ise basitce veri setinini büyüklüğünün batch_size ye oranı. Yani elinde bulunan verinin büyüklüğünü 1000 olduğunu varsayalım, batch_size değerini de 100 bu durumda 10 küme elde edebilirsin. Bu 10'da senin iterasyon boyutun oluyor.Ancak epoch, batch_size ve iteration_size üçlüsü rastgelelikle alakalı kavramlar değiller. Bu kavramları nasıl değiştirirsen değiştir rastgelelik ortadan kalkmayacaktır. Sonuçların her denemede farklı olmasının sebebi kullanılan yöntemin \"Stochastic\" olması.Stochastic Gradient Descent' in son cümlesi: \"The term \"stochastic\" indicates that the one example comprising each batch is chosen at random.\" şeklindeydi. Mini-Batch SGD için bu cümleyi one example kısmını batch_size olarak düşünebilirsin.Bir eğitim süresince bütün epoclar birbiri ile bağlantılı olarak işleyecektir. Yani bir epoch'un çıkış değeri diğer epoch için giriş değeri olmaktadır. Bu şekilde optimal loss değerine ulaşana kadar işlemlere devam edilmektedir.İyi çalışmalar.",
15. ->  ->  ->  Çok teşekkür ederim kafamdaki tüm sorulara cevap buldum sayenizde..",
    
### soru 

> quest: "Merhaba, First Step with TF da yapılan araştırmayı yaptığımda şöyle bir sonuç gördüm.  Batch Size değiştirdiğimde bir farklılık göremedim. Epoch sayısını arttırınca ve learnin rate arttırınca  rmse azalma oldu. Genelde hep bu şekilde mi sonuçlanıyor yoksa bu örneğe istisna bir şey miydi? Bu konuda yardımcı olabilir misiniz?",

> comments:
  
1. -> yapılan aşamaları sonda özetlemeye çalışmış.",
2. ->  bu konuda ben de merak edip farklı denemeler yaptım ama gördüğüm kadarıyla yalnızca eğitim hızına etki eden bir parametre gibi , ama tabiki kesin bir şey söylemem .. Daha deneyimli birinden öğrenmek daha sağlıklı olur.",
3. -> Merhabalar. Öncelikle Modellerde bir genelleme yapmak çok fazla söz konusu değil. Hiper parametrelerin hangisinde nasıl bir ayarlama yapacağınız, elinizdeki veri setine göre değişiklik gösterebiliyor. Özellikle batch-size ayarlamaları, çok büyük boyutlarda veri setiniz olduğunda size lazım olacaktır. Küçük tuttuğunuzda çok fazla zaman alacak ve bu da size maliyet anlamına gelecektir. Gelelim learning rate durumuna. Learning rate için de değer ayarlama önemlidir. Çok küçük tuttuğunuzda modeliniz öğrenmesi için çok fazla epoch gerekebilir hatta öğrenmeyebilir. Çok büyük tuttuğunuzda da aradan kaçırabileceğiniz değerler olabilir bu da öğrenme grafiğinizde dalgalanmalara sebep olabilir. Epoch değeri ise yine verisetine göre değişebilir. Gereksiz çok uzun tutulan epoch sayısı size zaman maliyetine sebep olacaktır. Hatta buna ek olarak bir süre sonra loss değerinizin artmasına da sebep olabilir. Tüm bunlardan, aslında bir şöyle bir çıkarım yapmak gerekiyor. Burada genelleme yapmayıp, her bir parametrenin işlevinin ne olduğunu kavramaya çalışmak gerekiyor. Hepimize iyi kurslar diliyorum.",
4. ->  ->  teşekkür ederim Serkan Bey.",
5. ->  Merhaba, bu konuyla alakalı Linear Regression with Synthetic Data kısmının sonunda seçenekler Summary of hyperparameter tuning kısmında özetlenmiş durumda. Buradaki aslında pük nokta hyperparametre üzerindeki değişikliklerin ya da etkilerin grafikler üzerinden okunması. Grafikler, hangi parametre üzerinde ihtiyaç ya da değişikliğe gitmemiz konusunda ipuçları veriyor..",
    
### soru 

> quest: "1- Kodlama Pratiği kısmında gerçek veriler bölümünde (Regression with a Real Dataset ) en son TASK,  Ne yaptıysam, gerçek değerlere yakın bir tahmin ürettiremedim. Yapanlar var mı?  2- Bias Kavramınıda tam kafamda oturtmuş değilim. Neden acaba? Bu tanım olarak anlıyorum biasta eğitime giriyor, yani aslında bias mantığıyla hataları öğretmek ve onlardan kaçınmak için mi? binevi hatalardan ders çıkar mantığı?  3- Birde epoach ile iterasyon arasındaki fark nedir? mini batch değeri 30 olduğunda 1. iterasyonda 30 kümeyi parça parça işliyor anlamında mı? Sonra 2. iterasyonu istersen yine 30 kere öğrenmeye devam ediyor.  4- GRAFİKLİ ŞEKİLDE, \"some fraction of the gradient's magnitude\" ile kastetiiği şey nedir? Eklediği şeyi anlayamadım. Terimlere uzağız galiba bağdaştıramadım.  Şimdiden çok teşekkür ederim cevaplarınız için..",

> comments:

1. ->  2 - Bias Linear Regression da doğrumuzun (modelimiz) y - ekseni üzerindeki konumunu ayarlamada yardımcıdır. Eğer bias değeri vermezsek modelimiz originden geçmek zorunda kalacaktır ve bu çoğu veri setinde muhtemelen kötü sonuçlar elde etmemize neden olacak ve modelimize zarar verecektir. 3 -epoch - iteration - batch size arasındaki bağlantı için örnek: 12 verilik veri setimizde batch size = 12 olsun, bu durumda bir epoch 1 iterasyon ile sonlanacak, bir de batch size = 6 iken deneyelim, bu durumda bir epoch 2 iterasyonda sonlanacaktır. 4 - belirlediğimiz learning rate sayesinde bulduğumuz sıçrama mesafesi ile eğrimiz üzerinde sıradaki konumumuzu belirliyoruz, eğri üzerinde bu şekilde gezinerek modelimize vermemiz gereken optimum ağırlık değerlerini arıyoruz. Eksik veya yanlış bir bilgi varsa lütfen bilgilendirin. İyi çalışmalar..",
2. ->  ->  Peki o zaman Epoch sayısını arttırdığımızda nasıl oluyor? Yine bir örnekle açıklama imkanın olur mu rica etsem? Sonucunu biliyorum, mantığını anlamaya çalışıyorum ezber olmaması için 🙂.",
" ->  -> Batch_size'ı sabit tutarak, elimizdeki veri seti 10 epoch ile underfitting, 50 ile optimum , 100 ile overfitting modeller oluşturabilir. Burada iterasyon sayısı aynı çünkü batch değişmiyor. Overfitting ve underfitting konularını biraz daha araştırıp, batch, epoch sayılarını da çok yüksek veya düşük değerler ile model üzerinde denediğinde konunun netleşeceğini düşünüyorum..",
3. ->  ->  Çok teşekkür ediyorum, tavsiyeni dikkate alacağım...",
4. ->  1-predict_house_values fonksiyonunda hata olduğunu başka bir postta yazmışlardı. O yüzden print ettiğiniz feature value, label value değerleri predicted value değerlerinin gerçek değerleri değil. Ben predict_house_values fonksiyonunda şu kısmı değiştirdim:print (\"%5.0f %6.0f %15.0f\" % (training_df[feature][10000+i],training_df[label][10000+i],predicted_values[i][0] ))Sonra sizin yazdığız değerler ile eğittim modeli. Loss function zaten nerdeyse değişmiyor yani zaten olabilecek en iyi score bulmuşsunuz gibi gözüküyor. Root_mean_square 83 civarı ve sonuçlar da ona uygun. Tek özellik ile tahmin yapıldığını göz önüne alınca çok iyi bir score beklemek doğru değil zaten..",
5. -> ->  Çok teşekkür ederimm..",
6. ->  4-\"some fraction of the gradient's magnitude\" dediği resimdeki kırmızı ok. w parametresi güncellenirken w:=w-learning rate*(Loss fonksiyonunun w parametresine göre kısmi türevi). \"Loss fonksiyonunun w parametresine göre kısmi türevi\" kısmı gradyantın genliği oluyor. some kelimesini kullanmasının nedeni learning rate faktöründen kaynaklı olduğunu düşünüyorum. Çünkü parametreyi güncellerken genliği learning rate ile çarptığın için scale etmiş oluyorsun. kırmızı ok learnin rate*magnitude oluyor yani. bu değer sonra w değerinin o anki olduğu konumunun üzerine gradyanın negatif yönünde(loss functionun azalması için) ekleniyor ve yeni w değeri bulunuyor. Biraz karmaşık oldu sanırım ama parametre güncelleme ile ilgili görsel ekledim daha açıklayıcı olabilir..",
    
### soru 

> quest: "Merhabalar,  Linear Regression with a Real Dataset kısmında aklıma takılan bir kısmı sormak istiyorum. Batch değerleri n e bağlı olarak training_df 'in o aralıktaki değerlerinden oluşuyorken ve bu değerleri  alıp prediction işlemi gerçekleştiriliyorken aşağıda for içerisinde indexi direkt olarak i kabul etmiş.Burada olması gereken 10000+i değil midir(Sadece training_df  için )? Yoksa ben mi bir şeyleri kaçırdım.",

> comments:
  
1. ->  Merhabalar,Evet dediğiniz gibi olmalı, tahmin için kullandığı değerler yerine veri setinin ilk değerlerini almış. Yazım sırasında gözden kaçırılmış olmalı.İyi çalışmalar.,
    
### soru 

> quest: "Merhaba benim sorum NumPy Ultraquick tutorial ile  alakalı. \"np.random.random([6])\" bu kod satırı bize 0'la 1 arasındaki  random floating değerlerini buluyor.Peki -2 ile +2 arasındaki floating değerlerini nasıl bulacağız?",
> comments:


1. ->  0 ile 1 arasında dödüğü için (np.random.random() * 4) - 2 yapılacak. Şöyle düşün en küçük değer 0 olabilir ve bizim yazdığımız denklemde 0*4 - 2 = -2 oluyor . En büyük değer için ise 1 olabilir, 1 ise 1*4 - 2 = 2 olabilir. Bu şekilde düşünmemiz gerekiyor. Bu denklemi nasıl elde ediyoruz dersen hiç araştırmadım ama mantığım şu yönde. en küçük sıfır gelceği için aralıktaki en küçük sayıyı + olarak yazmak ve istenilen fark ile çarpmak. Çarpmak dediğim yukarıda gösterdiğim şekilde tüm terimi değil..", 1. ->  yada şu şekilde bir kısayolu var : np.random.randint(low = -2, high =2, size=(6)) -2 ve 2 arasında 6 değer atar.",
2. ->  ->  Ama bu sadece bize tam sayıları veriyor, bize ondalıklı sayılar lazım..",
3. ->  teşekkürler.",
4. ->  np.random.uniform(low=-2, high=2).",
5. ->  import numpy as np4*np.random.random_sample((5,))-2.",
6. ->  ->  Bu konu aralık aritmetiği (interval arithmetic) ile alakalı. Bildiğimiz aritmetik işlemler aralıklar üzerinde de geçerli. Aslında şöyle bir denklem kuruyoruz x.(0,1) + y = (-2,2) . Burada (0,1) 0 1 aralığını gösteriyor.Denklemin çözümüx.0 + y = -2x.1 + y = 2den x =4 y=-2 olarak bulunuyor. Dolayısıyla (np.random.random() * 4) - 2 işlemi bize istediğimiz aralığı veriyor.",

### soru 

> quest: "Merhabalar,  Linear Regression with Synthetic Data çalışma kodunda Task 2 'de epoch sayısını artırma yapılması isteniyor.Ben 200 olarak yaptığımda her çalıştırdığımda farklı grafikler elde ediyorum.Yani aynı parametreler ile modeli eğitirken her run edildiğinde farklı sonuç çıkabiliyor mu?O zaman bir model için optimum value leri nasıl saptayacağız?Epoch 200 iken model converge oladabiliyor,olmayadabiliyor.  Teşekkürler",

> comments:

1. -> Merhaba,Makina Öğrenimi algoritmaları kararsız yapılar olması sebebiyle her yeni başlangıçta farklı sonuçlara ulaşmaktadır. Bunun sebebi de rassallıktan(randomness) dolayı olmaktadır. Yani algoritmamız her ne kadar aynı data ile çalışıyor olsa bile datanın sıralamasındaki değişim sonuçları etkilemektedir. Eğer her seferinde aynı sonuçları almak istiyorsanız random seed değeri verebilirsiniz. Bu değer her seferinde aynı değeri üretmek için sabitlenmektedir. Eğer bu değere sabit bir sayı tanımlamamışsak varsayılan olarak sistem zamanını kullanır. Bu yüzden her seferinde farklı sonuçlar elde etmemize sebep olmaktadır.import numpy as npnp.random.seed(42)tf.random.set_seed(42)Yukarıdaki satırları kodunuza eklerseniz her seferinde tutarlı bir şekilde aynı sonuçları alabilirsiniz.42 olarak belirttiğim değer keyfi olarak seçilebilir, sabit kaldığı sürece sonuçlar değişmeyecektir.İyi çalışmalar.",
2. ->  ->  Çok teşekkürler:).",
3. -> Merhaba,model.fit metodunu incelediğimde model.fit metodunun shuffle isminde bir parametre aldığını ve bu parametre değerinin default olarak TRUE olduğunu gördüm. Bu parametrenin açıklaması ise :\"Shuffle the training data on each epoch\" yani her epochta eğitim datasını karıştırır bu yüzden veriler karıştığı için her algoritma çalışmasında aynı sonucu elde edemeyiz. Araştırdığım kadarıyla Keras'ın model.compile() metodu otomatik olarak bu shuffle değerini true yapmaktadır. [Link](https://github.com/keras-team/keras/blob/7a39b6c62d43c25472b2c2476bd2a8983ae4f682/keras/engine/training.py#L1584) linkinden inceleyebilirsiniz.). Bu shuffle'ı False yapabilmek için önerilenler arasında Keras'tan önce numpy kütüphanesini import etmek ve ederken de seed metodunu çağırmak var. Yani;\"import numpy as npnp.random.seed(1337)from keras.models import Sequential\"tarzında bir yaklaşımda bulunmak. Buradaki seed metodu her çağrıldığında aynı random sayıları döner. Yine araştırdığıma göre Keras'ta bu shuffle randomness'ı kullanabilmenin yolu numpy kütüphanesindeki random.seed metodunu kullanmak. Eksiğim var ise eklemelere açığım 🙂 iyi çalışmalar.",
4. ->  ->  Merhaba,Her ne kadar shuffle parametresini False yapsak ve np.random.seed(1337), bir değere sabitlesek bile, tensorflow için random seed'i set etmez isek yeterli olmamakta. (tf.random.set_seed(1337)) ile tutarlılık sağlanmakta.Edit: Hatta sadece tf.random.set_seed(1337), model.fit() fonksiyonunun shuffle parametresinin True ya da False olmasına bakılmasızın tutarlılığı sağlamakta, şimdi test ederek sonuçları inceledim.",
5. ->  ->  Merhaba,Cevabınız için teşekkür ederim. Evet, bu shuffle değerini model.compile metodunda kendisi eziyorYukarıda verdiğim bilgi kaynaklarını şöyle paylaşabilirim.[Each time I run the Keras, I get different result](https://github.com/keras-team/keras/issues/2479#issuecomment-213892402) - [Link](https://github.com/keras-team/keras/issues/2743#issuecomment-219777627). Burada numpy seed'inin randomness'ın kalkması için yeterli olduğundan bahsediliyor. Eğer bu bilgiler hatalı veya eksikse ekleme ve bilgilendirme için teşekkür ederim, iyi çalışmalar dilerim.",
6. ->  ->  tensorflow'un da numpy random seed'i eziyor olması ya da hiç görmüyor olması muhtemel, bahsettiğiniz kaynağı ben de inceledim ancak test ettiğim zaman tutarlılığı sağlamadığını farkettim. Sonrasında[Keras LSTM - why different results with \"same\" model & same weights](https://stackoverflow.com/questions/46119435/keras-lstm-why-different-results-with-same-model-same-weights)burada tensorflow için de seeed değeri atandığını gördüm. Denediğim zaman sorunsuz tutarlılık sağlandı.Kaynak için teşekkürler. iyi çalışmalar 🙂",
7. ->  ->  Kaynağınızı inceledim, bilgilendirme ve ekleme için tekrar teşekkürler iyi çalışmalar 🙂.",
    
### soru 

> quest: "Merhaba, işaretlediğim kısmı anlamakta sorun yaşıyorum. \"smooth out noisy gradients\"  derken ne demeye çalışıyor ve Noisy Gradient kavramı nedir?",

> comments:
  
1. ->  Merhaba,Noisy data verisetimizdeki anlamsız verilere denir. Bu anlamsız veriler tahminleri olumsuz etkiler. Örneğin modelimizi eğitirken oda sayısı 100 olan bir ev veri örneği soktuğumuzda çıkacak olan lineer grafiğimiz (lineer olduğunu varsayıyorum) olması gerekenden daha az performanslı olacaktır çünkü diğer oda sayısı değerleri muhtemelen 1-10 arasında olacaktır. Anladığım kadarıyla burada demek istediği redundancy i batch size arttığında artan bir şey olarak tanımlamış ve batch size'ınız arttıkça bu artış noisy (gürültülü) içerikleri yumuşatmak için kullanışlı olabilir demek istemiş. Noisy Gradient dediği anladığım kadarıyla Noisy Data için kullanılmış. Yanlışım var ise düzeltilmesinden memnun olurum 🙂 İyi çalışmalar dilerim.",
2. ->  ->  teşekkür ederim, anladım şimdi..",
3. ->  Noisy gradient, resmin sağındaki gibi loss fonksiyonumuzun dengesiz şekilde değerler almasına deniyor. Eğer minibatch-size'ı çok düşük seçersek veya stochastic gradient descent kullanıyorsak(zaten SGD'de batch-size=1 oluyordu) loss'u hesaplarken çok az örnek kullandığımız için; loss değeri bir iterasyonda birden yükselip diğer iterasyonda birden azalabilir. Çünkü loss değerimizi çok az miktarda örneği değerlendirip güncelliyoruz.",
4. ->  ->  teşekkür ederim..",
    
### soru 

> quest: "Merhaba,  First Step with TF kısmındaki programming exercises bölümündeki örnekleri açmak üzre Colab uygulamasına girmek istedigimde aşağıdaki \"failed to fetch \" mesajıyla karşılaştım.Yardımcı olabilcek olan varmı örnek uygulamalara erişebilmem için ? Teşekkürler",

> comments:
  
1. ->  Merhaba sizin internet erişiminiz yada Colab uygulamasındaki serverlardaki bir sorundan olabilir. Bence bir süre sonra tekrar deneyin açılacaktır..",
2. -> ->  Merhaba ,chromeda yine acılmadı ama firefox kurdum onda sorunsuz acıldı cok teşekkürler.",
    
### soru 

> quest: "Az önce kursun haftalık kısmını bitirdim ama aklıma takılan bir nokta var. Biz weightleri gradient descent ile ayarlarken şu şekil bir denklem kullanıyoruz: w1 = w1 - (learning_rate)*(costun_w1e_göre_türevi) Bu denkleme göre n tane featurimiz olsa hepsinde aynı learning rati mi kullanıcağız. Learning rateye ayar çekerken bunu tek bir featureye göre yapıyorduk bu feature uyan learning rate bütün featurelara uyar mı deriz yoksa learning rate için bütün featurelara karşılık gelen learning ratelerin olduğu bir vektör mü oluşturmamız gerekiyor? Umarım iyi anlatabilmişimdir.",

> comments:
  
1. -> Merhaba. Learning rate gradient decent algoritmasının bir hiperparametresidir(hiperparametreleri modeli kuran kişinin seçebileceği parametreler olarak düşünebilirsiniz. örn: K-MEANS algoritmasındaki k değeri, garadient decent de learning rate vb. Model parametreleri ise tahmin fonsiyonumuzdaki feature'ların katsayıları bigi aslında bulmaya çalıştığımız şeylerdir). Gradient decent ise direk türev alarak optimizasyon yönteminin alternatifidir. feature ve sample sayısının çok fazla olduğu veri setlerinde direk türev alma yöntemi çok uzun sürdüğü için gradient decent kullanılır. Her iterasyonda optimum noktaya yaklaşabilmek için de learning rate belirleyerek ilerleriz. Asıl sorunuzu en sonda cevaplayayım. Evet her ağırlık için tek bri tane learning rate belirleriz. Ağırlıkların farklı olmalarının bir önemi yoktur. Çünkü her iterasyonda formülün doğası gereği hata fonksiyonun mininmum noktasına yaklaşmaya devam ederiz. Tabi ki learning rate'i optimum noktayı atlayacak kadar büyük seçmediysek.,
    
### soru 
> quest: "Merhaba, Validation Set kısmındaki pratikte \"test setini ve doğrulama setini nasıl böldüğünüz önemli değil \" diyor fakat Validation set ile test boyutu aynı olması gerekmez mi ? Örneğin valid set size % 20 ise test set' de %20 olması gerekmez mi? Sonuçta modele en çok girdi nereden veriliyorsa o tarafta öğrenme artması söz konusu olur.",

> comments:
  
1. ->  Merhaba,Validation Set'de aslında bir test settir. Dolayısıyla Test Set'in karşılaması beklenen iki şartı;- İstatistiksel olarak anlamlı sonuçlar ifade edecek kadar büyük mü?- Bütün seti(trainin data) temsil edebiliyor mu?karşılamalı. Bütün bunları sağladığı sürece test ve validation setlerini nasıl böldüğümüzün önemi olmayacaktır..",
    
### soru 

> quest: "Merhabalar herkese, Bugün genel tekrar yaparken kafama birkaç soru takıldı onları sormak istedim.  1-) Uygulama kısmında \"epoch\" ve \"batch size\" parametrelerini arasındaki ilişkiyi tam kavrayamadım.  2-) Gene uygulama kısmınını son kısmında \"correlation matrix\" den bahsedilmiş, bir takım değerler üzerinden (1.0,0.0 ve -1.0 gibi) yorum yapılıyor. O değerlerden nasıl bir yorum çıkarabiliriz ? 3-) Son olarak da, \"Generalization\" kısmında diagramda \"Hidden truth\" diye bir kavram var sanırım bizim referansımız ama onun kaynağı ne onu tam kavrayamadım. Yardımcı olursanız sevinirim.",

> comments:
  
1. ->  1- bir epoch veri setimizdeki tüm verileri işlememiz anlamına geliyor, batch-size ise gradient descent grafiğini hatırlarsanız orda adım adım minimuma doğru gitmeye çalışırken veri setimizdeki kaç örneği değerlendirerek loss hesaplayıp parametreleri güncelleyeceğimizi ifade ediyor. Mesela 1000 verimiz olsun, 100 batch-size ve 10 epoch belirleyelim. 1 epoch için 10 iterasyon gerekiyor ( veri sayısı / batch-size). Toplamda 10 epoch ise 10x10 = 100 iterasyon gerekiyor.2- correlation matrix ise feature'larımızın birbirleriyle ne kadar ilişkili olduğunu gösteriyor. 1 olması birbirlerine tamamen bağlı olduklarını -1 olması ise tamamen zıt olduklarını gösteriyor. Mesela oda sayısı ve evin metrekaresi sütunlarımız olsun. Bunların correlation değerlerinin 1'e yakın olması beklenir. Fakat evin yaşı sütunu ile evin değeri sütunlarının correlation değerlerinin -1'e yakın olması beklenir. Umarım açıklayabildim.",
2. ->  ->  teşekkürler sanırım daha iyi anladım verdiğiniz örneklerle..",
3. -> 1. soruna güzel cevap verebileceğimi düşünmüyorum.2. soru benim de aklıma takılmıştı, internette biraz araştırma yaptıktan sonra böyle bir görselle karşılaştım, ([Link](https://en.wikipedia.org/wiki/Correlation_and_dependence#/media/File:Correlation_examples2.svg) anlamama yardımcı oldu. Correlation değerinin 0 olması aralarında lineer bir bağıntı olmadığını gösteriyor sanırım. Yani düzenli olarak \"artarsa artar, azalırsa azalır\" gibi yorum yapamıyoruz anlamına geliyor. (yanlışım varsa lütfen düzeltin)3. sorun için şöyle bir şey diyebilirim, onun verdiği örnekten devam edecek olursak, makine öğrenmesi modelimizi belirli bir veri setiyle besleyeceğiz, bu kısıtlı bir veri seti olacak. Elimizdeki verilere %100 uyuyor olması \"hidden truth\"tan gelecek yeni verilerle uyuşmayabilir. Gerçek hayat verilerinde mutlaka anomaliler olur. Kullandığıımız verileri de %100 doğrulukla tahmin eden bir model de bu yüzden düzgün bir şekilde \"generalized\" tahminlerde bulunamaz. Çünkü makinemizi beslediğimiz veri setinde de anomaliler olacaktır. Asıl amacımız elimizdeki veri setini %100 doğrulukla tahmin etmek değil, \"hidden truth\"a olabildiğince yakınsamaktır. Karışık oldu kusura bakma, umarım anlatabilmişimdir 🙂,
4. ->  ->  Teşekkür ederim yanıtın için fotoğraf anlamamda yardımcı oldu..",
5. ->  ->  Rica ederim..",
6. -> epoch, batch ve batch size için oldukça açıklayıcı bir makale: [Link](https://machinelearningmastery.com/difference-between-a-batch-and-an-epoch/)
7. -> 2. Sorunuz için sunu söyleyebilirim. Korelasyon katsayısı iki değişken arasındaki ilişkinin yönü ve derecesi hakkımda bilgi verir. Regresyon analizinde model kurarken bağımsız değişkenlerinizin bağımlı değişkenimizle ilgili olan değişkenler olması önemlidir. Yapilan uygulamada oda sayısı ve nüfus bağımlı.değişkenimiz olan medyan ev değeri ile modelleri çok basarili çıkmamıştır. Eğer model anlamlıligina ve parametre tahminlerine bakilsaydi büyük ihtimalle anlamsız çıkarlardı.Dolayısıyla uygulamada daha sonra bağımlı değişkenle acaba hangi değişkenler (yani features) arasında yakın ilişki vardır sorusuna bakmak için korelasyon incelemesi yapıldı. Sonuç olarak bağımlı değişken ile yani medyan ev değeri ile medyan gelir arasında pozitif yönde 0.70 lik bir ilişki bulduk. Bu değişkeni modelimizde kullanabiliriz anlamına geliyor. Çünkü 0.70 yeterli bir korelasyon olarak değerlendirilebilir.Zaten regresyon analizinin özü de biraz koreasyonlarla ilgilidir. Bağımlı değişkenimizi etkilemeyecek değişkeni modele katmanın anlamı olmaz düşüncesindeyim. Biraz uzun oldu ama umarım faydası olur..",
    
### soru 

> quest: "Herkese merhaba. Benim kafamı takılan bir soru vardı. Learning rate, loss, cost function ve gradient arasında nasıl bir ilişki var? Bu kavramları tam olarak oturtmak istiyorum. Cevaplarınız için şimdiden teşekkür ederim ????",
> comments:

  
1. -> Merhaba,Yapay Zeka'da modelimizi eğitir ve ilerideki tahminlerimizi bu modeli kullanarak gerçekleştiririz. Elimizde bir modelimiz olsun. Bu model evin oda sayısına göre evin fiyatını tahmin etmeye çalışsın. Modelimizi eğittikten sonra modelimize oda sayısı vererek bir tahminde bulunmasını isteyelim. Oda sayısının tekabül ettiği gerçek fiyat değeri ile modelimizin tahmin ettiği değer arasındaki fark \"loss\" olmaktadır.Cost function ise modelimizdeki tüm tahminlerin hata oranlarını bize ortalama olarak döndürür ve biz modelimizdeki toplam hata oranını bu fonksiyon sayesinde bulabiliriz. Bir nevi modelimizin doğruluğudur(accuracy).Gradient Descent Algoritması ise bahsettiğimiz bu cost function'ı minimize etmek için kullanılır. Bunu da cost function değerinin derivative'ini alıp theta değerinden çıkararak yapar. [Link](http://community.globalaihub.com/community/status/1043-1043-1586253928/#comment.2355.2369.2369) bu linte gradient descent algoritmasını ve learning rate'i anlatmaya çalıştım ama Learning Rate'den de kısaca bahsetmem gerekirse, Gradient Descent fonksiyonu cost function'ımızı minize ederken minimum değere atacağı adım büyüklüğünü simgeliyor. Learning değeri çok küçük bir değer verilirse minimuma yaklaşması, küçük adımlar atacağı için çok uzun sürecek, eğer çok büyük verlirse de belki minimumu aşacağı için de yanlış çalışacaktır. Bu bahsettiğim açıklamayla ilgili  -> 'in güzel bir örneği de şurada mevcut: [Link](http://community.globalaihub.com/community/status/664-664-1586282486/#comment.2420.2475.2475) Umarım açıklayıcı olmuştur. İyi çalışmalar.",
2. ->  Ben de kısa açıklamalarını yazacağım. Cost function dediğimiz aslında modelimizin girdi ve çıktıları arasındaki ilişkiyi tahmin etme açısından ne kadar hatalı olduğudur. Loss dediğimiz kavram ele alınan bir girdiden çıkan tahmin ile bu girdinin gerçek çıktısı(etiketi) arasındaki fark. Learning rate : Öğrenme aşamasında ağırlıkları güncellerken kullanacağımız katsayı, kısaca ne kadar öğreneceğim sorusuna lr ile cevap verilebilir.",
3. ->  Harika cevaplarınız için teşekkür ederim. ???? Çok açıklayıcı oldu...",
4. -> cost functionı loss functionların toplamı olarakta düşünebilirsin.",
5. ->  Merhabalar, bahsettiğin parametreleri maliyet (cost) fonksiyonu üzerinden anlatmaya çalıştım.",
    
### soru 

> quest: "İyi akşamlar  Benim Validation Set kısmında kafama tam oturtamadığım şeyler var. Uygulama: [Link](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/validation_and_test_sets.ipynb?utm_source=mlcc&amp;utm_campaign=colab-external&amp;utm_medium=referral&amp;utm_content=validation_tf2-colab&amp;hl=tr) 1) Bu uygulama kısmında neden en başta label valuelarımızı  scale ediyoruz? 2)Anladığım kadarıyla yapmamız gereken şey test setimizi tamamen bir kenara koymak ve model tamamen hazır olmadan kullanmamalıyız. Valudation setimiz de bizim modelimizin parametrelerini daha uygun bir şekilde düzeltmemiz için erken test yapmamızı sağlıyor.Fakat  neden train seti bölmeden lossu indirgeyip testte test edip yeterli accuracy ye ulaşamayınca dönüp parametreleri değiştirmek yerine validation sete bölüyoruz?  Onu açıkcası tam anlayamadım. Not: Önceden K-Fold cross validation'ı incelemiştim oradaki kullanımı güzeldi ama onun dışında nasıl kullanılacağını açıkcası pek anlayamadım.  Şimdiden herkese teşekkür ederim.",

> comments:
  
1. ->  Bilgiğim kadarıyla scaling veya normalizing işlemleri, verileri daha iyi ve kolay eğitmemizi sağlıyor. Train seti bölmeden loss'u minimize etmeye çalışırsak overfitting ile karşılaşabiliriz ve istediğimiz loss ve accuracy'i test sette alamayabiliriz. En iyi modeli, validation setteki loss oranına bakıp oluşturuyoruz, daha sonrasında test sette test ediyoruz. Eğer hala train ve validation arasındaki loss farkı çoksa, test sete geçmeye gerek yoktur, zaten model kötü çalışıyordur. Parametreleri değiştirmek gerekir. İyi günler.",
2. ->  1. sorunun cevabı şöyle: mesela bir feature 1-10 arasında değişiyor diğeri 50-400 arasında. bunlar modelimizi train ederken etki etme yüzdeleri daha farklı oluyor. birisi daha çok etki ederken diğeri daha az ediyor mesela. bu da accuracy'i düşürüyor. bu yüzden scale ederek etkilerini eşitliyoruz..",
3. ->  Biraz geç bir yanıt oldu ama şöyle düşünebilirsin; bir modeli eğitiyorsun test ediyorsun ve çıkan test sonucuna göre hyperparametreleri tune ediyorsun. Bunu loss değerin iyice düşene kadar tekrar tekrar yapıyorsun. Bu durum overfitting tehlikesini barındırır. Çünkü modelini test sete göre ayarlıyorsun aslında. Kuşları tanıyan bir modelin olduğunu düşün ve dikkat etmeyip test setine çoğunlukla papağan resimleri koyduysan bu sefer modelini sürekli papağanları tanımak için tune ettiğinden modeli load edip kullanmayı denediğinde farklı kuş türlerini tanımadığını görürsün. Bu tabii uç bir örnek ama yine de nasıl overfittinge yol açağını gözlemlemek bu yoldan mümkün. Yanlışım olabilir..",
    
### soru 

> quest: "Merhaba,  Gradient descent algoritmasında dataseti batchlere ayırdıktan sonra modele soktuğumuzda her batch'in işlenmesinden sonra weight değeri güncelleniyor mu?  Yoksa tek seferde sokmuşusuz gibi en son güncelleniyor, sadece dataseti parçalar halinde işlemiş mi oluyoruz? Eğer böyleyse bathe bölmemiz nasıl bir fark yaratabiliyor tek seferde değerlendirmekten?",
> comments:

1. ->  Merhaba, Evet, güncelleniyor. Mini-batch'lere ayırmamızın amacı elimizde büyük bir veri seti olduğu düşünülürse(1 milyon örnekten oluşan) mini-batch'lere ayırmak yerine, weight ve bias değerlerimizi bir kere güncellemek için tüm veriyi işlemeye çalışırsak loss değerimizi minimize etmek çok çok fazla zaman alabilir. Bu yüzden daha küçük parçalara ayırıp o şekilde parametreleri güncellemek daha hızlı ve efektif oluyor.",
2. ->  \"if the batch size is 6, then the system recalculates the model's loss value and adjusts the model's weights and bias after processing every 6 examples.\" (bu Cümle 12 verilik bir veri seti için Söylenmiş yani tamamını almıyor , 6 - 6 işliyor) First Steps With TF pratik kısmında yazan cümleye gore, batch_size a bağlı olarak ağırlıklar güncellenmekte. Bu egzersizde her şey çok daha net hale geliyor. İyi çalışmalar.,
3. -> ->  Teşekkür ederim ????  ->  Gözden kaçırmışım teşekkür ederim 🙂",

### soru 

> quest: "İyi Akşamlar,  Task 4: Find the ideal combination of epochs and learning rate kısmında: learning_rate= 10  # Replace ? with a floating-point number epochs= 10   # Replace ? with an integer  Bu değerler ile label*feature grafiğinde sorunun altındaki solution kısmı ile benzer grafiği elde ediyorum. Ancak loss grafiğinde bir anlık tepe değeri yapıp ardından sıfıra ulaşıyor. Bu durum yanıttan farklı. Böyle bir durumun gerçek hayattaki projelerde etkisi kazanç veya zarar açısında durumu ne olur ? Bir de okurken gözden de kaçırmış olabilirim kayıp grafiğinin bu şekilde değil de bir parabol tarzında olup alçalan şekilde olmasının gerekliliği neden ?   Teşekkür ederim",

> comments:
  
1. ->  Muhtemelen learning rate değeri 10 olduğundan parametreleri (ağırlık ve bias) güncellerken adım sayısı büyük olduğu için yerel minimum noktasından daha uzağa gitmiştir. O sıçrama onu gösterir. Genellikle learning rate oranı daha küçük seçilir. Örneğin 0.001 veya 0.01 gibi.",
2. -> Gerçek hayatta bir hedefin olduğunu düşün (örneğin bir bakkal) .Sen ve bakkalın arasındaki mesafe 200 metre olsun learning rate aynı zamanda bir anlamda senin adım boyutun oluyor sen bakkala giderken learning rate'in çok yüksekse sen 200 metre sonra durman gerekirken bakkalı geçip gidiyorsun 300 metre ilerliyorsun mesela. Bu sefer bakkala ulaşabilmek için geriye gitmen gerekiyor geri dönerken 100m geri gitmen gerekir ki(yani bu sefer başlangıçtaki yönünün tersine bakkalı geçtiğin için geri dönüp bakkala doğru) bakkala ulaşabil ama sen yine adım boyun cok yüksek olduğu için 150m gidip yine bakkalı geçiyorsun. Aslında sen 450m yol aldığın halde 200mlik hedefe bir türlü ulaşamadın çünkü learning rate'in (adım boyun) çok yüksek bunun için böyle büyük adım atarak hedefi tutturmaya çalışmak yerine daha küçük adımlarla daha uygun bir şekilde yaklaşmaya çalışıyoruz. Ayrıca burada Learning Rate' i değiştirip gözlemlersen daha kalıcı olabilir güzel anlatamamış olabilrim. 🙂 [Reducing Loss: Optimizing Learning Rate](https://developers.google.com/machine-learning/crash-course/fitter/graph)
3. -> \"Kayıp grafiğinin bu şekilde değil de bir parabol tarzında olup alçalan şekilde olmasının gerekliliği neden ? \" kaybı en aza indirirken belli bir değer aralıklarında weight - loss kontrolleri yapmamız isteniyor (learning rate) , aşırı sıçramalardan kaçınmak (yoldan çıkmamak) için learning rate i çok yüksek seçmiyor, aynı zamanda modelimizin ilerleyebilmesi için de aşırı düşük değerler seçmekten kaçınıyoruz. İşte bu yaklaşımı görselleştirdiğimizde parabolik azalmayı gözümüzde canlandırabiliriz. Bu arada Modelimiz her iterasyonda ağırlık değerlerini kontrol ederken optimum değerleri kullanmaya çalışıyoruz. Bu değerler \"data dependent\" olduğundan ve kesin bir \"learning rate - epoch - batch_size\" belirlenemeyeceğinden bahsediyor kursta ama \"summary\" kısmındaki genel önerilerin güzel açıklandığını ve ilerleyen içeriklerde de çok işimize yarayacağını düşünüyorum. Eğer yanlışım varsa lütfen düzeltin. İyi Çalışmalar..",
4. ->  \"Kayıp grafiğinin bu şekilde değil de bir parabol tarzında olup alçalan şekilde olmasının gerekliliği neden ? \" bu soru benimde aklıma takılmıştı. ->  a ek olarak şöyle bir çözüm buldum ama ne kadar doğru bilmiyorum. Loss eğrisini çizerkenki değişkenimiz w. Burada w ikinci dereceden bir değişken ve kat sayısı pozitif. Bundan dolayıda çizdiğimiz grafiğin şekli yukarı yönlü bir parabol olacaktır diye düşünüyorum..",
"Mesut Yılmaz ->  selam. Loss function da sabit olan, y' değil de y olması gerekiyor. Yani y dediğimiz, desired (wx+b sonucunda çıkmasını istediğimiz) değer yani diğer bir deyişle label'ımız ve y' ise wx+b sonucunda tahmin ettiğimiz değerdir. Özetle loss bulurken yaptığımız şey, olması gereken değere o anki w ve b ile ne kadar uzağız bunu bulmak. Çıkan farka göre w ve b'yi güncelleyip bu şekilde devam edeceğiz. Yanlışım varsa düzeltin lütfen..",
5. ->  Evet orada bir hata olmuş, teşekkür ederim. Ancak w^2 nin katsayısı yinede pozitif oluyor..",
6. -> Cevaplarınız ve ilginiz için teşekkür ederim. Bugün bir daha inceleyeceğim.",
    
### soru 

> quest: "Merhabalar, merak ettiğim şey eğitimin içindeki kodlama pratiği ile alakalı... Acaba sınavda bu modellerin tamamen kurulup, kodlanması mı istenecek yoksa daha çok yorumlama üzerine mi olacak?   Birde, bu resimde seçtiğim alanı anlayamadım, bias bir hata değeri değil mi? neden eğitiliyor?, Sanırım o kısımda bir eksiklik seziyorum kendimde... Şimdiden teşekkür ederim cevaplarınız için...",

> comments:
  
1. ->  Bildiğim kadarıyla birden fazla bias kavramı var. [Link](https://developers.google.com/machine-learning/glossary#bias) bu linkteki bias kavramlarına bakabilirsin. Belki yardmcı olur.",
"Machine Learning Glossary  |  Google Developersdevelopers.google.comCompilation of key machine-learning and TensorFlow terms, with beginner-friendly definitions..",
2. ->  Merhaba,Buradaki bias değeri sanırsam hipotez denklemimizdeki(basit lineer regresyon denklemi -> h(x)=theta0+theta1.x) theta0 değeri yani grafiğimizdeki çizilen doğrunun orgiirn noktasına olan uzaklığı. Örneğin bias değerim yani theta0 değerim 1 olursa grafikteki doğru y eksenini 1 noktasında keser. (tam çizmeyi beceremesem de resimdeki gibi). Bias'ın amacı bildiğim kadarı ile daha iyi genellememize ve modelimizi tek bir veri noktasına daha az duyarlı hale getirmemize yardımcı olmasıdır.",
3. ->  bias terimi olmadan model orjinden geçmeye zorlanacağı için veriye uyumu düşecektir. Bias terimi eklemek ve eğitmek daha becerikli modeller üretmemize olanak sağlıyor.",
4. -> ->  Yani bu bias değerleri de eğitiliyor. Benim anlamadığım asıl nokta buydu aslında....",
5. ->  Bias değeri y eksenini kestiği noktadır. ML de basit bir örnek verecek olursak Linear Regression modelinin formülü y=b+wx dir. b=bias w=katsayı x=bağımsız değişken y= bağımlı değişkendir. Maaş ve tecrübe adında iki sütunumuz olsun. Eğitilsin. 5 yıllık tecrübeye sahip birisi kaç lira maaş alır diye sorumuz olsun. bias değerimiz 50, w değeri 1023 olsun. y=50+1023*5 y=5165 değerini buluruz..",
6. ->  Bu konu aslında \"Descending into ML\" kısmının video dersinde(3/5) grafikle anlatılmış bi daha izlemeni tavsiye ederim . Ayrıca [Link](https://www.linkedin.com/pulse/derin-%C3%B6%C4%9Frenme-uygulamalar%C4%B1nda-temel-kavramlar-skor-ve-%C3%A7arkac%C4%B1/) bu yazıyı da inceleyebilirsin 🙂",
"Derin öğrenme uygulamalarında temel kavramlar : perceptron, skor fonksiyonu ve hata hesaplaması(loss function)www.linkedin.comPerceptron ve Lineer Fonksiyonlar  Lineer fonksiyonlar, y = W.x+ b şeklinde tanımlanan fonksiyonlardır..",
7. -> Emekleriniz ve cevaplarınız için teşekkür ederim....",
    
### soru 

> quest: "Merhaba iyi akşamlar çok kısa ve basit bir soru soracağım.Kursun başında lineer regresyondaki kesme parametresi olarak verilen bias parametresi bizim ileride bias variance trade off yapıcağımız bias ile aynı kavram mı ? Aynı ise ilişkisini anlayamadım teşekkür ederim.",

> comments:
  
1. -> Bir üstteki soruda şu şekilde cevap verilmiş bu soruya sanırım gözünüzden kaçtı:\"Buradaki bias değeri sanırsam hipotez denklemimizdeki(basit lineer regresyon denklemi -> h(x)=theta0+theta1.x) theta0 değeri yani grafiğimizdeki çizilen doğrunun orgiirn noktasına olan uzaklığı. Örneğin bias değerim yani theta0 değerim 1 olursa grafikteki doğru y eksenini 1 noktasında keser. (tam çizmeyi beceremesem de resimdeki gibi). Bias'ın amacı bildiğim kadarı ile daha iyi genellememize ve modelimizi tek bir veri noktasına daha az duyarlı hale getirmemize yardımcı olmasıdır.\".",
2. -> Merhaba Berk, İkisi farklı şeyler anladığım kadarıyla. İlki sadece parametre. Yani y=ax denklemini düşündüğümüzde bu denklem orijinden geçecektir. Lakin verilen data illa orijinden geçmesi, her dataya uyum sağlamayan doğrular olacağından bias parametresi dediğimiz örn; y=ax+b gibi bir b (bias) ekliyoruz.Trade off ta kullanılan bias ise verinin öğrenilip öğrenilemediği ile ilgili..",
    
### soru 

> quest: "Merhabalar  Validation setin anlatıldığı exerciseda validation set, training setin içinden ayrılarak oluşturulmuş. Benim önceden çalıştığım Andrew NG'in yapmış olduğu başka bir courseda Andrew hoca üzerine basa basa dev test ile test setin aynı distributiondan gelmesi gerektiğini belirtmişti her zaman. Benim bu noktada biraz kafam karıştı aydınlatabilir misiniz?",

> comments:
  
1. ->  Merhaba, training setin içinden ayrılarak oluşturulması, test ile validation set'in farklı distributionlara sahip olması anlamına gelmiyor. Fotoğraf verilerimiz olduğunu düşünelim. Training setimiz telefon kamerasıyla çekilen fotoğraflardan oluşuyor olsun. Validation setimizi de ordan ayırıp oluşturduk diyelim. Test setimizdeki veriler de telefon kamerasıyla çekilmiş fotoğraflardan oluşuyorsa aynı distribution'a sahip olmuş oluyorlar. Bence burada andrew ng'nin sözünü inkar eden bir durum yok. (Fotoğraf örneğini ben de andrew ng'den görmüştüm :)).",
2. ->  ->  Doğru diyorsunuz ben de düşündüm ancak testset yerine train setten ayırınca acaba bilmediğim bir şey mi var diye düşündüm. Teşekkür ederim 🙂.",
3. ->  ->  Supervised Learnin için söylüyorum, test setinde label bulunmadığı için train setimizi train-test olarak ayırarak geliştirdiğimiz modelin etkinliğini test ediyoruz..",
4. ->  Train setimizi bölerek test ve validation setlerini oluştururuz. Model eğitilirlen test setini kullanarak val_acc ve val_loss gibi değerlerinie bakarak modelin etkinliğini görebiliriz. Model eğitimi bittikten sonra test setini kullanarak gerçek hayattaki örnekleri tahmin/sınıflandırma performansına bakarız..",
    
### soru 

> quest: "Merhabalar  gradient   magnitude'ın ne olduğunu tam anlamıyla anlayamadım.Bu hazırladığımız modelin ağırlığı mıdır yoksa farklı bir şey midir ve bir de gradient magnitude ile learning rate arasındaki ilişki nasıldır ?  teşekkürler,",

> comments:
  
1. ->  Gradient değişimi ifade ediyor. Magnitude ise değişimin büyüklüğünü ifade ediyor. Learning rate fazla olduğunda her bir iterasyonda grafik üzerinde minimuma (eğimin 0 olduğu nokta) ulaşmaya çalışırken daha büyük adımlar atacağımız için; gradient magnitude ile learning rate doğru orantılı olmuş oluyor. Fakat değişimin fazla olması her zaman iyi sonuçlar vermeyebilir.",
2. -> değişimin fazla olması, eğimin sıfır olduğu noktayı kaçıracağın anlamına gelebilir..",
3. ->  Teşekkür ederim.",
    
### soru 
> quest: "Merhaba. Anladığım kadarı ile overfitting modelimizin verdiğimiz training datasına çok yakın tahminler yapmaya çalışması ve bir bakıma predict yaparken kullanacağı datanın da benzer özelliklerde olacağı yönünde bir önyargı oluşturması. Bunun önüne geçmek için data setimizi bir takım distrubition kurallarına göre train ve test olarak ayırmamız gerektiği anlatılıyor. Ancak overfitting'in nasıl oluştuğunu tam olarak oturtamadım. Biraz daha ayrıntılı bir cevap alabilir miyim?",

> comments:
  
1. -> Merhabalar. Overfitting, oluşturulan modeller için önlem alınması gereken problemlerin başında yer alanlardan birisidir. Modeli eğitirken her ne kadar shuffle yöntemiyle veriler karıştırılarak eğitime sunulsa da, her bir çevrimde yapılan denemeler sonrasında bir süre sonra modelin veriye aşırı uyum sağlayacağı, yani veriyi ezberleyeceği belirtiliyor. Bu durumda model çok başarılı sonuçlar üretebiliyor. Siz de modelin %99 oranında başarılı olduğu gibi oranlar görebiliyorsunuz. Gelin görün ki model, eğitildiği veri setini ezberlediği için o oranlara ulaşıyor. Bir nevi input değerlerine (her bir x için) karşılık gelecek ağırlık (w) değerlerini ezberliyor da diyebiliriz. Ancak problem, modelin daha önce hiç görmediği veri setinde ortaya çıkıyor. Çoğu kişinin de üzerinde hemfikir olduğu nokta, overfitting gerçekleşmiş olan modellerin, diğer modellerde aynı başarımı gerçekleştirmediği. Bunun için ilk olarak, train ve test ayrımları gerçekleştirildi. Anlatımda da görüldüğü üzere daha karmaşık veri kümelerinde bu da yeterli olmadığı için, bu defa train-validation-test işlemi gerçekleştiriliyor. Şöyle açıklayayım:Elimizde 1000 veri olsun. Bunun 100-200 arasında bir veriyi ayırıyoruz ve modele hiç sokmuyoruz. Bu ayrım sırasında elinizdeki sınıfların oranını korursanız iyi olur. Geriye kalan 800 veri içerisinden de yine %80-%20 gibi train ve validation verilerini ayırarak modelimizi eğitiyoruz. Optimum sonuca ulaşınca da modeli hiç görmediği veri üzerinde deniyoruz.Tüm bunlar modelin elimizdeki veriyi ezberleyerek (bir nevi suni) yüksek oranda başarım gösterdiğini beyan etmesinin önüne geçilmesi için yapılıyor. Kendi modelinizde de train sırasında baktınız %100 başarıma ulaşmaya başladı, modeli gözden geçirmenizde fayda var.Hepimize iyi kurslar diliyorum.",
2. -> Merhaba,Overfitting'i [Link](https://medium.com/data-science-tr/overfitting-underfitting-cross-validation-b47dfda0cf4e) sitesinden alıntılayacağım şu durumla açıklayabilirim:\"3 gün sonra istatistik sınavına gireceğinizi düşünün. Geçmiş 10 yılın sorularının olduğu bir arşiv var elinizde, sınavın geçmiş senelere benzeyeceğini düşünüyorsunuz ve bütün soruları-cevapları ezberliyorsunuz.Burada eğitim seti geçmiş 10 yılın soruları, model sınavın geçmiş senelere benzeyeceğini düşünmeniz, test seti hiç görmediğimiz istatistik sınavı, başarı kriteri aldığınız not. Sınav soruları beklendiğiniz gibi gelmez de kötü not alırsanız bu olaya overfitting denir.\"Bu açıklamaya göre eğer modelimiz eğitim için kullandığımız veri setimiz üzerinde gereğinden fazla çalışıp artık ezber yapmaya başlamışsa overfit oluşur. Tabii overfit oluşturan başka durumlar da vardır. (Aşağıda açıkladım.)Overfitting olduğunda tahminler eğitim veri seti için harika sonuçlar verirler ama eğitim veri setinde olmayan durumlarla karşılaştıklarında nasıl davranması gerektiklerini bilmeyeceklerin bu pek efektif olmaz. Bizim amacımızda zaten eğitim veri setinde olmayan değerleri tahmin edebilmek.Overfit oluşturabilecek durumlar arasında aşağıdakiler yer alabilir:1.Modelimiz çok karmaşıktır ve gözlem sayısından çok parametremiz vardır.2.Verisetimizde az eğitim verisi vardır3. Eğitimi sırasında o kadar çok iterasyon yapılmıştır ki eğitim loss'umuz 0'a çok yaklaşmıştır.Ne yapılabilir?:1. Feature sayımızı azaltmak. Birbirileri ile olan korelasyonu yüksek kolonlar silinebilir veya bu deatureları kullanarak yeni featurelar oluşturulabilir (Örneğin veri setinizde bir evin genişliği ve bri evin yüksekliği featureları var ise bu iki feature değerini çarpıp alan feature'ı oluşturabilirsiniz.)2.Verisetimize daha fazla veri ekleyebiliriz.3.Regülarizasyon(Regularization) yapabiliriz.Hatam veya yanlışım var ise düzeltmelere açığım 🙂 İyi çalışmalar dilerim.",
"Makine Öğrenmesi Dersleri 8: Cross Validationmedium.comOverfitting (High Variance).",
3. ->  Merhaba Atilla. -> 'ın da söylediği gibi overfitting, modelimizin train datasındaki verilerden ögrenirken en ince ayrıntısına kadar ögrenmesi, aşırıya kaçması diyebiliriz. Ama bizim model oluştururken aradımız şey optimum deger olmalı aksi takdirde test datamızdaki birçok veri modelimize %90 uysa bile %10'luk kısmı yüzünden ki bu overfitting yüzünden oldugunu varsayıyorum yanlış kabul edilecek ve başarı oranı hep %10 larda kalıcaktır. Bu olayın tam terside olabilir(Underfitting). Iterasyon tablosunda accuracy degerine bakarak en uygun model türünü belirlemek daha kolay olacaktır..",
4. ->  Cevaplarınız için teşekkür ederim.",
5. ->  Kurs hakkında bir sorum daha olacak. Kodlar kurs boyunca hep bu şekilde (sadece düzenleme yapacağımız yerler açık ve blokların işlevleri yazıyor) mi devam ediyor yoksa sonraki derslerde kodların, kullanılan fonksiyonların nasıl çalıştığına dair açıklamaları da var mı? Bazı derslere baktım ama göremedim.",
6. ->  Verilen cevaplar pasta gibi üstüne bir de çilek ekleyelim.",
7. ->  Overfitting oluşmasınını temel sebepleri şunlardır. Veri sayısınınız ve çeşitliliği az olması, bias ve variance nin yüksek veya düşük olması, modelin karmaşıklığı yüksek olmasıdır. Overfitting problemini çözmek için genellikle kullanılan yöntemler şunlardır; Data Augmentation (Veri Arttırma) yöntemini kullanarak verilerin sayısını arttırabiliriz. Modelimizi çok karmaşık değilde basit şekilde tutabilirsek overfitting engellenmiş olur. Diğer bir yöntem ise modelin train loss un azaldığı validation loss un arttığı epochda eğitimin durdurulmasıdır. Train Test Validation setlerinin oranını değiştirebiliriz. Mesela %70 Train %15 Validation %15 Test olarak ayırabiliriz. Train boyutunu çok tutmamak lazımdır..",
8. ->  Bazı durumlarda yüksek epoch sayısının fazla olması overfitting durumu yaratabilir, ezberlemeyi engelleyebilmek için epoch sayısını düzgün ayarlamak gerekiyor ayrıca katmanlar arasındaki geçişleri azaltarak ezberlemenin önüne geçilebilir. Bunun için dropout değerinin düzgün belirlenmesi gerekir..",
9. ->  Bir ornek vererek aciklayayim.Bir ogerenciye matematikte belli bir soru seklini ogrettigimizi varsayalim.Bir kac ornek verdik anlattik, ogrencinin gayet iyi anladigini gorduk(training).Sonra ayni soru sekli uzerinde sadece sayilari degistirdik ogrenci soruyu cozebildi(validation).Son olarak sorunun seklini verdigimiz ornekler ogrenci tarafindan gercekten anlasildiysa cozebilecegi seviyede degistirdik, gorduk ki ogrenci yapamadi, hatta soruya dogru bile yaklasamadi.(Test)Yani verdigimiz ornekleri gercekten anlamamis, ezberlemis(overfitting).",
1. ->  Overfittingin temel sebebi modelinizdeki layer sayisi arttikca model derinlesir yani complex hale gelir.Kucuk bir training seti bu complex modeli train etmek icin kullanirsaniz kotu generalization yani overfitting meydana gelir.Bunun ustesinden L2 Regularization ve/veya Dropout(ayni anda kullanilmasi tavsiye edilemz) kullanarak gelinebilir..",
    
### soru 

> quest: "[Link](http://community.globalaihub.com/community/status/1028-1028-1586434538)) yazdığım gibi sadece epoch değerleri ile oynayarak  loss/rmse grafiğininin eğimini sıfıra yaklaştırabiliyoruz   Ancak ilerleyen örneklerde Task 4: Find the ideal combination of epochs and learning rate [Link](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb#scrollTo=r63YkMx82WVr) ve Task 5: Adjust the batch Size [Link](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb#scrollTo=0NDET9e6AAbA) Örneklerinde olduğu gibi   Learning rate  Epochs batch_size   parametrelerini değiştirerek farklı grafikler elde edebiliyoruz….  Yapılan örnekler simple linear regression olmasından dolayı, feature/label grafiklerinde kırmızı çizginin mavi çizgilerin üzerine oturması ya da hatayı minimize edecek şekilde oturmasına dikkat ettiğimiz durumlarda bile;   1. yalnızca epochs değerini ayarlayarak çalıştırdığımız örneklerde; epoch değeri 300 lerde eğim sıfırlanıyor iken, learning rate, epochs ve batch_size parametrelerini değiştirerek yaptığımız örneklerde epoch değeri 13- 17 aralığında iken loss / rmse grafiğinin eğiminin sıfırlandığını görüyoruz…    Genel olarak amacımız learning rate, epochs ve batch_size parametrelerini ayarlayarak loss/rmse grafiğinin eğiminin min epoch değerinde sıfırlanmasını mı yakalamaktır..

> comments:
  
1. ->  Merhabalar,Yazmış olduğunuz, yazacağınız her satır kodun size bir zaman maliyeti olur. Bu aşamada asıl amacımız loss/rmse grafiğini minimize etmek evet ancak bunu yaparken en az adımda olmasını istiyoruz, çünkü adım ne kadar azalırsa sonuca o kadar hızlı ulaşırız. Bu durumda asıl amacımızın gerçek minimuma ulaşmak değilde, en hızlı şekilde optimal bir minumum değer elde etmek.Bunu basitçe test edebilirsiniz: epoch için 1000 seçtiğinizde eğitim için geçen zaman ile 250 seçmeniz halinde geçen zamanı karşılaştırın. Tabi veri setinin çok küçük olması(12 sentetik gözlemden oluşmakta) muazzam bir zaman farkı oluşturmayacaktır. Ama birde bunu yüzbinlerce gözlemden oluşan bir sette yaptığınızı düşünün. İlk yapacağınız şey, sonuca daha hızlı ulaşmanın yolunu aramak olacaktır.İyi çalışmalar.",
2. ->  ->  az sonra graikler ile göstereceğim anlatacağımı",
3. -> ->  ayrıca bende sormak isterim bu soru devamında ,hızlı ulaşmaya çalışılmasında şöyle bir şey takıldı aklıma,aslında demek istediğim 1000 veri den 500 batchsize ve 2 iteration yapsam 1 epoch denk geliyor. Yani bir kere ileri ve geri aktarılıyor değil mi? 250 batchsize 4 iteration yapsam yine 1 epoch olur. 2 epoch için bunlar 500 batchsize 8 iteration olur dimi? Buda bir 8 iteration 8 batch demek galiba. Epoch’u artırınca zaman daha çok yer kaplıyor gibi bununla birlikte iteration da fazlalaşıyor. Peki iterationlar mini-batch ihtiyaç duyar mı peki learning_rate dahilinde, çünkü bunlarda zamanı gecikterecek olaylar dimi?",
4. -> -> 250 batch size ile 2 iterasyonda batch size da değişiklik olmaz 250 olarak kalır, gerisinde sorun yok, ancak epoch ile sınırlayarak konuyu daha basit bir şekilde açıklamaya çalışmıştım. Batch size küçüldükçe iterasyon artmakta, ayrıca learning_rate'ye göre de epoch süresi değişmekte. Özetle dediğiniz gibi batch_size ve learning_rate de gecikmeye sebep olmakta.Bu yüzden biz sadece epoch için optimal değeri aramayacak, learning_rate, batch_size ve epoch üçlüsü için optimal değerleri arayacağız.Basit bir örnek vermek gerekirse: Direkt olarak örnek çalışmadan bakacak olursak: learning_rate=0.01 iken 350 epoch da optimum değere ulaşırken, learning_rate = 0.14 olarak değiştirirsek 70 epoch gibi bir değerde minimum loss'a ulaşmaktayız. Buraya batch size ı da ekleyip farklı sonuçlara ulaşmak mümkün. Deneysel olarak hangi parametre ne kadar zaman kaybına sebep olmakta ayrı ayrı deneyerek gözlemleyebilirsiniz. Tabi küçük bir veri setinde olmasını tavsiye ederim :)İyi çalışmalar..",
5. ->  Teşekkürler -> , cevap son derece yeterli ancak sentetik veri seti olduğunu anlamak ile birlikte,Aynı veri seti üzerinden düşünürsek ideal olan loss/rmse grafiği için soruyorum... min epoch değerinde eğimin sıfırlanması daha iyidir gibi bir sonuç mu çıkarmamız lazım.....",
6. ->  ->  Tam olarak öyle demeyelim min epoch dediğimiz zaman epoch'un minimum olabileceği değer 1 olduğu için yanlış anlaşılabilir. Eğimin sıfırlanmasına olanak sağlayan optimal epoch değeri iyidir gibi bir sonuç çıkarabiliriz.",
    
### soru 

> quest: Merhabalar;   Task 2: Increase the number of epochs [Link](https://colab.research.google.com/github/google/eng-edu/blob/master/ml/cc/exercises/linear_regression_with_synthetic_data.ipynb#scrollTo=lLXPvqCRvgI4) uygulamasını yaparken   10 epoch değerinin çalışmadığı aşikar,   (ki burada epoch vs RMSE grafiğine baktığımızda bekletimiz grafiğin eğiminin sıfıra yakınlaşması ya da sıfır olması )  Ancak sorun şu;  epoch= 450 değerini aldığımız zaman epoch değeri 300 ( +20) lerde iken eğim sıfırlanıyor ve 450 epoch değerine kadar sıfır olarak kalıyor..  epoch=600 yaptığımızda da epoch değeri 350 - 375 arasında iken eğim sıfırlanıyor ve 600 epoch a kadar sıfır eğimle devam ediyor. Epoch değerinin makbul olanı 450 midir? 600 müdür?   NOT: (Belki de soru şöyle olmalı ) Deneysel çalışmalar yapmam gerektiğini ve bu değerlerin ileride çok daha net olarak belirleyebileceğimizi biliyorum. Ancak şu an için buna takılmak doğrumudur? Değil midir?",

> comments:
  
    1. ->  Merhaba, bence bu değerlere takılmana gerek yok. Veri setinden veri setine bu parametreler farklılık gösterebilir. Büyük ihtimalle hiçbir zaman bu kadar küçük veri setleri ile çalışmayacaksın. Temelleri anlayıp, deneysel sonuçlar üzerinde çok fazla zaman harcamamak gerektiğini düşünüyorum.",
    
    
### soru 

> quest: "Herkese iyi günler. Biraz konu dışında olacak ama kursta ilerlerken laptopa Tensorflow kurmaya karar verdim ancak ne kadar denediysem de hep hatayla karşılaştım. Anaconda,Pycharm vs. denedim ancak her defasında hata aldım. Acaba daha önce bu tarz hatayla karşılaşıp yardımcı olabilen olur mu?  Hata mesajı: ImportError: DLL load failed: Belirtilen modül bulunamadı.",

> comments:
  
1. ->  Ben spyder kullanıyorum , bu tarz bi modül yükleme hatasıyla karşılaşmıştım . Spyderı son sürümüne güncelleyince problem çözülmüştü...",
2. ->  Merhaba,Anaconda yüklü ise makinanda, aynı zamanda Anaconda Prompt gelmekte, Anaconda Prompt'u açtığın zaman(base) C:\\Users\\user_name> ile başlayan bir komut ekranı açılacak,Bu ekranda iken console'ye \"pip install tensorflow\" yazarsan sorunsuz bir şekilde tensorflow kurulacaktır..",
3. ->  ->  ve ->  Pycharm üzerinden kurmaya çalıştığımda kurulurken Tensorflow kuruldu demesine rağmen, programın derlenmesi sırasında hata verdi. daha sonra pip ile kurduğumda sorunsuz bir şekilde çalıştı bende de.",
4. ->  ->  Merhaba, dediğiniz gibi console'da \"pip install tensorflow\" ile install yaptım. Sonrasında yeniden Spider'da tensorflow'u import edemedim. Aşağıdaki hatayı aldım. Ne yapmam gerekir? Çok teşekkürler..",
5. ->  ->  Merhaba,1 - simply download MSVCP140.dll, unzip it and then paste it in system32 folder..2 - pip install tensorflow --upgrade --force-reinstall (Bu işlemi anaconda prompt üzerinde yapmanız gerekmekte)Bu iki adımı dener misiniz, önerilen çözümler arasında bulunmaktalar. Daha önce karşılaşmadığım için kesin bir yol söyleyemiyorum..",
6. ->  ->  Teşekkürler ama başarılı olamadım. Başka çözümler bakıyorum netten. Çok teşekkürler..",
7. ->  Pycharm için nasıl bir indirme yöntemi kullandın? Pycharm sürümün kaç? Python sürümün kaç?.",
8. ->  Bende anaconda üzerinden kurdum daha sonra python IDLE için cmd kısmında pip install numpy ve tensorflow yazınca sorunsuz çalıştı yani şuan hem spyder,jupyter hemde normal python IDLE üzerinde kullanabiliyorum, tensorflow web sitesindeki talimatları dikkatlice uygularsanız sorunsuz çalışacaktır..",
9. ->  Herkese teşekkür ederim değerli yorumları için. Günün sonunda sorunu çözmüş olmanın keyfini yaşıyorum. Anacondayı silip tekrar kurdum ve farkettim ki sorun \"Microsoft Visual C++\" ın güncel versiyonunun yüklü olmamasından kaynaklıymış.",
10. ->  Bende de 2.1 versiyonunda sorun çıkmıştı. 2.0'a dönünce sorun çözüldü. Eğer sizin versiyon da 2.1 ise onu kaldırıp 2.0'ı yüklemenizi öneririm. 2.0 yüklemek için pip install tensorflow==2.0.0 komutunu kullanabilirsiniz..",
    
### soru 

> quest: "Merhaba arkadaşlar, konuyu anlamakta biraz zorluk çekiyorum ama örnek üstünden sormak istedim. Şimdi elimde çeşitli şekillerde para birimlerim olan bir data set örneği var (bu para birimleri labels oluyor) bide bu datasette paraların ağırlıkları, boyutları gibi örnekler var (yani features). Ben bunlardan bir model oluşturmak istersem (training) kullanmış olduğum şey labeled examples oluyor. Buna” firstmodel” diyelim. Böylelikle ağırlıktan hangi para birimi olduğunu soranlara (classification model) tahmin yürütebiliyorum (suprevised learning). Modeli oluşturduktan sonra, para birimi verilmeyen ama features verilen bir “başka” datasetti, “firstmodel” dediğimiz datasete uyguladığımda ise unlabeled examples için inference yapmış oluyorum buda “secondmodel” olsun. Bu “secondmodel” ise bir kullanıcının gözü kapalı şekilde TL parabirimini en hızlı seçme olasılığını nedir sorusuna tahmin yürütebiliyor olsun. (regression)(unsupervised learning). Bu anlattığım doğrumu dur? Yani bir etiketsiz veri datasetini eğitmek istersem yada geliştirmek istersem, önceden etiketli bir data setiyle birleştirmem mi lazım, çıkarım için? yoksa bu ikiside farklı datasetler olabilir ve ikiside farklı işlerde kullanılabilir mi? örneğin bir firma \"firstmodel\" kullanarak sadece para tahmini yaparken \"secondmodel\" başka bir firmada grublama için kullanılabilir mi? Burası iyice karıştı bende, yanlışım var ise lütfen düzeltir misiniz?",

> comments:
  
1. ->  Merhaba, Aslında bahsettiğin first model ve second model aynı model. First modelde veri setiyle model oluşturup eğitiyorsun. Second model dediğin şey bilinen değerlerden bilinmeyen değeri tahmin etme. Model kurarken test datasında model doğruluğunu test ederken yapılan işlem. Regresyon ile sınıflama problemleri zaten ayrı konular. Sınıflama birbiri ile kıyas yapılamayan haliyle ortalaması alınamayan değişkenler için yapılır(örneğin: boy ve kilodan cinsiyet tahmin. Kadınlar ve erkeklerin ortalamasını alıp %30 kadın %70 erkek diyemezsin. İkisinden birini seçmek zorundasın nihai tahminde). Regresyon modelleri ise sürekli değerler alabilen bir değişken tahmin edilirken kullanılır.2 months ago 7 people like this.Like ReportReply",
2. -> Merhabalar,Sizin örnekleriniz üzerinden devam ederek açıklamaya çalışayım, Firstmodel için söylediğiniz doğru, supervised learning, sonucunun ne olduğunu bildiğimiz veri ile algoritmamızı eğitip, ileriye dönük olarak elimize gelecek olan verilerin classification için hangi sınıfa, regression için ise hangi değere sahip olduğunu tahmin etmemiz aşamalarını içermektedir. Yani özetle Supervised learning'de yaptığımız SONUCU BİLİNEN(Etiketlerimiz oluyor.) geçmiş tecrübelerden öğrenerek, geleceğe yönelik tahminde bulunmak.Ancak second model için vermiş olduğunuz örnekte ki insan/ kullanıcı her ne kadar gözü kapalı olsa bile TL parabimini seçme olasılığından bahsediyorsak da Supervised Regression olur çünkü kullanıcının TL etiketine ait feature bilgisine sahip olması lazım aksi takdirde bu olasılık hesabı oldukça basit bir şekilde söylenebilir(toplam TL banknotu sayısının veri seti büyüklüğüne oranıdır.). Unsupervised Learning için ne yazık ki regression ile alakalı bir kaynak görmüş değilim bu kısım yanlış olabilir. Bildiğim kadarı ile Unsupervised learning için etiket bilgisine ihtiyaç yoktur. Amaç ortak özelliklere sahip olan kümeleri ortaya çıkarmak olduğu için, örneğinizi eğer kullanıcının gözü kapalı bir şekilde paraları kümelemesi olursa bu unsupervised learning'e örnek olacaktır. Bu durumda kullanıcının yapacağı işlem her bir banknotu eline alarak birbirleri ile kıyaslamak sureti ile kendisine aynı hissi veren banknotları aynı gruplara koymak olacaktır.2 months ago 10 people like this.Like ReportReply",
3. -> Çok teşekkür ederim x 

### soru 

> quest: "Merhabalar arkadaşlar, Bir soru sormayacağım ancak Simple Lineer Regresyon çalışırken farkettiğim bir mantık hatasını paylaşmak istedim.  Linear Regression with Synthetic Data alıştırmasını incelerken, train_model()'de batch_size parametresine fonksiyona parametre olarak gelen batch_size değeri atanmamış onun yerine None değeri atanmış. Alıştırmanın sonuna doğru batch_size ile alakalı düzenleme yapmamızı en küçük hangi değerde çalışabildiğini bulmamızı istemiş. Bu hali ile batch_size için biz 0.0001 gibi bir değer bile girsek hata almamız gerekirken hatasız bir şekilde çalışacaktır. Çünkü varsayılan olarak model bütün data ile train işlemini gerçekleştirmekte.     İyi çalışmalar.",

> comments:
  
1. ->  Bilgilendirme için teşekkürler furkan bey..",
1. ->  Önemli bir bilgilendirme. Ben de model fonksiyonlarına bakarken fark ettim.def train_model() fonksiyonunu;batch_size=None => batch_size=batch_size,şeklinde güncelleyince, batch size değikliğinin etkileri gözükmekte..",
    
### soru 

> quest: "Merhabalar, yukarıdaki \"Adjust the batch size\" örneğinde batch size'ı 1 olarak seçtiğimde loss'u 2,3724 MSE ise 1,54 olarak hesaplandı. Batch size'ı 4 olarak seçtiğimde ise daha düşük loss ve MSE'ye sahip oldum (loss = 0.8753 MSE = 0.93) . Cevap olarak en küçük batch size'ı 1 olarak seçebileceğimiz gösterilmiş.  Cevabı 1 olarak göstererek bize ne açıklanmak istenmiş? Daha düşük loss ve MSE ' ye sahip olduğumuzda daha doğru size'ı bulduğumuz anlamına geldiğini söyleyemez miyiz?",

> comments:
  
1. ->  Merhaba, 100 epoch'ta converge edecek şekilde seçebileceğiniz en küçük integer değer nedir diye soruyor. Cevap için o yüzden 1 yazmış, loss ve mse değerlerine dikkat ederek yazmamış.",
2. ->  İlk görünteki en son paragrafta,100 epochs ta bile modelin 'converge' yani optimuma yakınsamış duruma gelmesi için verebileceğimiz en küçük batch_size sorulmuş.Problem özünde 1 bile yetebilmiş bunu göstermeye çalışıyor aslında.Problem için belki de 4 ten daha optimal bir değer vardır,ancak batch_size=1 olarak kabul edildiğinde de model,optimale yaklaşabilmiş. batch_size bir hyperparametre olduğu için,kesin ve net bir değeri vardır diyemeyiz.Lütfen eksik veya yanlış ifade ettiğim bir şey varsa uyarın. 🙂.",
"Merve Horoz Çok teşekkürler ->  ->  ..",
3. ->  [Link](http://community.globalaihub.com/community/status/618-618-1586424145/#comment.2599.2784.2784)
    
### soru .

> quest: "Herkese iyi çalışmalar,attığım görüntüdeki anormal olan parametreyi açıkçası pek anlayamadım,bu değerleri göz önünde bulundururken 1000 e bölünmüş ölçekli olan değeri ile mi kıyaslıyacağız ?",

> comments:
  
1. -> Burada benim anladığım yanlışlık şu; Öncelikle oradaki değerlerin anlamlarını söyleyim.mean: ortalama,std:standart sapma,gerisi şöyle hesaplanıyor elimizde 1den 9a kadar sayılar olsun(küçükten büyüğe doğru sıralanmış şekilde veriler karışıksa önce sıralanmalı) [1-2-3-4-5-6-7-8-9] Burada min:en küçük yani (1) değeri oluyor max: en büyük yani (9) değeri oluyor. %50 medyan(ortanca):5 değeri oluyor sıralayınca ortadaki %25: 1. çeyreklik değerimiz yani (3) oluyor. %75: 7 değerimiz oluyor. Şimdi bu bilgilere göre tablodaki yorumum şöyle: total_rooms feature'ının değerlerine bakınca her çeyrek için değerleri sırasıyla 2-1462-2127-3151-37937 yani her çeyrekte genellikle 1000 civarı bir değişim olurken %75den max a geçerken yaklasık 34000 lik bir değişim olmuş. Bu da birçok nedenden dolayı olabilir yani aslında 3793 gibi bir sayı yazarken yanlışlıkla fazla yazılmıs olabilir.(Ayrıca Eğer gerçekte böyle bir değer var ve diğer bütün değerler bu değerden uzaksa istatistikte bazen böyle sapan değerler veri setinden çıkarılarak göz ardı edilebiliyor.)Mesela housing_median_age,median_income ve median_house_value kısmında herşey normal gibi fakat total_rooms taki gibi bir anormallik ayn şekilde total_bedrooms,population ve households da da görülüyor. Benim bildiklerim ile anladığım ve yorumamlamam budur. Yanlışım varsa beni de düzeltirseniz sevinirim. İyi çalışmalar dilerim.2 months ago 9 people like this.Like ReportReply",
2. -> Merhaba Emre,Cemhan'ın söylediklerine bir ekleme yapmak istedim. Bu konuda bahsedilen istatistikte outlier yani aykırı değer olarak geçiyor. Elimizdeki verileri küçükten büyüğe sıralayınca en ortadaki değer medyan(Q2 veya 50th percentile), medyan ile minimum ortasındaki değer first quartile(Q1 veya 25th percentile), son olarak medyan ile maksimum ortasındaki değer ise third quartile(Q3 veya 75th percentile) olarak geçer. Bu değerlere bakarak veri setimizin dağılımı hakkında az çok bilgi sahibi olabiliriz.Gelelim en başta bahsettiğim outlier tanımlamasına. Verisetimizdeki aykırı değerleri saptarken aşağıdaki formülü kullanıyoruz(birkaç farklı yaklaşım var ancak en basiti):IRQ = (Q3 - Q1)Upper fence = Q3 + 1.5*IRQ, lower fence = Q1 - 1.5*IRQOutlierlar bu upper fence ve lower fence yani sınırlarının dışında kalan değerler oluyor. Bunlar ise bizim verisetimizi bozabilecek değerler. Tabi bu değerlerin model eğitiminde kullanılıp kullanılmayacağı data scientistin tecrübe ve öngörüsüne kalıyor. Aşağıda datasetteki median_income değerleri ile çizilmiş bir box plot örneği paylaştım. Bu grafik ile yukarıda bahsettiğim tüm değerleri veri üzerinde görebiliyoruz, upper fence üzerinde kalan tüm değerler outlier oluyor.İyi çalışmalar..",
3. -> Teşekkürler 🙂",
4. ->  ->  'un yorumunda bahsettiği veri bilimcinin tecrübe ve öngürsüne kalma durumunu birazcık açmak istiyorum. Aşırı değerleri gördüğümüzde onları veri setinden çıkarıp çıkarmama noktasında oldukça dikkatli olmak gerekiyor. Eğer aşırı değer veri girişinden kaynaklı bir hataysa bunu veri setinden çıkarabilirsin. Örneğin ilköğretim öğrencilerinden elde edilen bir veride 45 yaş hatalı bir veri girişi diyebiliriz. Ev fiyatlarını düşündüğümüzde diyelim ki 1 milyon değerinde bir ev ile karşılaştık. Bunun öncelikle bir outlier(aykırı değer) mi yoksa extreme value (uç değer) mi olduğuna karar vermelisin. Diyelim ki evin oda sayısı 25, banyo sayısı 5, salon sayısı 3 gibi değerler bu durumda bu evin fiyatı bir uç değer( extreme value) olarak değerlendirilir. Ancak ev 2 oda bir salon ve diğer özellikleri de ortalama değerler bu durumda veri girişinde bir hata olma ihtimali daha yüksektir..",
    
### soru 

> quest: "Merhabalar,   Reducing Loss: Playground exercise'da, eklemiş olduğum resimde de görüldüğü üzere, bize verilen datalardan dördüncüsünün eğitimi için kullanabileceğim optimum değerler (batch size, learning rate... ) hakkında öneride bulunabilecek var mı?   Teşekkürler, iyi çalışmalar herkese.",

> comments:
  
1. ->  Bence en iyisi deneysel olarak çalışıp,verdiğin parametreler ve hiperparametreler arasındaki ilişkiyi gözlemlemek olacaktır.Optimum değerler kullanıp,optimum sonuca ulaştığın senaryonun çok eğlenceli ve eğitici olamayacağı düşüncesindeyim 🙂.",
" ->  ->  Kesinlikle katılıyorum, dört veri seti üzerinde de birçok değer denedim ve gözlemledim fakat sonuncusunda optimum değil optimuma yakın bir şeyler bile bulamadım. Yanıtınız için teşekkür ederim. 🙂.",
2. ->  Bu kısım neural network ile ilgili alıştırma kısmı bu alıştırmaya neural network kısmında sonra bakılır diye düşünüyorum .yinede yapmak istersen featureları değiştirerek söyle bişey elde ettim.",
3. ->  [Coursera | Online Courses From Top Universities.](https://www.coursera.org/learn/deep-learning-business/discussions/weeks/6?sort=lastActivityAtDesc&page=1&q=)attığım linkteki ücretsiz kursun discussion formunda değerleri paylaşanlar olmuştu bunun için göz atabilirsin",
4. -> Merhaba,Bu spiral dataset ile oynarken input feature olarak polar koordinatların (r,θ) işe yarayabileceğini düşündüm.playground.tensorflow.org daki input feature'lar sabit ancak kaynak koduna istediğiniz feature'ı basitçe ekleyebiliyorsunuz.playground reposunu forklayıp aşağıdaki modifikasyonu yapınca, az sayıda neuron ve layer kullanarak oldukça başarılı sonuç alınıyor.[Link](https://github.com/cankut/playground/commit/b982c86e0d89f42b68fcda8be70cdc78df56583f)
"Tensorflow — Neural Network Playgroundplayground.tensorflow.orgTinker with a real neural network right here in your browser.",
    
### soru 

> quest: "Merhaba, kursun ilk haftasında bulunan First Step with TF bölümündeki alıştırma kodlarını Python'a aktardım. Hem arkadaki kodları görmek hem de biraz elleri kirletmek için iyi olacağını düşündüğümden sizinle de paylaşmak istiyorum. Fakat, plot_the_loss_curve kısmında bir hatayla karşılaşıyorum, bunun çözüm arama yeri burası değil sanırım ama en azından diğer kısımlar çalışıyor. İyi çalışmalar dilerim.[Link](https://github.com/oguzhari/GoogleMLCrashCourse)

> comments:
  
1. ->  ML kısmından çok anlamıyorum ama sayfanızda yayınlamış olduğunuz koddaepochs = 10ile epochs değerini integer olarak tanımlıyorsunuz, rmse ise bir dizi olarak ki kodunuzu çalıştırdığımda 10 elemanlı bir dizi olarak geliyor. tek değer ile 10 elemanlı bir diziyi aynı grafiğe oturtmaya çalıştığınız için hata veriyor olabilir....",
2. ->  Çok teşekkürler.",
3. ->  Merjaba, ->  hocamın da söylediği gibi epochs değeri sizin kodunuzda scalar bir değer olarak yani 10 olarak gelmiş. Grafiği çizebilmek için rmse ve epochs değerlerinin ilk boyutlarının aynı olması gerekiyor. (len(epochs) ve len (rmse) ile kontrol edebilirsiniz.) Github kodunuzu forklayıp sorunu giderdim ve pull request oluşturdum. Gözden geçirdikten sorna uygun görürseniz pull requesti kabul edip yazdığım düzeltme kodu ile sizin yazdığınız kodları birleştirebilirsiniz.Edit: @->  hocamın belirttiği kısmı -> 'ü kodun içinde referans göstererek düzenledim..",
4. -> ->  Merhaba, katkınız için çok teşekkür ederim..",
5. ->  -> Merhabalar,Hata ile alakalı değil ancak kodunu incelediğim zaman train model de batch_size = None olarak yazılmış. Bu durumda my_batch_size değişkenini ne kadar değiştirirsek değiştirelim biz bütün batch_size değerini her zaman data boyutuna eşit almış olacağız. batch_size yi parametre olarak aldığın batch_size ye eşitlersen farklı batch'lerde nasıl sonuçlar üretildiğini gözlemleyebilirsin.Hata içinde basit bir şekilde np.arange(1,ecpocs+1) yaparsan sorun çözülecektir diye düşünüyorum.EDIT: Hatanın kaynağını şimdi buldum, örnek olarak paylaşılmış olan kodda train_model() fonksiyonunda epochs update edilmekte, Colab üzerinde ki kodda bunu görebilirsin. epochs update işlemini:# The list of epochs is stored separately from the# rest of history.epochs = history.epochşeklinde yapılmakta böylece geriye epochs için bir skaler döndürmek yerine liste döndürüyor train_model fonksiyonu..",
6. -> ->  Çok teşekkür ederim",
    
### soru 

> quest: "Herkese, iyi akşamlar. Goldilocks learning kavramını tam olarak anladığımı düşünmüyorum. Anladığım kadarıyla en az sayıda adımla minumum local değere ulaştığımız learning rate değeri goldilocks oranına eşittir diyebilir miyiz? Yoksa bu ifadem yanlış mıdır?",
> comments:

  
1. -> Burada dün tartışıldı.Bakabilirsiniz. [Link](http://community.globalaihub.com/community/status/1043-1043-1586253928/#comment.2355.2369.2369)
2. ->  ->Postları incelemiştim ama gözümden kaçırmış olmam lazım teşekkürler..",
3. ->  Önceki başlıkta learning rate kavramına ait açıklamalar var ancak kurstaki sorunun cevabı olan 1.6 yı nasıl bulduğumuzu açıklayan arkadaşların bir gönderisini göremedim. Yardımcı olacak birisi olursa sevinirim..",
    
### soru 

> quest: "Herkese merhaba! Anladığımı düşünmekle beraber emin olamadığım ufak ayrıntıları sizlere danışmak istedim .Anladığım kadarıyla \"mini-batch\" parametresiyle ağa girecek olan veri sayısını belirliyoruz ve her bir epochta bu sayıda veri işleniyor . Her  epochta seçilen veriler üzerinden ağırlıklar hesaplanıyor ve epoch sonunda backpropation ile son güncelleme yapılıyor. Aklıma takılan kısım öncelikle her epochta mini-batch boyutunda seçilen veriler farklı ve rastgele mi oluyor , özellikle farklı olmasına dikkat ediliyor mu  ? Bir diğer detay is şu ; her epochta , bir önceki ağırlıklar üzerinden güncelleme yaparak ilerleniyor değil mi ?",

> comments:
  
1. ->  Merhabalar,Her epoch ta toplamda aynı veriler (toplam train veri setiniz) kullanılacağından her mini-batch te rastgele veya sıralı seçmiş olmanız çok fark ettirmeyecektir.Epoch sonunda tüm train verniz elden geçirilmiş olacak.Sorunuzda daha önemli olan kısım şurası her epoch ta hesaplanan ağırlıklar bir sonraki epoch ta güncellenmeye devam edilecek ki daha iyi bir yakınsama olsun.Her seferinde ağırlıkları yeniden set edip her batch te hesaplarsanız her epoch sonucunda yaklaşık aynı yakınsamayı yapmış olursunuz. (Rastgelelik ten dolayı şanslıysanız en iyi ağırlıkları bulursunuz ama bu çok çok iyi şans işi)Sonuç olarak;Ağırlıklar her epoch ta ve her batch te güncellenerek ilerleniyor.SaygılarımlaMehmet.",
2. ->  Öncelikle cevabınız için teşekkürler .Yani her epochta aslında bir öncekinde işlenen veri tekrar işleniyor ... (Ben her epochta farklı veriler işleniyordur ve bu işlenen veriler de rastgele ve bir öncekinden farklı olacak şekilde seçiliyordur diye düşünmüştüm ) yani sonuç olarak mini-batch parametresiyle belirlediğimiz sayı kadar veri değerlendirmiş oluyoruz .Ama bu kısımda anlamadığım ben daha küçük bir veri seti oluşturmak varken neden mini-batch parametresi kullanmış oluyorum ?",
3. ->  \"Derin öğrenme uygulamalarında, veri setinde bulunan tüm verileri aynı anda işleyerek öğrenme, zaman ve bellek açısından maliyetli bir iştir. Çünkü öğrenmenin her iterasyonunda geriyeyayılım (“backpropagation”) işlemi ile ağ üzerinde geriye dönük olarak gradyan (“gradient descent”) hesaplaması yapılmakta ve ağırlık değerleri bu şekilde güncellenmektedir. Bu hesaplama işleminde veri sayısı ne kadar fazla ise hesaplama da o oranda fazla sürmektedir. Bu problemi çözmek için; veri seti küçük gruplara ayrılmakta ve öğrenme işlemi seçilen bu küçük gruplar üzerinde yapılmaktadır. Bu şekilde birden fazla girdinin parçalar halinde işlenmesi “mini-batch” olarak adlandırılmaktad
 ır.\ ".", 4. ->  [Derin Öğrenme Uygulamalarında En Sık kullanılan Hiper-parametreler](https://medium.com/deep-learning-turkiye/derin-ogrenme-uygulamalarinda-en-sik-kullanilan-hiper-parametreler-ece8e9125c4)
5. ->  ->  sorduğun soru benim de kafama takılmıştı.Sanırım zaman ve bellek açısından maliyetli olacağından dolayı.",
6. ->  ->  aynı yazıyı ben de inceledim ama en son yorumumda sorduğum soruya yanıt bulamadım maalesef . Acaba bazı şeyleri tam olarak anlayamadım mı yoksa sadece büyük veri setlerini uğraşmadan küçültmek için uygulanan bir yaklaşım mı emin olamadım.",
7. ->  ->  Benim de anladığım kadarıyla yapılan yaklaşım şunun gibi; Mesela seçim döneminde istatistik şirketleri seçim anketleri yapıyor ve hangi adayın ne kadar oy alacağını tahmin etmeye çalışıyorlar.Bunu yaparken kitle=Bütün halk ama bunu yapmak çok fazla zaman ve imkan gerektirdiği için bunun yerine belirli örneklem büyüklüğü ile hareket edip genelleme yapıyorlar. Bu sonuç kesinlikle kitlenin sonucu değil kitlenin bir alt kümesi olarak alınan örneklemin sonucu oluyor. Fakat kitleyi temsil özelliği taşıyor. Eğer veri sayımız çok değilse kitle ile işlem yapmak her zaman daha iyidir en doğru sonucu verir. Fakat veri sayımız çok fazla (milyonlarca veya milyarlarca veri varsa) bunla uğraşmak yerine onu temsil edebilecek bir alt küme alarak genelleme yapmaya çalışıyoruz..",
8. ->  ->  1 epoch tüm verinin işlenmesi anlamına geliyor. Dolayısıyla veri setimizdeki her bir veri her epoch'ta tekrar işlenmiş oluyor. mini-batch-size = 100 ise ve 1000 adet verimiz varsa, 1 epoch için 1000 / 100 = 10 iterasyona ihtiyacımız var.",
9. ->  ->  cevabınız için teşekkürler şu an her şey netleşmiş oldu benim için..",
10. ->  Merhabalar,->  Her epochta bütün setin üzerinden geçiyoruz ancak amacımız Loss'u minimize etmek olduğundan sıralı seçim yapacak olursak istediğimiz sonuca ulaşmamız oldukça zorlaşacaktır. Elimizde bulunan seti en iyi şekilde örnekleyen batch veya batch'ler ile minimum loss değerine ulaşabiliriz. Yani her bir epoch da batch size kadar veri rastgele seçilmektedir ki Stochastic bu rastgeleliğin varlığına işaret için bulunmaktadır.->  Daha küçük bir veri seti oluşturmak varken neden mini-batch kullanıyoruz sorusunun cevabı ise, daha küçük bir set oluşturmak elinde olan veri setini daha az sayıda gözlem ile örneklemek demek oluyor. En doğru örneklemi ve örneklem büyüklüğünü tespit mini-batch ile kıyaslandığında daha maliyetli olacağı için mini-batch kullanıyoruz..",
    
### soru 

> quest: "Selamlar herkese, kursta ilerlerken Gradient Descent ve benzeri algoritmalarda Türev,İntegral... gibi Calculus 1-2 konuları ve ilerleyen bölümlerde ise Linear Algebra,Statistics,Probability gibi konuların algoritmayı anlamak için bilinmesi gerektiğini gördüm. 10. sınıf olduğumdan saydığım bu konuların hemen hemen hiçbirini okulda görmedim ki zaten matematik anlamında sadece okula güvenmek yanlış olur. Kendi başıma bu konuları çalışmaya karar verdim, ingilizce / türkçe bu konuları çalışabileceğim kaynaklar önerebilir misiniz? Haftalık olarak konuların gerisinde kalmayacak şeklide ama yeterli matematik altyapısını da öğrenerek ilerlemek istiyorum. Teşekkürler...",

> comments:
  
1. ->  [Link](https://www.youtube.com/watch?v=DJ7DoGoU9E0&list=PLcNWqzWzYG2vUwIrhpYTwqm0qboR5yQRA) Lineer Cebir : Lineer Denklem Sistemleri ve Matrisler ile Gösterimi (www.buders.com)www.youtube.comBUders üniversite matematiği derslerinden lineer cebir dersine ait \"Lineer Denklem Sistemleri ve Matrisler ile Gösterimi\" videosudur. Hazırlayan: Kemal Duran....",
2. ->  bende lisede ve üniversitede bu konuların çoğunu hiç görmedim. ben çok faydasını gördüm görüyorum her aradığın konu için videolar var inşallah işinize yarar..",
3. ->  ->  Tamamdır, teşekkürler",
4. ->  Birkaç tane kaynak önerebilirim senin için umarım faydasını görürsün. Youtube üzerinden jbstatistics kanalına bakmanı öneririm. Özellikle istatistik , olasılık gibi konularda anlamanı kolaylaştıracak bir kanal. [Link](http://tutorial.math.lamar.edu/) bu site üzerinden de temel calculus konularını anlamanda sana yardımcı olacak problemler ve çözümler yer alıyor. Bunu da incelemeni öneririm..",
5. ->  Professor Leonard 'da aynı zamanda Youtube'da bir kanalı olan ve anlatımı son derece iyi olan bir hoca. Bu kanala da bakmanı tavsiye ediyorum. İyi çalışmalar..",
6. ->  ->  Önerileriniz için sağolun 🙂",
7. ->  [Link](https://mml-book.github.io/book/mml-book.pdf) Bu kitapta gerekli konuları bulabilirsin. Ayrıca Matematik Dünyası Dergisinin her bir konu ile ilgili sayılarını edinebilirsin..",
8. ->  Bu konseptleri öğrenmem için (oturup kağıt kalemle işlem yapmama pek fayda sağlamasa da) 3blue1brown'un Essence of Calculus serisinin çok faydası olmuştu bana. Limit, türev, integral kavramlarının ne olduğunu ve aralarındaki ilişkiyi anlamama çok yardımcı oldu. İzlemeni tavsiye ederim..",
9. ->  Kaynak ismi verip kendini o kaynakla kısıtlamamanı öneririm. Eksik olduğun konularla alakalı bir liste yapıp YouTube üzerinden aratabilirsin bence.",
    
### soru 

> quest: "Öncelikle herkese merhabalar ve iyi çalışmalar. Aklımda Epoch ve Batch size ile ilgili bir soru takıldı. Şimdi anladığım kadarıyla verimiz büyük olduğunda tüm bu veriyi batch olarak gradient descent algoritmasına sokmamız performans ve hız açısından problemlere neden olacaktır. Bu yüzden verisetini küçük batchlere bölüp öyle algoritmamıza sokmamız daha iyi olacaktır. Böldüğümüz veri boyutları batch size, Tüm veri setinin itere edilmesi işlemi epoch, tüm batchlerin epoch'a sokulma adımları da iterasyon oluyor. Şimdi benim anlayamadığım şey, benim elimde 10000 verim var ise ve ben bu verileri 100'er batchlere bölmek istiyorsam 1 epoch'un tamamlanması için gereken iterasyon sayısı 100 olacak. Ve ben 10 kez epoch yapılmasını istiyorum yani toplamda tüm epochların tamamlanması için 1000 iterasyon yapılacak. Ben batch size'ımı 200 yaparsam ve epoch sayımı da 20 yaparsam toplamda yapılan itere sayısı mantıken aynı olacak. Bu iki değer kümeleri için de gradient descent algoritması matematiksel olarak aynı oranda mı minimuma yakınsar? (Train esnasında çevresel fakörler olan cpu gpu hızları vs gibi şeyleri ayrı tuttuğumuzda). Şimdiden teşekkür ederim. Edit: Ömer Tüksoy'un düzeltmesi ile sorudaki matematiksel hatalar düzeltilmiştir. (Batch size ve epoch değerleri)",

> comments:
  
1. -> Muhtemelen aynı oranda minimuma yakınsamaz, kesinlikle deneyip görmek gerek ama. farklı batch size'lar ile çalıştırıp optimum sonucu elde etmek en mantıklısı olacaktır..",
2. ->  Bence; İteresyon sayısı eşit olucak ama batch_size=100 alınca epoch 10 alınca her bir weight'i 10 kere güncellerken batch_size=200 alıp epoch 5 alınca her bir weight'i 5 kere güncelleyecek dolayısıyla muhtemelen 10 epoch yapınca loss 5 epocha göre biraz daha küçük olacaktır. Kısacası epoch 5 ise loss function 5 kere güncellenirken epoch 10 iken 10 kere güncellenecek. Dolayısıyla farklı olacaktır. Benim düşüncem bu yönde yanlış düşünüyorsam hocalarımız beni de düzeltirse sevinirim..",
3. ->  ->  Merhaba,Sorulan soruya aşağıda yanıt verirken batch ve epoch kavramlarını açıkladım. Burada ufak bir yanlış anlaşılma var sanırım, iterasyonlar yani güncellemeler her bir epoch başına değil her bir batch_size başına yapılıyor.İyi çalışmalar..",
4. ->  Cevaplarınız için çok teşekkür ederim. Epoch güncelleme sayısı çok aşırı olmamak kaydıyla daha fazla olduğunda daha çok yakınsayacağı bana da mantıklı geldi. Eğer bir hatamız varsa bizi düzeltmekten çekinmeyin. Tekrardan teşekkür ederim..",
5. -> Merhaba Fethi,Soruna geçmeden önce batch ve epoch için net tanımlamalar yapalım: Batch, bir modeli eğitirken 1 iterasyonda (weight update) kullandığın sample sayısıdır. Daha açıklayıcı olması açısından, 10.000 veri için 100 batch seçersen 1 epoch içerisinde 100 kere iterasyon yapmış olursun, bu iterasyon sayısı 200 batch için 50 olur. Epoch ise bir modelin tüm data ile kaç kere eğitileceğidir.Sorunda verdiğin örnekte ufak bir yanlış var, orayı düzeltelim. 10.000 veri, 100 batch ve 10 epoch toplam 1.000 iterasyon (10.000/100*10) yapar. Verdiğin ikinci örnekteki 200 batch ve 5 epoch ise 10.000/200*5 = 250 iterasyon yapar. Dolayısıyla ikinci örnekte de aynı itersyon sayısını sağlamak istiyorsan epoch değerini 20 seçmelisin.Şimdi modelleri karşılaştırmaya geçebiliriz. Tamamen aynı şartlar altında çalışan iki farklı model eğittim. Eğitirken 50 adet 'feature'a sahip 10.000 satır içeren ve uniform dağılmış bir data kullandım. Datanın sınıfları ise 0 ve 1 olmak üzere iki adet. Modellerin her ikisi de tek hidden layerdan oluşuyor ve 64 adet nörona sahip. Sorunun yanıtını daha net görebilmek için ilk modeli 100 batch, 50 epoch seçerek, ikinci modeli ise 200 batch, 100 epoch seçerek eğittim.Aşağıda iki modelin karşılaştırmasını görebilirsin. Her iki model de toplam 5.000 iterasyon yaptığı anda (mavi ve yeşil) train losslar için aynı oranda yakınsamıyorlar ama arada ciddi bir fark da yok diyebiliriz. Tabii burada train datası üzerinden loss aldığımızı unutmayalım.Farklı batch sayıları ile aynı epoch arasındaki ilişkiyi de turuncu ve yeşil çizimleri karşılaştırarak inceleyebilirsin.2 months ago 17 people like this.Like ReportReply",
6. ->  ->  Merhaba,Cevabınız için teşekkür ederim. Ben kafamda epoch ve batch size değerlerini ters orantılı olarak düşündüğüm için hem basit matemaitiksel işlemimde hem de sorumda bir hata oluşmuştu onu da aydınlattığınız için ayrıca teşekkür ederim..",
    
### soru 

> quest: "Merhabalar, ben kurstaki Reducing loss (iterative approach) bölümünü tam olarak anlayamadım, işaretlediğim yerdeki denklemleri ve devamını bana açıklayabilir misiniz, ve işaretlediğim yer türev midir, türevse bunu çözmem için gerekli olan türev bilgisini nasıl öğrenebilirim daha liseliyim, eğer kısaysa sizler açılayabilir misiniz?  Teşekkür ederim.",

> comments:
  
1. ->  Türev değil, y' modelin tahmini, y ise gerçek değeri..",
2. ->  y' türev değil fakat türev de kullanılıyor gradient hesaplarken, en azından basit olarak öğrenmen yararına olur. Oradaki denklemde tahmin yaparken tek feature(x1) kullanarak tahmin yapılmış. Bunu ev fiyatı belirlerken oda sayısı olarak düşünebilirsin, yani o formülde sadece oda sayısını dikkate alarak ev fiyatını tahmin etmeye çalışıyor. W ve b değerleri de ilk olarak 0 atanmış. y' yani tahmin değerimiz de formülde değerleri yerine yazdığımızda 0 çıkacak doğal olarak. Gerçek ev fiyatı 0 olamayacağından hata değerimiz çok yüksek çıkacak ve w ve b değerlerimizi hata değerimiz(loss) azalacak şekilde tekrar tekrar güncelleyeceğiz. Gradient Descent ve Loss kısımlarında daha detaylı olarak bunlardan bahsediyor. Oraları okuduğunda kafanda daha iyi canlanabilir. Lisedeyken böyle şeylerle uğraşmaya başlaman süper, tebrik ederim 🙂",
3. ->  ->  Şimdi anladım, yardımınız için teşekkür ederim.",
    
### soru 

> quest: "Herkese merhablar, böyle basit bir soruyla sizi meşgul ettiğim için özür dilerim ama multiple linear regression çalışırken lineer modellerin genel olarak kolay açıklanabilir olduğunu fakat kolay açıklanabilirliğin çoğu zaman accuracy'den feragat etmeyi gerektirdiğini öğrendim. Lineer modeli daha esnek bir hale getirmek veya tutarlılığı arttırmak için iki bağımsız değişkeni birbiri ile çarpıp modele ekleyebileceğimizi öğrendim. Bu yeni eklenen terime \"interaction term\" denildiğini öğrendim. Fakat yine de hangi koşullar altında, neden böyle bir işlemi yapacağımızı ve nasıl çoklu bağımsız değişkenler üzerinde yapacağımızı kafamda oturtup bunu kullanabileceğimiz bir örnek bulamadım. Rica etsem bu tekniği kısa bir şekilde açıklayıp, kullanabileceğimiz bir örnek verir misiniz?",

> comments:
  
1. ->  Benim bildiğim kadarıyla bağımlı değişkene en az etki eden değişkenler üzerinden etkisiz olanları ya modelden çıkarmalıyız yada dönüştürme işlemleri uygulanabilir. Örnek vermek gerekirse çoklu boyut problemi veya çoklu doğrusal bağlantı problemi gibi problemlerde ilgi değişkenleri azaltmamız gerekebilir.Bunun için PCA , lassa ridge gibi methodlar kullanılabilir. sizin dediğiniz değişkenleri çarpmakta ayrı bir method olabilir , o konu hakkında kesin bir bilgim yok.Özetle asıl amaç en etkili ve etkin bağımsız olan değişkenleri kullanmak. Yanlışım varsa lütfen mentor arkadaşlar düzeltsin.",
2. -> ->  Öncelikle cevapladığınız için teşekkür ederim, benim bahsettiğim metot'da değişken azaltma yok gibime geldi.Şöyle bir modelimiz olsun:y = β1*x1 + β2*x2Biz burada bahsettiğim gibi bir interaction term eklemek istersek modelimizin son hali şu şekilde olur:y = β1*x1 + β2*x2 + β3*x1*x2Anladığım kadarıyla bir \"interaciton\" bağımsız bir değişkenin başka bir bağımsız değişkenin değerine göre bağımlı değişkene yaptığı değişiklik değişiyorsa var oluyor. Mesela şöyle bir durumu inceleyelim bir ilacımız olsun ve bu ilacımızın salgılattığı kolestrol miktarına bakalım ve kolestrol miktarını tahmin etmeye çalışalım. Eğer ilacın alınan dozu (bağımsız bir değişken) 'na göre salgılanan kolestrol miktarı başka bir bağımsız değişken olan cinsiyet değişkenine göre farklılık gösteriyorsa burada interaction term kullanmamız gerekiyor. Örnek olarak bu ilaç örneğinden devam edelim.Cinsiyetin salgılanan doza etkisi olmadığı durumdaki modelimiz:y = α + β1 ∗ doz şeklinde olurduCinsiyetin etkisi olsaydı ama interaction ile alakası olmasaydı (hangi cinsiyet olduğu farketmeden alınan doz her iki cinsiyet'de de aynı büyümeyi sağlasaydı):y = = α + β1 ∗ doz + β2 * cinsiyet (burada cinsiyet dummy variable olarak kabul ediliyor)Cinsiyetin etkisi interaction ile birlikte olsaydı:y = α + β1 ∗ doz + β2 * cinsiyet + β3 × doz ∗ cinsiyetİsterseniz bunların grafiğini çizmeyi deneyip aklınızda daha da oturmasını sağlayabilirsiniz.Ama verdiğim tüm bu örnekler 2. bağımsız değişkenin bir dummy variable olduğu durumlardı. Sayısal bir değer olsaydı nasıl olurdu, veya gerçek hayatta bu tür bir metodu nasıl kullanabilirim hala daha anlayamadım. Eğer bir yanlışım varsa veya eklemek istedikleri bir yer varsa değerli mentörlerimiz yardımcı olabilir mi? Cevabınızı 4 gözle bekliyorum. Sağolun...",
3. -> Merhabalar,Bu durumu en iyi açıklayan konunun 'kernel trick' olduğunu düşünüyorum. [Link](https://towardsdatascience.com/truly-understanding-the-kernel-trick-1aeb11560769) Bu makaledeki şu resimdeki data seti inceleyecek olursak. [Link](https://miro.medium.com/max/1440/1*eU9PzjVcLNbNEzBC2g_iWg.jpeg) Bu data set doğrusal olarak sınıflandırılabilir değil. Bunun için örneğin özelliklerimizin x ekseninin X, y eksenin de Y özelliği olduğunu varsayarsak (0,0) noktasını merkez gibi görünüyor.Her örneğin merkeze olan uzaklığını tanımlayacak bir yeni özellik tanımladığımızı düşünelim sqrt((xi-x0)**2 + (yi-y0)**2) şeklinde hesaplanan yeni bir özelliğimiz olunca aslında sağdaki şekil elde edilmiş olacak.Sağdaki şekilde doğrusal olarak ayrıştırılabilir bir probleme dönüşüyor.Artık kısaca (yaklaşık olarak - aynı örneğe göre-) yeni özelliğe göre 0.5 değerinden küçükler bir sınıfa ait büyükler diğer sınıfa aittir söylenebilir (Bu dataya bakarak bizim gözle gördüğümüz tabikide).İyi çalışmalar,Mehmet",
4. -> Soru oldukça güzel.Öncelikle değişkenlerin birlikte ele alınması (x1 * x2) çalışılan konu için mantıklı bir yaklaşım olmalı.Regresyon modellerinde hangi modeli kullanmamız gerektiği sorusu ile karşılaştığımızda modelin R^2' (Belirlilik katsayısı) (Coefficient of Determination)sini kullanırız. Daha yüksek R^2 daha açıklayıcı model demektir. R^2 0 ile 1 arasında değişen değerler alır. Örneğin bir regresyon modelinin R^2 si 0.72 çıkması bağımlı değişkendeki değişimin yüzde 72'si model tarafından açıklanıyor şeklinde yorumlanabilir. Burada çok çok önemli bir nokta bir modelin veriye uyumunu değerlendirmek için bir tek R^2'ye güvenmemek gerek. İstatistikçiler hata terimlerinin dağılımı, katsayıların anlamlılığı, tahminlerin güven aralığı gibi farklı teknikler ile de modellerinin kalitesini ölçerler.Basit bir regresyon modeli (y = beta0*x1 + beta1*x2) nin belli bir R^2 si var buna bağımsız değişkenlerin birlikte ölçümlerini eklersek (y = beta0*x1 + beta1*x2 + beta2*x1*x2) acaba daha iyi bir model elde eder miyiz ?Bunun cevabı için R^2 ye başvurmak yeterli mi ? Maalesef değil. Çünkü bir modele yeni terimler eklemek her zaman R^2 yi yükseltir. Bu yüzden yeni terim eklediğimizde modelin veriye uyumunu kontrol etmek için terim sayısının etkisinden arındırılmış Düzeltilmiş Belirlilik Katsayısı (Adjusted R^2) kullanılmalıdır. Daha yüksek Adjusted R^2 daha iyi model demek..",
5. ->  Bunun yanında model katsayılarının anlamlılığını test edip eğer interaction_term'in katsayısı istatisiksel olarak anlamlı ise (p<0.05) ise modeline eklemelisin.Not: Python'da yukarıda verilen hesaplamalar kolaylıkla yapılabiliyor.Gerçek bir örnek olarak cinsiyet yerine başka bir ilaç düşünebilirsin. İlaçlar ayrı ayrı kullanıldıklarında etkileri olumlu olsun ama beraber kullanıldıklarında girdikleri tepkime sonucu ürettikleri başka bir kimyasal yüzünden olumsuz etki göstersin. Bu tip bir durumda bu iki ilacın birlikte ele alınıp regresyon modeline eklenmeleri gerekir..",
6. ->  ->  Çok net anlatmışsınız, teşekkürler..",
7. -> Bulunduğum sektörden (sigortacılık) şöyle bir örnek ile açıklayabilirim.Araçların kasko fiyatlarını tahminlemek için genelleştirilmiş lineer modeller kullanırız. Aslında söylemde basitleştirmek istersek olasılık dağılımı değiştirilmiş multi lineer regresyon. Örneğe gelecek olursak sürücülerin yaşı ve cinsiyeti bilgileri ile kasko fiyatlarını tahmin ettiğimizi düşünelim. Fiyatlar regresyon modellerinde erkeklerde 45-50 li yaşlarda, kadınlarda 40 - 45li yaşlarda bir tık yüksek tahminlenir. Bunun sebebi ise araştırıldığında bu yaşlardaki insanların çocuklarının gençlik dönemlerinde olduğu ve araçları izinsiz alarak kazalar yapıkları şeklinde açıklanır. Bu durum herkes için geçerli olmayacağından o yaşlar için interaction yapılması gerekmektedir. cinsiyet ve yaş Interaction'ı sonrası elle bir miktar modele müdahale edilerek fiyattaki bu artış bir miktar törpülenir. Interaction kurmazsanız yaş ve cinsiyet bağımsız değişkenlerini birlikte etkileşime sokamaz ve yalnızca istediğiniz noktaya müdahale edemezsiniz. Interaction'da zaten etkileşim anlamına gelmektedir. Modellerde doğrusunu bildiğiniz ve düzeltebileceğiniz istisna durumlar için belirli değişkenler arasında kullanırsınız. Umarım bir miktar açıklayabilmişimdir.",
    
### soru 

> quest: "Learning Rate kısmında bir grafik var, learning rate'i kendimiz ayarlayıp lossu minimize edebiliyoruz. Kafama takılan büyük bir learning rate seçmek burda avantajlı gibi gözüküyor, ilk adımı büyük atıyor minimum lossa yaklaştıkça adım büyüklüğünü azaltıp hedefe ulaşıyor. Bunun normalde mümkün olmaması gerekiyor sanırım, Rate eğitim boyu aynı kalması gerekmez mi?",

> comments:
  
1. ->  Learning rate orada da eğitim boyunca aynı, orada adım olarak gördüklerin gradient descent sonucu oluşan değerler. Gradient descent işleminde local minimuma yaklaşıldıkça vektörlerin boyu kısalır.Aşağıda linkini koymuş olduğum videoyu izlersen kafanda biraz daha oturacaktır. Anlatılan ders Gradient, dolayısıyla oklar local minimuma doğru değil local maximuma doğru gidiyor. Ancak biz lossu minimize etmeye çalıştığımız için bu vektörlerin negatiflerini kullanıyoruz.[Link](https://www.khanacademy.org/math/multivariable-calculus/multivariable-derivatives/gradient-and-directional-derivatives/v/gradient-and-graphs),
"Gradient and graphs (video) | Khan Academywww.khanacademy.orgLearn how the gradient can be thought of as pointing in the \"direction of steepest ascent\". This is a rather important interpretation for the gradient. ",
2. ->  learning rate ile costun türvini çarpıyoruz cost azaldıkça learning ratein etkiside azalıyor gibi anladım ben.Weight = Weight - (learning rate)x(costun türevi)",
3. ->  ->  cost ile mean square errorü kastettim. Bide yukarıda cost azaldıkça değilde costun türevi azaldıkça demek istedim. Minimuma yaklaştıkça costun türevi azalıyor, türev azaldıkça learning rate ile çarpımı daha az değişikliğe sebep oluyor..",
4. -> ->  Kursta gradient olarak adlandırdığımız değişken cost'un türevi dediğiniz değer mi? Bu fonksiyonun adı cost olarak mı geçiyor?",
5. ->  ->  Eger cost , mean square error dersek yanlis olur.Hedefimiz prediction ise MSE, ancak classification ise Categorical Cross Entropy ya da Binary Cross Entropy olabilir.Yani cost function probleme gore degisir.Buun disinda soylediklerinize katiliyorum.",
6. -> Learning rate sabitken bile adımlar itere edildikçe kısalır çünkü gradient descent fonksiyonunda cost fonksiyonumuzun derivative'İ alınır ve bu değerimiz*learning rate theta değerimizden çıkarılır. Theta değeri dediğimiz değer bizim minimize edilmiş cost function değerimizdir. Örneğin theta1 lineer regresyon formülündeki (y=ax1+bx2+cx3) x1 değerini temsil eder. x2 ve x3 için de değerler aynıdır.(Burada 3 tane feature'ımız varmış gibi kabul ettiğimiz için x3'e kadar gittik. Bu featurelar daha az veya daha fazla olabilir.) Yani her iterede bir sonraki theta değeri daha az kısalır. Resimde formülü görebilirsiniz. Learning rate için kesinleşmiş bir değer yoktur, veri setine göre en optimum learning rate değeri değişir. Learning rate olması gerekenden büyük olursa minimum local değerini ıskalar ve böylece aslında minimum değere yaklaşacağına uzaklaşmış olur. Learning rate değerimiz çok küçük olursa da minimuma ulaşmak çok fazla vakit alır. Aslında orada değişen şey learning rate değil derivative'i alınmış fonksiyonun azalma sayısıdır. Örneğin 10x^2 değerinin derivative'ini alırsanız 20x olur. Yani ilk iterede 10x^2-20x kadarlık bir azalma olmuş. Tekrar 20x'in derivative'ini alırsanız değer 20 olur yani burada da 20x-20'lik bir azalma olmuş. (10x^2-20x) > (20x-20). Bu nedenle atılan stepler de itere boyunca küçülür.",
7. ->  Aslında şöyle düşünülebilir, bir arazide su birikintisini arıyorsunuz ve learning rate sizin adımınızın mesafesi olsun. Su birikintisine yaklaşana kadar büyük büyük adımlar atıyorsunuz mesela 5 metre diyelim ama artık su bitikintisine 3 metre uzakatasınız. 5 metre adım atınca diğer tarafta 2 metre uzaktasınız. Bir türlü yaklaşamıyorsunuz. Mecburen adım mesafenizi 5 metreden daha küçük hale getirmelisiniz ki mesela 1 metre olsun. 2 metre uzaklıktan sonra 2 iterasyon sonra yani 2 adım sonra artık su birikintisindesiniz.En başında da adımınızı 1 metre atabilirdiniz ama su birikintisine ulaşana kadar çok adım atmış olurdunuz. Bu da sizi yani sistemi yorardı. Amacımız minimum adımla su birikintisine ulaşmak aslında. Böyle düşünebilirsiniz.",
    
### soru 

> quest: "Learning rate, batch ve epoch değerlerinim optimumunu bulmak için her zaman deneysel bir yol mu izlemeliyiz ? Bu değerleri bulmanın deneysel yoldan farklı olarak formül yada farklı bir yolla bulmak mümkün mü?",

> comments:
  
1. ->  Bunları bulmanın direk bir formülü yok ne yazık ki. Sıfırdan bir algoritma geliştirmiyorsanız genelde kullandığınız algoritmanın makalelerinde bu tarz değerler en alt sayfada paylaşılır(veya makale içerisinde). Tabiki bu yaptığınız uygulamanın türü ve elinizde ki verinin yoğunluğuna göre çok değişim gösteriyor. Aslında işin mühendislik kısmı bunlarla oynayıp(hyperparamer tuning) ve belirli optimizasyonlar yaparak(örneğin modelin eğitilme süresi - deneme süreniz - hesaplama gücü kullanımı vs ) optimal değerleri seçmekteKısacası , direk formül yok ama yapılmış çalışmaları incelemek en yararlısı..",
2. ->  Bunu ben de merak ediyorum. Lakin her veri kümesi ve içerisindeki gürültü vs. gibi faktörler farklı olduğundan sanırım bir formülü yok. Kursta da belirtildiği gibi, gözetimli öğrenmede (supervised learning) bir makine öğrenme algoritması birçok örneği inceleyerek kaybı en aza indiren en iyi modeli bulmaya çalışıyor. Bu sürece de ampirik risk minimizasyonu (empirical risk minimization) deniyor. Ampirik = Deneysel yani deneyerek bulmak gerekiyor diye biliyorum. Lütfen hatam varsa düzeltiniz. İyi günler????..",
3. ->  En iyi learning rate değerinin ne olduğunu bulmak için analitik bir yöntem yok. Bunun için diğerlerine göre daha iyi olan learning rate'i deneysel yol izleyerek buluyoruz..",
4. ->  Büyük bir veri setinde bu değerleri bulmak deneyerek bulmak zaman kaybına neden olabilir baya",
5. ->  ->  Deneyerek bulmak yerine akademik çalışmalarda çok kullanılan değerleri deneyip sonuçları inceledikten sonra hangi kısımlarda yanlış tahminler veiryor ona göre tune / ince ayar çekmeniz gerekir.",
6. ->  [Link](https://arxiv.org/pdf/1506.01186.pdf) şu yayını okumanı tavsiye ederim.Ramazan Kartal Hiper parametre analizi icin 'Grid Search' konusuna bakabilirsin. [Link](https://medium.com/deep-learning-turkiye/derin-ogrenme-uygulamalarinda-model-dogrulama-ve-hiper-parametre-secim-yontemleri-823812d95f3)",
"Derin Öğrenme Uygulamalarında Hiper Parametre Seçim Yöntemlerimedium.comDerin öğrenme uygulamalarında hiper parametreler yazı serisinin ilk bölümünde, derin öğrenme uygulamalarında en sık kullanılan hiper….",
"Muhammed Fatih Gültekin Optuna diye bir kütüphane var bakabilirsin.",
7. -> Grid search, random search veya informed search gibi metodolojilere bakılabilir. Tabi bunlar da deneme temelli yöntemler.",
"Ramazan Ünlü Learning rate için kesin bir kural yok. Ancak gradient descent’ın mantığı gereği hangi değeri seçerseniz seçin optimum noktaya yaklaştıkça etki değeri kademeli olarak azalır. Çok küçük değerler seçildiğinde local minumuma düşme riski artar. 0.01 0.001 gibi değerler yeterince iş görüyor genelde.Epoch için bayesian error ile eğitim setindeki hata karşılaştırılabilir. Eğer yeterince veriniz varsa, ağ yeterince büyükse ancak bayesian error ile eğitim setindeki hata farkı yüksekse epoch sayısının artırılmasının fayda sağlayacağı sonucuna varılabilir..",
8. ->  Ramazan Ünlü 'Epoch icin Bayesian error ile egitim setindeki hata karsilastirilabilir ' ifadesini hic anlayamadim.Egitim seti derken egitim sirasinda demek istediniz sanirim.Egitim sirasindaki hata derken loss dan bahsediyorsunuz saniyorum.Soylemek istediginiz: Hedefiniz prediction yapmaksa Mean Square Error kullanarak hesaplayacaginiz loss degerine bakilarak epoch konusunda karar verebilirsiniz gibi birsey sanirim.",
9. ->  Deneyerek learning rate gibi hyperparametrelerin optimum degerini bulmaya calismanin zaman tuketici bir is oldugunu gozonune aldigimizda,bu isin basinda oldugumuzu varsayarak en iyi yontem uzmanlarca denenmis kullanagelinene basvurmak olabilir diye dusunuyorum.Mesela Adam optimizer ile learning_rate=1e-3 kullaniyorum ya da yukarida soylendigi gibi 0.01 ya da 0.001 seciyorum genellikle...",
    
### soru 
> quest: "merhaba! reducing loss konusuna çalışırken gradient descent ve stochastic gradient descent arasındaki farkı tam anlayamadım. bana bu konuda yardımcı olabilir misiniz?",

> comments:
  
1. ->  Merhaba, bu sorunun aynısı sorulmuştu ->  arkadaşımız tarafından. O post'u okursan soruna yanıt bulabilirsin bence..",
" ->  ->  Onu okumuştum ama orda mini-batch SGD ve SGD arasındaki fark sorulmuş ve yanıt olarak batch ifadesi açıklanmış daha çok fakat ben GD ve SGDyi sormuştum bildiğim kadarla bu ikisinde batch size hiperparametresi girilmiyor bu yüzden sorumun yanıtını tam alamadım ama yine de teşekkürler yanıtın için.",
2. ->   ->  Gradient Descent loss'u minimize etmek için kullanılan yöntemin adı. Stochastic Gradient Descent ise loss'u minimize ederken veri setimizdeki her bir örneğimiz için gradient descent'in bir adım atması anlamına geliyor. Yani tek bir örneğe bakarak loss'taki değişimi hesaplayıp parametreleri güncelliyor. Gradient Descent'in önüne gelen kelimeler, kaç tane örneği dikkate alarak iterasyon yapacağımızı belirtiyor. Aslında SGD, batch-size'in 1 e eşit olduğunu belirtmek için kullanılan özel bir isim sadece.2 months ago 16 people like this.Like ReportReply",
3. ->  ->  şimdi anladım teşekkür ederim.",
    
### soru 

> quest: "Merhaba. Train-Test setlerimizi ayırırken, videoda 1 milyar datamız olsa dahi %10 test %90 train olarak ayırmaktan bahsediliyor. 1 milyar gayet büyük bir rakam. Bu şekilde ayırmak eskiden(küçük datasetler için) mantıklıydı ama artık bir çok konuda big data mevcut. Bunu %98 train, %1 test(10 milyon!), %1 validation(10 milyon!) olarak ayırmamız daha mantıklı olmaz mı?",

> comments:
  
1. ->  Evet ben de andrew ng deep learning kursunda dediğiniz gibi 98-1-1 şeklinde yapılmasının daha mantıklı olduğunu duymuştum 🙂.",
2. -> Öncelikle literatürde bu konu üzerinde net bir fikir birliğine varılmış değil. Kimi araştırmacılar 80:20 oranını benimserken kimileri 90:10 bazıları 70:30 oranlarının daha iyi sonuçlar vereceğini iddia ediyorlar. Andrew Ng çok büyük veri setlerinde 98:1:1 ve hatta 99:0.5:0.5 oranlarının kullanılması gerektiğini ifade ediyor. Klasik makine öğrenmesi algoritmalarında veri miktarı arttıkça başarı oranı belli bir noktadan sonra düzlüğe ulaşmakta, derin öğrenme algoritmalarında ise veri miktarı arttıkça başarının artışı daha doğrusal. Bu yüzden klasik makine öğrenmesi algoritmaları kullanıyorsanız train:test oranını biraz daha düşük tutabilirsiniz. Derin öğrenme algoritmalarında ise eğitime mümkün mertebe daha fazla veri sağlamak daha başarılı sonuç demek.Tabii ki gerçek bir uygulamada en güzeli farklı oranları deneyip sonuçları karşılaştırmak.Ayrıca ek olarak toplamda ne kadar veri kullanmalıyım sorusunu kendinize sorabilirsiniz.Bir modelin eğitimi için ne kadar veri gerektiği İstatistiksel Öğrenme Teorisi (Statistical Learning Theory) ile çözülebilecek bir problem. Bu büyüklük kullanacağınız modelin karmaşıklığı, parametreler arasındaki ilişkiler, gürültülü veri miktarı ve her bir değişkenin varyansı gibi çeşitli etmenlere bağlı.Peki bunları bilmeden bir makine öğrenmesi algoritması geliştirilebilir mi? Sorunun cevabı evet. Her problem farklıdır. En güzel yöntem farklı oranları deneyip sonuçları karşılaştırmak.",
3. ->  Test ve validation datasını o kadar büyük tutmanın çok bir manası yok. Dediğiniz gibi 10 milyon test datası pek kullanışlı değil. Belirli bir boyutlara ulaştığınızda o oranları kendinize göre şekillendirebilirsiniz. Gerçek hayat uygulamaları yaparken \"benchmark\" datasetleri gibi elinizde çok data bulunmayabiliyor..",
4. ->  ->  %1i 10 milyon yapıyor, söylendiği gibi %10 teste ayırsak 100 milyon test datası olacak.",
5. ->  ->  Onu hesaplamadım tabi direk yazılan oranı aldım ama genelde 10 bin yeterli bir sayı. 100 Milyonluk kaliteli etiketli veri (resim olarak düşünürsek) çoğu zaman elde edebildiğimiz bir veri değil. Benchmark , algoritmaların denendiği datasetlerde ancak böyle veri olabiliyor.",

### soru 

> quest: "Merhabalar. Goldilocks learning rate değeri tam olarak nedir ve nasıl bulunur? Tam anlayamadım açıklayabilir misiniz ? Teşekkürler. [goldilocks](https://community.globalaihub.com/community/hashtag/goldilocks/)

> comments:

1. -> Bunun için öncelikle Goldilocks prensibinin tanımına bakmamız gerekiyor. [Link](http://www.tolgaakkus.com/goldilocks-prensibi-79-gun/) websitesinden aldığım kısa bir özeti alıntılamak istiyorum.\"Ormanda yaşayan üç kişilik bir ayı ailesi var. Anne ayı yaptığı çorbaları tabaklara doldurur ve çorbalar soğuyana kadar ailecek dışarı çıkarlar. O sırada dışarda gezen minik kız Goldilocks evi görünce içeri girer ve çorbalara ile karşılaşır. Önce büyük tabaktaki baba ayının çorbasına bakar ama ağzı yanar, sonra orta tabaktaki anne ayının çorbasına bakar yine ağzı yanar, en son küçük tabaktakini içer, küçük çorba her şeyi ile tam Goldilocks’a göredir.\"Buna göre burada Goldilocks learning rate aslında gradient descent algoritmamızın en optimizasyonlu duruma gelmesi için gereken learning rate değeridir. Goldilocks prensibini hatırlarsak burada Goldilocks kendi ağzının yanmayacağı optimum tabak büyüklüğünü bulup ondan çorba içiyordu.Learning rate ise gradient descent algoritmamızda her bir iterasyonda optimum değere atılacak adım büyüklüğünü temsil eder. Ama önemli bir not vermek istiyorum; itere edildikçe optimuma atılan adımlar azalır bunun nedeni cost function'ımızın her derviative'de daha da küçülmesidir. Yalnız bu sizi yanıltmasın, learning rate'i seçerken bu adım küçülmelerine güvenmemeliyiz çünkü bahsettiğim bu küçülmeler 2-3 iterasyonda olan şeyler değil ve learning rate her iterasyonda atılacak adımın büyüklüğünü simgelediği için learning rate'İ küçük seçerseniz algoritmanız çok yavaş, çok büyük seçerseniz de optimum noktayı aşacağı için yanlış çalışacaktır. Uygun learning rate'i bulmak için ise bir çok yöntem mevcut:Sabit değer olarak belirlenebilir, ya da adım adım artan bir değer olarak da belirlenebilir (örneğin belli bir öğrenme adımına kadar 0.001 o adımdan sonra 0.01 gibi), momentum değerine bağlı olarak belirlenebilir ya da adaptif algoritmalar tarafından öğrenme esnasında öğrenilebilir.Burada benim hakim olduğum yöntem bir değer aralığı belirleyip o değer aralığında kalan değerleri learning rate değeri olarak verip algoritmayı denemek. Örneğin 0.1-1 arasında bir aralık seçtiğinizde ve gradient descent algoritmasında learning rate olarak verdiğinizde çıkan sonucun optimuma yaklaşma step sayısı, optimumdan uzaklaşma durumlarına göre doğru aralığı seçebilirsiniz. Eğer bu aralıkta learning rate bulamazsanız aralığınızı genişletebilirsiniz. Diğer learning rate bulma yöntemleri ile ilgili bir bilgiye sahip değilim maalesef ama olunca burayı güncellerim 🙂 Yanlışığım eksiğim olursa lütfen düzeltmekten eklemekten çekinmeyin, umarım açıklayıcı olmuştur :)Önemli bir trivia: Gradient Descent algoritmamız ne kadar optimumsa grafiğimizdeki cost function çizgimiz o kadar düzdür ve eğimi yoktur. Derivative de zaten eğim almak demek olduğu için artık alınacak bir eğimi kalmadığından artık iterasyonlar optimum değere ulaştıktan sonra cost function değerimizi küçültmeyecektir. Buradan şöyle trivia bir şey söyleyebilirim. Eğer Gradient Descent algoritmamız lokal minimum değerine ulaştıysa ve bulunduğu lokal minimumdan daha optimum bir lokal minimum değeri varsa bir sonraki iterasyonda daha optimum olan lokal minimum değerine yaklaşmaz, bulunduğu noktada kalır. Resimdeki denklemi incelediğinizde optimuma ulaşan cost function değerinin eğimi (derivative'i) sıfır olacağı için (resimdeki grafikte mevcut) thetaj=thetaj olur yani değer değişmez.2 months ago 36 people like this.Like ReportReply",
2. ->  ->  Teşekkür ederim.",
3. ->  Merhaba. Anladığım ve araştırdığım kadarıyla; Goldilocks Prensibi, matematik ve istatistikte \"Bias ve varyanstan gelen hataları azaltmak için 'mükemmel esnekliği' temsil eden doğrusal regresyon modelini\" ifade ediyor. Yani olabilecek en iyi modeli ifade ediyor. Kursta da belirtildiği gibi Goldilocks değeri, olabilecek en iyi learning rate değeridir ve kayıp fonksiyonunun ne kadar düz olduğu ile ilgili olan bir şeydir. Kayıp fonksiyonunun gradient'inin büyüklüğüne göre, o büyüklüğü telafi etmek için ona göre learning rate seçiliyor. Ben böyle anladım, yanlışım veya eksiğim olabilir. Tabii ki eleştiriye ve düzeltmelere açığım. İlk soru cevaplama deneyimim, mentorlarımız daha iyi cevabı verecektir????. İyi günler.2 months ago 12 people like this.Like ReportReply",
4. ->   ->  Teşekkür ederim.",
5. ->  Arkadaşlar goldilocks kavramını learning rate ile birlikte açıklamış ama kurstaki sorunun cevabı olan 1.6 değerini nasıl bulduğumuza değinmemiş. Bu konuya açıklık getirebilecek bir arkadaşımız var mıdır ?.",
6. ->  Merhabalar, eklediğim görselde açıklamaya çalıştım..",
7. ->  Eğitimin bir kısmında şu geçiyordu: \"Mükemmel öğrenme oranını bulmak şart değildir 'yeterince' hızlı yakınsayacak değer seçmek yeterlidir.\" Belirli öğrenme oranlarını seçmek çoğu zaman işe yarıyormuş. Andrew NG'den hatırladığım kadarıyla 0.03 0.01 0.1 0.3 değerleri gibi.",
8. ->  Cevaplar için teşekkürler ancak bu soruda nasıl 1.6 cevabını bulduğumuzu açıklamıyor:) soğuk ılık ve sıcak değerlerine sırasıyla 1,2 ve 3 verdikten sonra mı bu learning rate oranını bulacağız bu örnekte ?.",
9. ->  ->  Merhaba, 1.6 olmasının sebebi gradient descent fonksiyonumuzun sadece bir adımda optimum değere ulaşmasındandır. Amacımız olabilecek en kısa sürede ve adımda minimum noktasına gitmek. Learning rate'imiz minimum değere atacağımız adımı simgelediği için 1.6 büyüklüğünde adım bizi tek seferde minimuma götürdü. Burada Learning Rate değeri deneme yanılma ile bulunur kesin olarak her modelde 1.6'dır veya sabittir diyemeyiz. İyi çalışmalar dilerim.",
10. ->  ->  Örnek, bize orada öğrenme oranındaki değişimin adım sayısını nasıl değiştirdiğini göstermek için konulmuş sadece. Yani her iterasyonda en iyi değeri bulmak zorunda değiliz. Ne çok büyük olsun ne de çok küçük orta değerler işimizi görüyor. Önceki yorumda yazdığım gibi genelde ilk denen değerler var. Bu değerlere göre arttırıp azaltacağımıza karar veriyoruz..",
11. ->  ->  teşekkürler yanıt için.Ben buna yakın bir düşündüm ve sonucun 1.5 olabileceğini varsaydım. Cevap olarak 1.6 yazınca kaçırdığım bir şey olabilir diye sormak istedim teşekkürler..",
    
### soru 

> quest: "İlk soruyu soracak olmanın verdiği heyecanından dolayı yanlış bir şekilde sorarsam kusuruma bakmayın. Bugün ki çalışmamda Reducing Loss ile alakalı kafamda takılan konu Stochastic Loss ile mini batch loss function arasındaki farkı çok iyi anlayamadım. Bana bu konuda yardımcı olabilir misiniz? [ReducingLoss](https://community.globalaihub.com/community/hashtag/reducingloss/) [Minibatch](https://community.globalaihub.com/community/hashtag/minibatch/)

> comments:
  
1. -> Merhaba.Özetle şunu söyleyebilirim.Derin öğrenmede veri setinde bulunan tüm verileri aynı anda işleyerek öğrenme maliyetli bir iş.Veri sayısı ne kadar fazla ise hesaplama da o oranda fazla sürmekte.Bu problemi çözmek için veri seti küçük gruplara ayrılmakta ve öğrenme işlemi seçilen bu küçük gruplar üzerinde yapılmakta.Birden fazla girdinin parçalar halinde işlenmesi “mini-batch” olarak adlandırılmakta.Yani Stochastic gradient descent ile veriyi tekbir bütün olarak incelerken Mini-batch stochastic gradient descent ile daha küçük parçalara ayırarak inceleniyor..",
2. -> eski izlediğim videolarıda göz önünde bulundurarak mini batch parametresini kullanırken veriyi küçük küçük ayırıp modele yavaş yavaş veriyor diyebiliriz fakat burada bazı veriler uygun iken bazı veriler uygun olmayabilir bu yüzden gürültü çok olur burada önemli olan anladığım kadarıyla learning rate değeri önem kazanıyor onuda 1 yaptığımızda stochastic parametresi ile aynı işi yapmış oluyoruz bende eğitimden kalan bilgilerim ile cevap vermeye çalıştım umarım birşeyleri doğru anlamışızdır 🙂 iyi çalışmalar.",
3. ->  stochastic gradient descent öğrenme işlemini yaparken sadece tek bir örneğe bakmıyor mu ? ->.",
4. -> Veri setine bütün olarak bakıyor..",
5. ->  Stochastic gradient descent (SGD) takes this idea to the extreme--it uses only a single example (a batch size of 1) per iteration. Given enough iterations, SGD works but is very noisy. The term \"stochastic\" indicates that the one example comprising each batch is chosen at random yazıyor da o yüzden sordum. sizin dediğiniz bütün olarak bakmasına batch gradient descent deniyor diye biliyorum. ->.",
" ->  Biraz araştırdığım kadarıylaSchoastic gredient decent ile her iterasyonda sadece bir veri üzerinde işlem yapıyoruz.Bütün olarak bakmasına batch gredient descent deniyor.(Aslında Mini-batch değerini eğitim kümesinde bütün eleman sayısı kadar yaparsak, eğitim kümesindeki tüm veriler eğitime gireceği için yapılan işlem de yine “batch gredient descent” oluyor.)Mini-batch stochastic gradient descentte ise seçilen değerin 1 ile eğitim kümesindeki veri sayısı arasında ne çok küçük ne de çok büyük olmayan bir değer olarak belirlenmesi gerekiyor. ( Crash course da yanlış hatırlamıyorsam 100 ile 1000 arasında demişti. Elimizdeki veriyi yüzlük , binlik şekilde parçalara ayrıldığımızı düşünün).Bu konuyu ben de bugün yeni öğrendim. Lütfen herhangi bir hatam varsa belirtiniz ????",
6. -> Gradient Descent algoritması Cost Function'ımızı optimize etmek için uyguladığımız bir algoritmadır. Optimizasyonu ise cost function'ımızın sürekli derivative'ini alarak yapmaktadır. Cost functionımız ise aslında tüm tahmin edilen y değerleri-gerçek y değerlerinin karelerinin toplamı/(veri sayısı * 2). (Resim olarak ekledim. 2m olmasının sebebi derivative alınırken kolaylık sağlanması içindir.)Gradient Descent formülümüze baktığımızda ise (Resim olarak ekledim.) Cost function'ımızın derivative'inin alındığını ve bunun optimize olana kadar devam ettiğini yani iterative olarak yaptığını gözlemleyebiliriz. Gradient Descent formül resmindeki kırmızı alan ise bizim cost function'ımızdır. Gradient Descent algoritmamız her iterasyonda cost function'ımızın derivative'ini alır. Bu iterasyon sırasında itere edilen cost functionımızın range'ini belirleyebiliriz. Bu range çeşitlerine göre de Gradient Descent ayrılır. Örneğin;Batch Gradient Descent: Her iterasyonda tüm veri seti için cost function hesaplar ve hepsinin derivative'ini alır. Veri fazlalaştıkça bu yöntem yavaşlar.Mini-Batch Gradient Descent: Her iterasyonda tüm veri setini almak yerine veri setinin belli bir kısmı için cost function hesaplayıp derivative'ini alır.Stochastic Gradient Descent: Her iterasyonda verisetinden sadece bir örnek için cost function hesaplayıp derivative'ini alır.Eğer bir hatam veya eksiğim olduysa düzeltmelere açığım 🙂 Umarım açıklayıcı olmuştur 🙂2 months ago 22 people like this.Like ReportReply",
7. ->  ->  harika açıklamanız için çok teşekkür ederim. cost fonksiyonunu pythonda yazarken loss = np.sum(deltas**2) / 2 / observations olarak ifade ettiğimizi hatırladığım için yazma gereği duydum. hatam olabilir olursa lütfen bildirmekten çekinmeyin. Fethi hocam yukarıda \"y değerleri-gerçek y değerlerinin karelerinin toplamı/ortalamadır\" demişsiniz. burdan anladığım kadarıyla \"ortalama = m\" değeri oluyor yukardaki verdiğiniz resim ekine göre de. benim anladığım m = gözlem sayısı demek. acaba cost fonksiyonu için \"karesi alınmış farkların, toplamının, ortalaması\" mı demek istediniz de ben anlayamadım tam olarak kafam karıştı. çok yeni olduğum için burda verilen cevaplar bazen aklımda kural olarak kalabiliyor.",
8. ->  ->  Merhaba öncelikle cevabınız için çok teşekkür ederim. Evet orada bir hata yapmışım ve bu cevabı yazdıktan sonra da düzeltmesini yapacağım. Oradaki M, verisetindeki veri sayısı oluyor yani sizden alıntılayarak ve biraz düzenleyerek şu çıkarımınızı onaylayabilirim: \"cost fonksiyonu \"karesi alınmış farkların karelerinin toplamı değerinin veri boyutu*2 'ye bölünmesidir. Örneğin verisetimde 100 değer var ise bu 100 değer içn tahmin edilen değer - olması gereken değerlerinin karelerinin toplamı / (2*100) yaptığımzda cost function değerimizi bulabiliyoruz. (2 olmasının sebebi kareli değerin derivative'i alındığında işlem kolaylığı sağlamak) Fark toplamları/(örnek sayısı*2)=cost function değeri bulunuyor yazmam gerekirken uykusuzluğun da vermiş olduğu bir hal ile yanlış yazmışım 🙂 Eğer anlatamadığım bir yer var ise lütfen yazmaktan çekinmeyin 🙂.",
9. -> Veri setindeki tüm verileri aynı anda işlemek, hem zaman hem de bellek açışından çok maliyetli bir süreçtir. Maliyeti minimize etmek, tahmine bağlı hata oranını en aza indirmek için de en uygun ağırlık değerlerini bulmaya çalışıyoruz. Bunu da gradient descent optimizasyonu ile yapıyoruz. Veri miktarı ne kadar fazla ise bu hesaplama da o kadar uzun sürüyor. Dolayısıyla veri setini küçük gruplara ayırarak öğrenme işlemini bu küçük gruplar üzerinden devam ettiriyoruz. Bu şekilde verinin küçük gruplar halinde işlenmesi mini-batch olarak adlandırılıyor. Mini-batch parametresi olarak belirtilen değer, modelin aynı anda kaç veriyi işleyeceğini belirtiyor. Veri genelde 10-1000 arasında random seçilmiş örneklere bölünerek işleniyor. Mini-batch değerini 1 olarak belirlediğimizde de alabileceği en küçük değeri almış oluyor, buna da stochastic gradient descent diyoruz. Yani her iterasyonda sadece tek bir veri üzerinde işlem yapılması durumu. Her iki uygulamanın da, min-batch ve stochastic, kendi içinde artı/eksi yanları var. Önemli olan burada sizin veri setiniz ve belleğinize en uygun batch değerini belirlemeniz. Bunun için de ayrıca belli kriterler var zaten.2 months ago 42 people like this.Like ReportReply",
10. ->  ->  Aslı Hanım bu batch değerin belirlemede belli kriterler nedir? Örneğin makinenin core sayısı felan mı ya da ekstra veri üzerinden de bir kriter oluşturulabiliyor mu?.",
11. -> ->  Bununla ilgili şöyle bir liste paylaşayım sizinle;-Mini-batch değerinin seçiminde en uygun değer 1 ile eğitim kümesindeki tüm verilerin sayısı arasında ne çok küçük ne de çok büyük olmayan bir değer belirlenmelidir. Bu hızlı şekilde öğrenmeyi sağlayacaktır.-Batch size’ın büyük olması, daha doğru gradyan değerinin hesaplanmasını sağlamaktadır. Bu durum da linerizasyonu azaltmaktadır.-Belirlenen batch değerinin GPU belleğine sığması gerekiyor. Bu nedenle batch boyutu 2’nin katları şeklinde belirlenmelidir; 2, 4, 8, 16, 32, … 512 vb. Bu şekilde belirlenmemişse başarımda ani düşüşler yaşanabilir.-Batch size genelde 64 ile 512 arasında 2'nin katı olan değerlerden belirleniyor.-Eğitim kümesindeki eleman sayısı küçükse (yani 2000'den az ise) eğitim kümesindeki tüm elemanlar aynı anda kullanılabilir. Yani batch gradyan hesaplaması yapılabilir.-Evrişimsel sinir ağları (Convolutional Neural Networks) batch değerine karşı hassastır. Batch değerindeki küçük değişiklikler başarımda büyük etkiler oluşturabilir.-Batch boyutunun diğer bir kıstası da bellek boyutudur. Eğer küçük belleğe sahip ortamda çalışıyorsanız, batch büyük tutmakta zorlanabilirsiniz. Bu nedenle modeli tasarlarken öncesinde kullanabileceğiniz maksimum batch değeri hesaplamak verimli olacaktır.-Batch size küçük olması iyileştirme (reguralization) etkisi yaratmaktadır. Modele veri büyük gruplar halinde verildiğinde ezberleme daha fazla oluyor.-Batch işleminde, veri seti batch değeri olarak belirlenen değere göre parçalara ayrılmakta ve her iterasyonda modelin eğitimi bu parça üzerinden yapılmaktadır. Bununla birlikte bazı durumlarda veri kendi içinde gruplanmış olabilmektedir. Bu durum veri seti içinde korelasyon oluşturacak; bu veri setinden seçilecek test setin de yüksek başarım vermesini sağlayacak böylece ezberleme (“overfitting”) olacaktır. Bunu önlemek için eğitim başlamadan veri seti parçalara ayrılmadan önce veri seti karıştırılmalıdır (shuffle). Batch seçiminde verilerin rastgele seçilmesi önemlidir.2 months ago 15 people like this.Like ReportReply",
12. ->  ->  2'nin katı değil de 2'nin kuvveti yazmak istediniz sanırım",
13. ->  ->  evet 🙂",
14. ->  ->  teşekkürler verdiğiniz bilgiler için.",
15. ->  ->  Çok teşekkür ederim Aslı Hanım..",
16. ->  ->  Aslı hanım çok anlaşılır bir dille açıklamışsınız yavaş yavaş aşina olan bizler için çok güzel bir kolaylık bu teşekkürler..",
17. ->  Aydınlatıcı açıklamalarınız için herkese çok teşekkür ederim. Çok faydalı oluyor..",
    
### soru 

> quest: "Merhabalar! Bu akşam 21:00'de Facebook AI' da Yapay Zeka Araştırma Mühendisi olarak çalışan Tuğçe Taşçı ile Sinirbilim ve Yapay Zeka üzerine konuşuyor olacağız. Birazdan başlayacak canlı yayınımızı kaçırmayın ✨ [Link](https://www.youtube.com/channel/UCpB-u_FJegcM0WrMtr-W27w\) 

> comments:
  
    1. ->  Çok güzel bir yayındı. Ellerinize sağlık 🙂.",
    1. ->  ->  Teşekkür ederiz 🙂.",
    
### soru 

> quest: "ML ile ilgili bazı temel soru ve cevaplar:  Makine Öğrenmesi Nedir? Makine öğrenimi (ML – Machine Learning), yazılım programlarının açık bir şekilde programlanmadan sonuçları tahmin etmede daha doğru olmasını sağlayan bir algoritma kategorisidir. Makine öğrenmesinin temel dayanağı, giriş verisini alabilen algoritmalar oluşturmak ve çıktıları yeni veriler ortaya çıktıkça güncellerken bir çıktıyı tahmin etmek için istatistiksel analiz kullanmaktır.  Makine Öğrenmesinin Kullanım Alanları Nedir? Hayatımıza yerleşmiş çoğu teknolojide makine öğrenmesi teknikleri kullanılmakta. Kullanıldığı alanların spektrumu ise ifade edilemeyecek kadar geniş.  Twitter nasıl beğendiklerime göre anasayfamı kişiselleştirebiliyor? LinkedIn bana nasıl tanıdığım insanları önerebiliyor? Mail kutuma düşen spam mesajlar nasıl belirleniyor?  Google Maps gibi trafik uygulamalarında en hızlı rota belirlenirken, Facebook’a yüklenen fotoğraflara kişiler otomatik etiketlenirken, online alışveriş sitelerinde bir ürün arattıktan sonra size o ürünlerle ilgili reklamlar gösterilirken ve her gün bir şekilde kullandığımız Siri, Alexa, Google ve Cortana gibi asistanlarda makine öğrenmesi teknikleri sıkça kullanılıyor.  Makine Öğrenmesinin Alt Dalları Neledir? Makine öğrenmesinin başlıca alt dalları şunlardır:  Doğal Dil İşleme (Natural Language Processing - NLP) - İnsanların günlük hayatta kullandığı dili “anlayıp” bu bilgiyi metin özetleme, muhabbet edebileceğiniz chatbotlar yaratma, şirketinizin sosyal medyada nasıl algılandığını tespit etme gibi içine kelimelerin girdiği her alanda kullanan NLP her hayatımızda daha önemli bir role sahip oluyor.  Bilgisayarla Görü (Computer Vision - CV) - Otonom araçların yolda gitmesini, Instagram filtrelerinin yüzünüzü bulmasını, derinizdeki polis kameralarının kim olduğunuzu tespit etmesini, geri dönüşüm atıklarının türlerine göre ayrıştırılmasını ve daha nicesini sağlayan CV yarattığı yeni olanaklarla hem hayatımızı kolaylaştırıyor hem de yapay zeka etiğinin tartışılmasına sebep oluyor.  Dolandırıcılık Tespiti (Fraud Detection) - Bir alışverişin yaşandığı herhangi bir mecrada yaşanabilecek dolandırıcılık günümüzde ML algoritmaları ile yaşanmadan önce tespit edilebilmektedir. Fraud Detection’ı özellikle bankalar ve Amazon gibi e-alışveriş siteleri kullanmaktadır.",

> comments:
  
### soru 

> quest: "{ÖRNEK SORU} Yapay öğrenme modellerinin genelleştirme yeteneğini artırmak için hangi yöntemleri kullanabilirim?",

> comments:
  
1. ->  Genelleştirmeyi arttırmak için eğitimi erken bitirme, başlangıç weight değerlerini sınırlandırma, regularization ( örnek PCA) , inputa biraz gürültü(noise) eklemek örnek olarak verilebilir..",
2. ->  ->  Merhaba Enes bu bir örnek sorudur 🙂 Ama yine de dikkate alıp yanıtladığın için teşekkür ederim..",
3. ->  ünlü Merhaba Crash Course içeriğinde ön gereksinimler kısmından da başlasak olur mu acaba?.",